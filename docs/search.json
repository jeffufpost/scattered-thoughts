[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Homepage\nElectrical engineer by background, but enthusiast of all things data, electronics, governance, development, global health, skiing, football, and many other things."
  },
  {
    "objectID": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html",
    "href": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html",
    "title": "“Epidemic modeling - Part 3”",
    "section": "",
    "text": "“Examining the major flaw of the deterministic SEIR model”"
  },
  {
    "objectID": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#motivation-for-write-up",
    "href": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#motivation-for-write-up",
    "title": "Epidemic modeling - Part 3",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThis is the 3rd part of a multi-part series blog post on modeling in epidemiology.\nThe COVID-19 pandemic has brought a lot of attention to study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points.\nAfter introducing the concepts of compartmentalization and disease dynamics in the first blog post, the second part looked at a deterministic numerical solution for the SEIR model discussed, and the effects of the parameters \\(\\beta\\), \\(\\sigma\\), and \\(\\gamma\\).\nWhile arguments can be made that the compartments themselves don’t reflect the reality of COVID-19, this is not the point of this discussion; I want to focus on the idea that the population level dynamics forget about the individual progression of the disease.\nWith this mind, this third part is going to discuss the problems that arise when averaging the latent period (\\(\\frac{1}{\\sigma}\\)) and infectious period (\\(\\frac{1}{\\gamma}\\)) on the simulations.\nLet’s have a look at the individual progression of disease to understand what is wrong."
  },
  {
    "objectID": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#implications-of-deterministic-model",
    "href": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#implications-of-deterministic-model",
    "title": "Epidemic modeling - Part 3",
    "section": "Implications of deterministic model",
    "text": "Implications of deterministic model\n\n#hide\n!pip install plotly==4.6.0\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import poisson\nfrom scipy.stats import expon\nfrom scipy.stats import gamma\nfrom scipy.stats import weibull_min\n\ndef seir_model(init, parms, days):\n    S_0, E_0, I_0, R_0 = init\n    Epd, Ipd, Rpd = [0], [0], [0]\n    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n    dt=0.1\n    t = np.linspace(0,days,int(days/dt))\n    sigma, beta, gam = parms\n    for _ in t[1:]:\n        next_S = S[-1] - beta*S[-1]*I[-1]*dt\n        Epd.append(beta*S[-1]*I[-1]*dt)\n        next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt\n        Ipd.append(sigma*E[-1]*dt)\n        next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt\n        Rpd.append(gam*I[-1]*dt)\n        next_R = R[-1] + (gam*I[-1])*dt\n        S.append(next_S)\n        E.append(next_E)\n        I.append(next_I)\n        R.append(next_R)\n    return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T\n\nCollecting plotly==4.6.0\n  Downloading https://files.pythonhosted.org/packages/15/90/918bccb0ca60dc6d126d921e2c67126d75949f5da777e6b18c51fb12603d/plotly-4.6.0-py2.py3-none-any.whl (7.1MB)\n     |████████████████████████████████| 7.2MB 2.5MB/s \nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0)\nInstalling collected packages: plotly\n  Found existing installation: plotly 4.4.1\n    Uninstalling plotly-4.4.1:\n      Successfully uninstalled plotly-4.4.1\nSuccessfully installed plotly-4.6.0\n\n\n\nLatent period \\(= T_{Latent} = \\frac{1}{\\sigma}\\)\nUsing the numerical model in part 2 and in order to see the distribution of E → I, we set the initial number of E to be the same as the population, and plot the number of E over time as below:\n\n#collapse_hide\n# Define parameters\ndays = 30\nN = 10000\ninit = 0, N, 0, 0\nsigma = 1/5.2\nbeta = 0.5\ngam = 1/28.85\nparms = sigma, beta, gam\n\n# Plot simulation\nfig = go.Figure(data=[       \n    go.Scatter(name='E to I', x=np.linspace(0,days,days*10), y=100*(1-seir_model(init, parms, days).T[1]/N)), \n    go.Scatter(name='$\\\\text{Exponential distribution with} Scale = \\\\frac{1}{\\sigma}$', x=np.arange(days), y=100*expon.cdf(np.arange(days),loc=0,scale=1/sigma))\n])\n\nfig.update_layout(\n    title='Number of E moving to I over time when all population is exposed on day 0',\n    xaxis_title='Days',\n    yaxis_title='Percent of exposed having become infectious',\n    legend=dict(\n        x=0.6,\n        y=0,\n        traceorder=\"normal\",\n    )\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nThe plot above confirms the numerical model from part 2 assumes people go from E → I according to the exponential distribution.\n\n\nInfectious period \\(= T_{Infectious} = \\frac{1}{\\gamma}\\)\nThe same discussion above applies for the time from I → R here.\nFrom the discussion above, we know the numerical model in part 2 approximates the time from I → R as an exponential distribution.\nLet’s verifiy this in the plot below:\n\n#collapse_hide\n# Define parameters\ndays = 100\nN = 10000\ninit = 0, 0, N, 0\nsigma = 1/5.2   # 1/5 --&gt; 5 days on average to go from E --&gt; I\nbeta = 0.5\ngam = 1/28.85     # 1/11 --&gt; 11 days on average to go from I --&gt; R\nparms = sigma, beta, gam\n\n# Plot simulation\nfig = go.Figure(data=[       \n    go.Scatter(name='I to R', x=np.linspace(0,days,days*10), y=100*(1-seir_model(init, parms, days).T[2]/N)), \n    go.Scatter(name='$\\\\text{Exponential distribution with} Scale = \\\\frac{1}{\\gamma}$', x=np.arange(days), y=100*expon.cdf(np.arange(days),loc=0,scale=1/gam))\n])\n\nfig.update_layout(\n    title='Number of I moving to R over time when all population is infectious on day 0',\n    xaxis_title='Days',\n    yaxis_title='Percent of infectious having become recovered',\n    legend=dict(\n        x=0.6,\n        y=0,\n        traceorder=\"normal\",\n    )\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nThe plot above confirms the numerical model from part 2 assumes people go from I → R according to the exponential distribution."
  },
  {
    "objectID": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#comparing-exponential-distribution-to-covid-19-data",
    "href": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#comparing-exponential-distribution-to-covid-19-data",
    "title": "Epidemic modeling - Part 3",
    "section": "Comparing exponential distribution to COVID-19 data",
    "text": "Comparing exponential distribution to COVID-19 data\nAs we have seen above, this deterministic model implies \\(T_{Latent}\\) and \\(T_{Infectious}\\) are exponentially distributed and we know the exponential distribution is uniquely characterized by its scale where: \\[scale = \\frac{1}{mean}\\]\n\nLatent period\nFor COVID-19, as we have seen in part 2 of the blog, research has shown the following for \\(T_{Latent}\\): * mean = 5.2 days * range is [2,14] days * 95th percentile is 12.5 days\nAssuming an exponential distribution, however, we would obtain the following:\n\n\\(mean = \\frac{1}{scale} = 5.2\\ days\\)\n95th percentile would be 16 days\nAfter the first day in state E, 18% would move into the state I (the fastest in real-world data was 2 days so this is not possible)\n\nWhile we can adjust to scale to fit the real-world mean, the distribution does not match the real-world data.\n\n\nInfectious period\nSimilarly as above, for COVID-19 we have seen research has shown the following for \\(T_{Infectious}\\): * median = 20 days * range is [8,37] days\nAssuming an exponential distribution, however, we would obtain the following:\n\n\\(mean = \\frac{median}{\\ln2} = \\frac{20}{\\ln2} = 28.85\\ days\\)\n95th percentile would be 87 days, while we’d likely want it to be around 37 days\nAfter the first day in state I, 18% would move into the state R (the fastest in real-world data was 8 days so this is not possible)\n\nWhile we can adjust to scale to fit the real-world mean, the distribution does not match the real-world data - and for a parameter that influences the overall simulation, it is pretty far off.\nLet’s see what distribution looks more likely."
  },
  {
    "objectID": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#finding-a-better-fit-gamma-or-weibull-distributions",
    "href": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#finding-a-better-fit-gamma-or-weibull-distributions",
    "title": "Epidemic modeling - Part 3",
    "section": "Finding a better fit: Gamma or Weibull distributions?",
    "text": "Finding a better fit: Gamma or Weibull distributions?\nWe have seen how different the actual COVID-19 \\(T_{Latent}\\) and \\(T_{Infectious}\\) were from the deterministic model using exponential distributions.\nHere we want to find a better distribution, and one that immediatly comes to mind is the Gamma distribution.\nAnother is the Weibull distribution.\n\nCharacterizing the Gamma distribution\nThe gamma distribution is characterized by its shape parameter \\(k\\) and its scale parameter \\(\\theta\\), where: \\[Mean = k~\\theta\\]\n\n\nCharacterizing the Weibull distribution\nSimilarly, the Weibull distribution is characterized by its shape parameter \\(k\\) and its scale parameter \\(\\theta\\), where: \\[Mean=\\lambda \\Gamma \\left(1+{\\frac {1}{k}}\\right)\\] And: \\[Median = \\lambda (\\ln 2)^{1/k}\\]\n\n\nGamma distributed latent period\nLet’s first find a Gamma distribution to match \\(T_{Latent}\\) data for COVID-19.\nThe mean is 5.2 days.\nThe range is [2,14] days and 95th percentile is 12.5 days, so we could translate this as follows: * 5th percentile = 2 days * 95th percentil = 12.5 days\n\\[Mean = k~\\theta\\] \\[\\leftrightarrow k~\\theta = 5.2\\] \\[\\leftrightarrow k = \\frac{5.2}{\\theta}\\]\nWe find the following parameters result in a pretty close distribution: * \\(loc = 1.8\\) * \\(k = 0.9\\) * \\(\\theta = \\frac{5.2-loc}{k} = 3.\\dot{7}\\)\n\n#hide\nfrom prettytable import PrettyTable\nimport math\n\n\n#collapse_hide\np=100000\n\ndays=30\n\nk=0.9\nlocg=1.8\ntheta=(5.2-locg)/k\n\nscalee=5.2\n\ndf = pd.DataFrame({\n    'Exponential': expon.rvs(scale=scalee,size=p),\n    'Gamma': gamma.rvs(k,loc=locg,scale=theta,size=p)\n    })\n\nt=PrettyTable(['Distribution', 'Mean', 'Median', '5th percentile', '95th percentile'])\nt.add_row(['Exponential', df.Exponential.mean(), df.Exponential.median(), df.Exponential.quantile(q=0.05), df.Exponential.quantile(q=0.95)])\nt.add_row(['Gamma', df.Gamma.mean(), df.Gamma.median(), df.Gamma.quantile(q=0.05), df.Gamma.quantile(q=0.95)])\nprint(t)\n\nfig = go.Figure(data=[       \n    go.Scatter(name='Gamma E --&gt; I', x=np.arange(days), y=gamma.cdf(np.arange(days), k, loc=locg, scale=theta), line={'color':'red'}),\n    go.Scatter(name='Expon E --&gt; I', x=np.arange(days), y=expon.cdf(np.arange(days), scale=scalee), line={'color':'blue'}),\n])\n\nfig.update_layout(\n    title={\n        'text':'Exponential vs. Gamma CDF',\n        'x':0.5,\n        'xanchor':'center'\n    },\n    xaxis_title='Days',\n    yaxis_title='Percent of exposed having become infectious',\n    legend=dict(\n        x=1,\n        y=0,\n        traceorder=\"reversed\",\n    )\n)\n\nfig.show()\n\n+--------------+-------------------+-------------------+---------------------+--------------------+\n| Distribution |        Mean       |       Median      |    5th percentile   |  95th percentile   |\n+--------------+-------------------+-------------------+---------------------+--------------------+\n| Exponential  | 5.207469481987089 | 3.594278143717058 | 0.27075286628595824 | 15.622868963942889 |\n|    Gamma     | 5.196922877842177 | 4.054896063203861 |  1.9329997424767378 | 12.339859622859123 |\n+--------------+-------------------+-------------------+---------------------+--------------------+\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n#collapse_hide\np=10000\n\nk=0.9\nlocg=1.8\ntheta=(5.2-locg)/k\n\nscalee=5.2\n\ndf = pd.DataFrame({\n    'Exponential': expon.rvs(scale=scalee,size=p),\n    'Gamma': gamma.rvs(k,loc=locg,scale=theta,size=p)\n    })\n\nfig = px.histogram(df.stack().reset_index().rename(columns={\"level_1\": \"Distribution\"}), x=0, color=\"Distribution\", marginal='box')\nfig.update_layout(\n    title={\n        'text':'Exponential vs. Gamma distributions',\n        'x':0.5,\n        'xanchor':'center'\n    },\n    xaxis_title='Days',\n    yaxis_title='Count',\n    legend=dict(\n        x=1,\n        y=0,\n        traceorder=\"normal\",\n    )\n)\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\nGamma or Weibull distributed infectious period\nWhile we used a Gamma distribution for \\(T_{Latent}\\) above, we do not have a mean for \\(T_{Infectious}\\) data for COVID-19. We can still try to find a Gamma distribution that matches but it may be a bit more difficult to do.\nWith the median however, we could use the Weibull distribution as described earlier.\n\\[Median = \\lambda (\\ln 2)^{1/k}\\] \\[\\leftrightarrow \\lambda = \\frac{Median}{(\\ln 2)^{1/k}}\\] \\[\\leftrightarrow \\lambda = \\frac{20}{(\\ln 2)^{1/k}}\\]\nThe range is [8,37] days so we could translate this as follows: * 5th percentile = 8 days * 95th percentil = 37 days\nWe find the following parameters result in a pretty close Gamma distribution: * \\(loc = 3\\) * \\(k = 4\\) * \\(\\theta = 4.25\\)\nSimilarly, we find the following parameters result in a pretty close Weibull distribution: * \\(loc = 2\\) * \\(k = 2.3\\) * \\(\\lambda = \\frac{20-2}{(\\ln 2)^{1/k}} = 21.11\\)\n\n#hide\nfrom prettytable import PrettyTable\nimport math\n\n\n#collapse_hide\np = 10000\n\ndays=80\n\nk=4\nlocg=3\ntheta=(20-locg)/k\n\nlocw=2\nwk = 2.3\nwl = (20-locw)/(math.log(2)**(1/wk))\n\nloce=0\nscale=28.85-loce\n\ndf = pd.DataFrame({\n    'Exponential': expon.rvs(loc=loce, scale=scale,size=p),\n    'Gamma': gamma.rvs(k,loc=locg,scale=theta,size=p),\n    'Weibull': weibull_min.rvs(wk, loc=locw, scale=wl,size=p)\n    })\n\nt=PrettyTable(['Distribution', 'Mean', 'Median', '5th percentile', '95th percentile'])\nt.add_row(['Exponential', df.Exponential.mean(), df.Exponential.median(), df.Exponential.quantile(q=0.05), df.Exponential.quantile(q=0.95)])\nt.add_row(['Gamma', df.Gamma.mean(), df.Gamma.median(), df.Gamma.quantile(q=0.05), df.Gamma.quantile(q=0.95)])\nt.add_row(['Weibull', df.Weibull.mean(), df.Weibull.median(), df.Weibull.quantile(q=0.05), df.Weibull.quantile(q=0.95)])\nprint(t)\n\nfig = go.Figure(data=[       \n    go.Scatter(name='Expon I --&gt; R', x=np.arange(days), y=expon.cdf(np.arange(days), loc=loce, scale=scale), line={'color':'blue'}),\n    go.Scatter(name='Gamma I --&gt; R', x=np.arange(days), y=gamma.cdf(np.arange(days), k, loc=locg, scale=theta), line={'color':'red'}),\n    go.Scatter(name='Weibull I --&gt; R', x=np.arange(days), y=weibull_min.cdf(np.arange(days), wk, loc=locw, scale=wl), line={'color':'green'})\n])\n\nfig.update_layout(\n    title={\n        'text':'Exponential vs. Gamma vs. Weibull CDF',\n        'x':0.5,\n        'xanchor':'center'\n    },\n    xaxis_title='Days',\n    yaxis_title='Percent of exposed having become infectious',\n    legend=dict(\n        x=1,\n        y=0,\n        traceorder=\"normal\",\n    )\n)\n\nfig.show()\n\n+--------------+--------------------+--------------------+--------------------+-------------------+\n| Distribution |        Mean        |       Median       |   5th percentile   |  95th percentile  |\n+--------------+--------------------+--------------------+--------------------+-------------------+\n| Exponential  | 28.476757690391107 | 19.996705193640878 | 1.5333146455434519 |  85.121378779367  |\n|    Gamma     | 20.075920288429813 |  18.6884046455234  | 8.612411926089568  | 36.12390356768179 |\n|   Weibull    | 20.624411927557794 | 19.93798874457553  |  7.7287057647939   | 35.71926543212359 |\n+--------------+--------------------+--------------------+--------------------+-------------------+\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n#collapse_hide\nk=4\nlocg=3\ntheta=(20-locg)/k\n\nlocw=2\nwk = 2.3\nwl = (20-locw)/(math.log(2)**(1/wk))\n\nloce=0\nscalee=28.85-loce\n\np=10000\n\ndf = pd.DataFrame({\n    'Exponential': expon.rvs(loc=loce, scale=scalee,size=p),\n    'Gamma': gamma.rvs(k,loc=locg,scale=theta,size=p),\n    'Weibull': weibull_min.rvs(wk, loc=locw, scale=wl,size=p)\n    })\n\nfig = px.histogram(df.stack().reset_index().rename(columns={\"level_1\": \"Distribution\"}), x=0, color=\"Distribution\", marginal='box')\nfig.update_layout(\n    title={\n        'text':'Exponential vs. Gamma vs. Weibull distributions',\n        'x':0.5,\n        'xanchor':'center'\n    },\n    xaxis_title='Days',\n    yaxis_title='Count',\n    legend=dict(\n        x=1,\n        y=0,\n        traceorder=\"normal\",\n    )\n)\nfig.show()"
  },
  {
    "objectID": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#discussion",
    "href": "posts/2020-03-25-proba-distrib/2020-03-25-proba-distrib.html#discussion",
    "title": "Epidemic modeling - Part 3",
    "section": "Discussion",
    "text": "Discussion\n\\(T_{Latent}\\) is nicely matched with a Gamma distribution.\n\\(T_{Infectious}\\) is nicely matched by a Weibull distribution.\nThe take away however is that the exponential distribution matches neither - and in the case of \\(T_{Infectious}\\) it is very far off.\nWe have seen in the previous posts that both of these periods have an impact on the peak proportion of infectious people and the duration of that peak.\nNaturally, we need to investigate further the impact of changing the distributions from exponential to Gamma and Weibull on the simulations.\nThis is done in the next blog post where I build a new model to be able to take into account the actual distributions."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html",
    "href": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html",
    "title": "Epidemic modeling - Part 1",
    "section": "",
    "text": "“A deterministic numerical SEIR model”"
  },
  {
    "objectID": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#motivation-for-write-up",
    "href": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#motivation-for-write-up",
    "title": "Epidemic modeling - Part 2",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThis is the 2nd part of a multi-part series blog post on modeling in epidemiology.\nThe COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points.\nAfter introducing the concepts of compartmentalization and disease dynamics in the first blog post, this second part is focused on developing a deterministic numerical solution for the SEIR model discussed there.\nWhile normally the goal is to use real-world data to infer characteristics of the underlying disease (as will be done in later blog posts), here we want to use simulate the spread of a COVID-19 like disease in a population of 10000, and look at the effects of the different parameters on the spread."
  },
  {
    "objectID": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#recall-seir-model-equations",
    "href": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#recall-seir-model-equations",
    "title": "Epidemic modeling - Part 2",
    "section": "Recall SEIR model equations",
    "text": "Recall SEIR model equations\nSee the first blog post for derivation.\n\nContinuous-time:\n\n\\(\\frac{ds(t)}{dt}=-\\beta i(t) s(t)\\)\n\\(\\frac{de(t)}{dt}=\\beta i(t) s(t) - \\sigma e(t)\\)\n\\(\\frac{di(t)}{dt}=\\sigma e(t) - \\gamma i(t)\\)\n\\(\\frac{dr(t)}{dt}=\\gamma i(t)\\)\n\nDiscrete-time:\n\n\\(\\Delta S = -\\beta I S \\Delta T\\)\n\\(\\Delta E = (\\beta I S -\\sigma E) \\Delta T\\)\n\\(\\Delta I = (\\sigma E - \\gamma I) \\Delta T\\)\n\\(\\Delta R = \\gamma I \\Delta T\\)"
  },
  {
    "objectID": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#numerical-solution-to-this-deteministic-population-level-model",
    "href": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#numerical-solution-to-this-deteministic-population-level-model",
    "title": "Epidemic modeling - Part 2",
    "section": "Numerical solution to this deteministic population level model",
    "text": "Numerical solution to this deteministic population level model\n\nCoding the SEIR model\nTo build the SEIR model we simply use the discrete-time set of equations above.\nThe model will thus take as input the following:\n\nInitial proportion of S, E, I, and R in the population\n\\(\\beta\\) parameter pertaining to the population in question\n\\(\\sigma\\) and \\(\\gamma\\) parameters pertaining to the disease\nNumbers of days to run the simulation\n\n\n\nCode\n# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\n\n\nCode\n# Let's build a numerical solution\n\ndef seir_model(init, parms, days):\n    S_0, E_0, I_0, R_0 = init\n    Epd, Ipd, Rpd = [0], [0], [0]\n    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n    dt=0.1\n    t = np.linspace(0,days,int(days/dt))\n    sigma, beta, gam = parms\n    for _ in t[1:]:\n        next_S = S[-1] - beta*S[-1]*I[-1]*dt\n        Epd.append(beta*S[-1]*I[-1]*dt)\n        next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt\n        Ipd.append(sigma*E[-1]*dt)\n        next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt\n        Rpd.append(gam*I[-1]*dt)\n        next_R = R[-1] + (gam*I[-1])*dt\n        S.append(next_S)\n        E.append(next_E)\n        I.append(next_I)\n        R.append(next_R)\n    return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T\n\n\n\n\nCOVID-19 parameters\nSimulation parameters used for plot below:\n\nDays = 100\nPopulation = 10000\nNumber of susceptible people on day 0 = 9999\nNumber of exposed people on day 0 = 1\nNo infected or recovered people on day 0\n\nA lot of research is ongoing into the COVID-19 characteristics of \\(\\beta\\), \\(\\sigma\\), and \\(\\gamma\\).\nHowever, these are complex studies that require a lot of data and so far we have little information to go on.\nThe literature suggests the following:\n\n\\(\\underline{T_{Incubation}}\\):\n\nThe mean is 5-6 days but it can range anywhere from 2-14 days 1 2\nAnother paper reports a mean incubation period of 5.2 days and the 95th percentile at 12.5 days 3.\nThere are reports of pre-symptomatic infections4, but these are reportedly rare 5 so in the following models we will assume: \\[T_{Incubation} = T_{Latent}\\] And so: \\[\\sigma = \\frac{1}{5.2} days^{-1}\\]\n\n\\(\\underline{T_{Infectious}}\\):\n\nAgain it is very difficult to say for sure and the period of communicability is very uncertain for COVID-19.\nResearch suggests a median of 20 days of viral shedding after onset of symptoms 6.\nRanging from 8 to 37 days in survivors.\nWhile it is noted PCR positivity does not necessarily reflect the infectious period (virus may not be viable but the PCR amplification will result in a positive), for the purpose of this blog post we will assume the following: \\[T_{Infectious} = T_{Clinical}\\] To obtain an exponential distribution with median M, the scale A is calculated as follows: \\[A = \\frac{M}{\\ln2} = \\frac{20}{\\ln2}\\] This results in \\[\\gamma = \\frac{\\ln2}{20} = \\frac{1}{28.85}\\ days^{-1}\\] * \\(\\underline{Beta= \\beta}\\):\nWhile difficult to estimate this parameter as there is a lot of variation between countries, cultures, societal norms, etc.. a little thought experiment can help us evaluate the value for \\(\\beta = r\\rho\\) in Switerland or France for example.\nIf no control measures are put in place and people do not change habits (as is the case in this blog post), we can expect the following:\n\nAverage number of contacts per day:\n\n\\[r = 10\\ contacts\\ per\\ day\\]\n\nAverage probability of transmission from contact:\n\n\\[\\rho = 5\\%\\]\nAnd so: \\[\\beta = r\\rho = 0.5\\]\n\n\nRunning the simulation\n\n\nCode\n#Define parameters\ndays = 200\nN = 10000\ninit = 1 - 1/N, 1/N, 0, 0\nsigma = 1/5.2\nbeta = 0.5\ngam = 1/28.85\nparms = sigma, beta, gam\n\n# Run simulation\nresults_avg = seir_model(init, parms, days)\n\n\n\n\nCode\nfig = go.Figure(data=[       \n    go.Scatter(name='S', x=np.linspace(0,days,days*10), y=results_avg.T[0], line={'dash':'solid', 'color':'blue'}),\n    go.Scatter(name='E', x=np.linspace(0,days,days*10), y=results_avg.T[1], line={'dash':'solid', 'color':'yellow'}),\n    go.Scatter(name='I', x=np.linspace(0,days,days*10), y=results_avg.T[2], line={'dash':'solid', 'color':'red'}), \n    go.Scatter(name='R', x=np.linspace(0,days,days*10), y=results_avg.T[3], line={'dash':'solid', 'color':'green'}),\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n  title={\n      'text':'Deterministic SEIR model - COVID-19 parameters',\n      'x':0.5,\n      'xanchor':'center'\n    }\n)\nfig.update_layout(height=500, template=\"ggplot2\")\n\nfig.show()"
  },
  {
    "objectID": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#qualitative-analysis-of-beta-sigma-and-gamma",
    "href": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#qualitative-analysis-of-beta-sigma-and-gamma",
    "title": "Epidemic modeling - Part 2",
    "section": "Qualitative analysis of \\(\\beta\\), \\(\\sigma\\), and \\(\\gamma\\)",
    "text": "Qualitative analysis of \\(\\beta\\), \\(\\sigma\\), and \\(\\gamma\\)\n\nEffect of \\(\\sigma\\) (\\(T_{Latent}\\))\nLet’s have a look at the effect of \\(\\sigma\\) (or inversely, the latent period) on the SEIR simulation.\nA higher \\(\\sigma\\) means shorter average latent period, and vice-versa.\n\n\nCode\n## Let's try to see how the model changes \ndays = 1000\nN = 10000\ninit = 1 - 1/N, 1/N, 0, 0\nsigma_high = 1   # 1 --&gt; Average 1 day from E --&gt; I (ressembles SIR model)\nsigma_low = 1/100 #10 days on average, twice as long as COVID-19\nsigma_covid = 1/5.2\nbeta = 0.5\ngam = 1/28.85\nparms_fastEI = sigma_high, beta, gam\nparms_slowEI = sigma_low, beta, gam\nparms_avg = sigma_covid, beta, gam\n\n# Run simulation\nresults_fastEtoI = seir_model(init, parms_fastEI, days)\nresults_slowEtoI = seir_model(init, parms_slowEI, days)\nresults_avg = seir_model(init, parms_avg, days)\n\n\n\n\nCode\nfig = go.Figure(data=[    \n    go.Scatter(name=r'$S:\\sigma_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[0], line={'dash':'solid', 'color':'blue'}, legendgroup=\"COVID\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[1], line={'dash':'solid', 'color':'yellow'}, legendgroup=\"COVID\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[2], line={'dash':'solid', 'color':'red'}, legendgroup=\"COVID\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[3], line={'dash':'solid', 'color':'green'}, legendgroup=\"COVID\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{high}$', x=np.linspace(0,days,days*10), y=results_fastEtoI.T[0], line={'dash':'dash','color':'blue'}, legendgroup=\"high\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{high}$', x=np.linspace(0,days,days*10), y=results_fastEtoI.T[1], line={'dash':'dash', 'color':'yellow'}, legendgroup=\"high\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{high}$', x=np.linspace(0,days,days*10), y=results_fastEtoI.T[2], line={'dash':'dash', 'color':'red'}, legendgroup=\"high\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{high}$', x=np.linspace(0,days,days*10), y=results_fastEtoI.T[3], line={'dash':'dash', 'color':'green'}, legendgroup=\"high\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{low}$', x=np.linspace(0,days,days*10), y=results_slowEtoI.T[0], line={'dash':'dot', 'color':'blue'}, legendgroup=\"slow\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{low}$', x=np.linspace(0,days,days*10), y=results_slowEtoI.T[1], line={'dash':'dot', 'color':'yellow'}, legendgroup=\"slow\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{low}$', x=np.linspace(0,days,days*10), y=results_slowEtoI.T[2], line={'dash':'dot', 'color':'red'}, legendgroup=\"slow\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\sigma_{low}$', x=np.linspace(0,days,days*10), y=results_slowEtoI.T[3], line={'dash':'dot', 'color':'green'}, legendgroup=\"slow\", hoverinfo='x+y'),\n])\n\nfig.update_layout(\n    template='ggplot2',\n    height=500,\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of } \\sigma \\ \\text{on Deterministic SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n                                                \n\n\nWe notice a few things from the plot above on the impact of the average time from E → I:\n\nThe shorter the latent period:\n\nthe faster the epidemic propogates in the population\nthe higher the peak of infected individuals will be (meaning a higher chance hospital resources will be saturated)\n\nHowever, the latent period has no impact on the total number of individuals infected over the entire time of the epidemic.\n\n\n\nEffect of \\(\\beta = r~\\rho\\)\nLet’s have a look at the effect of \\(\\beta\\) on the SEIR simulation.\nA higher \\(\\beta\\) can either mean a higher average number of contacts per day (\\(r\\)) in the population and/or a higher probability of transmission of disease from I → S.\nThe opposite holds also.\n\n\nCode\n## Let's try to see how the model changes \ndays = 500\nN = 10000\ninit = 1 - 1/N, 1/N, 0, 0\nsigma_avg = 1/5.2\nbeta_avg = 0.5\nbeta_noepi = 1/30\nbeta_low = 0.1\nbeta_high = 4\ngam = 1/28.85\nparms_avg = sigma_avg, beta_avg, gam\nparms_noepi = sigma_avg, beta_noepi, gam \nparms_low = sigma_avg, beta_low, gam\nparms_high = sigma_avg, beta_high, gam\n\n# Run simulation\nresults_avg = seir_model(init, parms_avg, days)\nresults_noepi = seir_model(init, parms_noepi, days)\nresults_low = seir_model(init, parms_low, days)\nresults_high = seir_model(init, parms_high, days)\n\n\n\n\nCode\nfig = go.Figure(data=[    \n    go.Scatter(name=r'$S:\\beta_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[0], line={'dash':'solid', 'color':'blue'}, legendgroup=\"COVID\"),\n    go.Scatter(name=r'$E:\\beta_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[1], line={'dash':'solid', 'color':'yellow'}, legendgroup=\"COVID\"), \n    go.Scatter(name=r'$I:\\beta_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[2], line={'dash':'solid', 'color':'red'}, legendgroup=\"COVID\"),\n    go.Scatter(name=r'$R:\\beta_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[3], line={'dash':'solid', 'color':'green'}, legendgroup=\"COVID\"),\n    go.Scatter(name=r'$S:\\beta_{noepi}$', x=np.linspace(0,days,days*10), y=results_noepi.T[0], line={'dash':'dashdot','color':'blue'}, legendgroup=\"noepi\"),\n    go.Scatter(name=r'$E:\\beta_{noepi}$', x=np.linspace(0,days,days*10), y=results_noepi.T[1], line={'dash':'dashdot', 'color':'yellow'}, legendgroup=\"noepi\"),\n    go.Scatter(name=r'$I:\\beta_{noepi}$', x=np.linspace(0,days,days*10), y=results_noepi.T[2], line={'dash':'dashdot', 'color':'red'}, legendgroup=\"noepi\"),\n    go.Scatter(name=r'$R:\\beta_{noepi}$', x=np.linspace(0,days,days*10), y=results_noepi.T[3], line={'dash':'dashdot', 'color':'green'}, legendgroup=\"noepi\"),\n    go.Scatter(name=r'$S:\\beta_{low}$', x=np.linspace(0,days,days*10), y=results_low.T[0], line={'dash':'dash','color':'blue'}, legendgroup=\"low\"),\n    go.Scatter(name=r'$E:\\beta_{low}$', x=np.linspace(0,days,days*10), y=results_low.T[1], line={'dash':'dash', 'color':'yellow'}, legendgroup=\"low\"),\n    go.Scatter(name=r'$I:\\beta_{low}$', x=np.linspace(0,days,days*10), y=results_low.T[2], line={'dash':'dash', 'color':'red'}, legendgroup=\"low\"),\n    go.Scatter(name=r'$R:\\beta_{low}$', x=np.linspace(0,days,days*10), y=results_low.T[3], line={'dash':'dash', 'color':'green'}, legendgroup=\"low\"),\n    go.Scatter(name=r'$S:\\beta_{high}$', x=np.linspace(0,days,days*10), y=results_high.T[0], line={'dash':'dot', 'color':'blue'}, legendgroup=\"high\"),\n    go.Scatter(name=r'$E:\\beta_{high}$', x=np.linspace(0,days,days*10), y=results_high.T[1], line={'dash':'dot', 'color':'yellow'}, legendgroup=\"high\"),\n    go.Scatter(name=r'$I:\\beta_{high}$', x=np.linspace(0,days,days*10), y=results_high.T[2], line={'dash':'dot', 'color':'red'}, legendgroup=\"high\"),\n    go.Scatter(name=r'$R:\\beta_{high}$', x=np.linspace(0,days,days*10), y=results_high.T[3], line={'dash':'dot', 'color':'green'}, legendgroup=\"high\"),\n])\n\nfig.update_layout(\n    template='ggplot2',\n    height=500,\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of } \\beta \\ \\text{on Deterministic SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n                                                \n\n\nWe notice a few things from the plot above on the impact of \\(\\beta\\):\n\nThe higher \\(\\beta\\) is:\n\nthe faster the epidemic seems to propogate in the population\nthe higher the peak of infected individuals seems to be (meaning a higher chance hospital resources will be saturated)\n\n\\(\\beta\\) also appears to affect the overall number of people infected over the course of the epidemic\nA low \\(\\beta\\) means a low \\(R_0\\) and we have seen in the first part of this blog that no epidemic occurs when \\(R_0 &lt; 1\\)\nBut even if \\(R_0 &gt; 1\\), keeping \\(\\beta\\) low reduces the total number of people infected\n\n\n\nEffect of \\(\\gamma\\) (\\(T_{Infectious}\\))\nLet’s have a look at the effect of \\(\\gamma\\) on the SEIR simulation.\nA higher \\(\\gamma\\) means a shorter infectious period, and vice-versa.\n\n\nCode\n## Let's try to see how the model changes \ndays = 500\nN = 10000\ninit = 1 - 1/N, 1/N, 0, 0\nsigma_avg = 1/5.2\nbeta = 0.5\ngam_avg = 1/28.85\ngam_low = 1/200\ngam_high = 0.2\nparms_fastIR = sigma_avg, beta, gam_high\nparms_slowIR = sigma_avg, beta, gam_low\nparms_avg = sigma_avg, beta, gam_avg\n\n# Run simulation\nresults_fastItoR = seir_model(init, parms_fastIR, days)\nresults_slowItoR = seir_model(init, parms_slowIR, days)\nresults_avg = seir_model(init, parms_avg, days)\n\n\n\n\nCode\nfig = go.Figure(data=[    \n    go.Scatter(name=r'$S:\\gamma_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[0], line={'dash':'solid', 'color':'blue'}, legendgroup=\"COVID\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[1], line={'dash':'solid', 'color':'yellow'}, legendgroup=\"COVID\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[2], line={'dash':'solid', 'color':'red'}, legendgroup=\"COVID\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{COVID}$', x=np.linspace(0,days,days*10), y=results_avg.T[3], line={'dash':'solid', 'color':'green'}, legendgroup=\"COVID\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{high}$', x=np.linspace(0,days,days*10), y=results_fastItoR.T[0], line={'dash':'dash','color':'blue'}, legendgroup=\"fast\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{high}$', x=np.linspace(0,days,days*10), y=results_fastItoR.T[1], line={'dash':'dash', 'color':'yellow'}, legendgroup=\"fast\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{high}$', x=np.linspace(0,days,days*10), y=results_fastItoR.T[2], line={'dash':'dash', 'color':'red'}, legendgroup=\"fast\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{high}$', x=np.linspace(0,days,days*10), y=results_fastItoR.T[3], line={'dash':'dash', 'color':'green'}, legendgroup=\"fast\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{low}$', x=np.linspace(0,days,days*10), y=results_slowItoR.T[0], line={'dash':'dot', 'color':'blue'}, legendgroup=\"slow\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{low}$', x=np.linspace(0,days,days*10), y=results_slowItoR.T[1], line={'dash':'dot', 'color':'yellow'}, legendgroup=\"slow\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{low}$', x=np.linspace(0,days,days*10), y=results_slowItoR.T[2], line={'dash':'dot', 'color':'red'}, legendgroup=\"slow\", hoverinfo='x+y'),\n    go.Scatter(name=r'$S:\\gamma_{low}$', x=np.linspace(0,days,days*10), y=results_slowItoR.T[3], line={'dash':'dot', 'color':'green'}, legendgroup=\"slow\", hoverinfo='x+y'),\n])\n\nfig.update_layout(\n    template='ggplot2',\n    height=500,\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of } \\gamma \\ \\text{on Deterministic SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n                                                \n\n\nWe notice a few things from the plot above on the impact of the infectious period:\n\nThe longer the infectious period:\n\nthe faster the epidemic propogates in the population\nthe higher the peak of infectious individuals will be and the longer it will last (meaning a higher chance hospital resources will be saturated)\n\nAs opposed to the latent period above, but similarly as \\(\\beta\\), the infectious period has an impact on the total number of individuals infected over the entire time of the epidemic\nWith no epidemic if \\(\\gamma &gt; \\beta\\)"
  },
  {
    "objectID": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#discussion",
    "href": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#discussion",
    "title": "Epidemic modeling - Part 2",
    "section": "Discussion",
    "text": "Discussion\nSo we can see the latent and infectious periods, along with the value of \\(\\beta\\) are critical components in how the model will react.\nWorth noting also is that the higher \\(R_0\\) is, the faster the epidemic spreads and the higher the peak of infectious individuals will be (see further blog posts for some nuance on this).\nNotably, and as predicted in part 1 of the blog series, no epidemic occurs if: \\[R_0 &lt; 1\\] In other words, no epidemic if: \\[\\beta &lt; \\gamma\\]\nThere are major flaws with this model however. While this model is deterministic and uses average time to model \\(\\sigma\\) and \\(\\gamma\\), this is a major flaw and does not represent the reality for most diseases.\nPart 3 of this blog series will discuss this further."
  },
  {
    "objectID": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#footnotes",
    "href": "posts/2020-03-18-deterministic-numerical-solutions/2020-03-18-deterministic-numerical-solutions.html#footnotes",
    "title": "Epidemic modeling - Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWHO COVID-19 Situation Report 73↩︎\nCDC COVID-19 FAQ↩︎\nEarly Transmission Dynamics in Wuhan, China, of Novel Coronavirus–Infected Pneumonia↩︎\nCDC COVID-19 FAQ↩︎\nWHO COVID-19 Situation Report 73↩︎\nClinical course and mortality risk of server COVID-19↩︎"
  },
  {
    "objectID": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html",
    "href": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html",
    "title": "“Scraping COVID-19 data from data.gouv.fr”",
    "section": "",
    "text": "“A quick and very simple rundown on how to scrape hospitalization and testing data from the French open data site”"
  },
  {
    "objectID": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#introduction",
    "href": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#introduction",
    "title": "“Scraping COVID-19 data from data.gouv.fr”",
    "section": "Introduction",
    "text": "Introduction\nAs the summary explains, this blog post will very quickly explain how to automatically download French government data on hospitalization and testing pertaining to COVID⁻19."
  },
  {
    "objectID": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#data-sources",
    "href": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#data-sources",
    "title": "“Scraping COVID-19 data from data.gouv.fr”",
    "section": "Data sources",
    "text": "Data sources\n\nHospitalization data\n\nThe various datasets concerning hospitalization data are found here.\nIf you follow the link you will find 4 csv datasets concerning hospitalization data along with 5 other csv files with metadata and documentation.\n\nTesting data\n\nThe various datasets concerning testing data are found here.\nIf you follow the link you will find 2 csv datasets concerning testing data along with 2 other csv files with metadata and documentation.\nIn both cases we want to download the first of the links since they contain the pertinent daily updated data (do have a look manually at the metadata and documentation files to make sure this is what you want)."
  },
  {
    "objectID": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#code",
    "href": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#code",
    "title": "“Scraping COVID-19 data from data.gouv.fr”",
    "section": "Code",
    "text": "Code\n\n#collapse_show\n# Import libraries used below\nimport requests\nimport urllib.request\nimport urllib.parse\nimport time\nimport io\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport datetime\nimport os\n\n\nGetting the main page\nLet’s first have a look ath the main landing page that I provided above.\n\n# Store URL for each page\nurl_cases = 'https://www.data.gouv.fr/fr/datasets/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/'\nurl_tests = 'https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-tests-de-depistage-de-covid-19-realises-en-laboratoire-de-ville/'\n\n\n# Get response for each URL\nresponse_cases = requests.get(url_cases)\nresponse_tests = requests.get(url_tests)\n\nThe response here should be 200 (see life of codes here).\n\nprint(response_cases, response_tests)\n\n&lt;Response [200]&gt; &lt;Response [200]&gt;\n\n\n\n# Save the actual content of the page returned with BeautifulSoup\nsoupcases = BeautifulSoup(response_cases.text, \"html.parser\")\nsouptests = BeautifulSoup(response_tests.text, \"html.parser\")\n\n\n# Let's look at the links in the main page (for testing data - if you want cases, replace souptests with soupcases below)\nfor i in range(len(souptests.find_all('a', class_=\"btn btn-sm btn-primary\"))):\n    print(souptests.find_all('a', class_=\"btn btn-sm btn-primary\")[i].get('href'))\n\nNone\nhttps://www.data.gouv.fr/fr/datasets/r/b4ea7b4b-b7d1-4885-a099-71852291ff20\nNone\nhttps://www.data.gouv.fr/fr/datasets/r/72050bc8-9959-4bb1-88a0-684ff8db5fb5\nNone\nhttps://www.data.gouv.fr/fr/datasets/r/971c5cbd-cd80-4492-b2b3-c3deff8c1f5e\nNone\nhttps://www.data.gouv.fr/fr/datasets/r/db378f2a-83a1-40fd-a16c-06c4c8c3535d\nhttps://www.data.gouv.fr/fr/datasets/r/49ba79e6-0153-40b1-b050-821e102959eb\nNone\nhttps://www.data.gouv.fr/fr/datasets/r/59e82d52-e07a-4ae8-9a49-2d1fd2d2ec21\n\n\nWe see that the petrtinent file in each cases (testing or hospitalization data) are the first links in their page. So we save only this one as donw below:\n\n# If we want to save that first URL we can do as follows\ncasescsvurl = soupcases.find_all('a', class_=\"btn btn-sm btn-primary\")[1].get('href')\ntestscsvurl = souptests.find_all('a', class_=\"btn btn-sm btn-primary\")[1].get('href')\n\n\n\nGetting the CSV data\nWe now have the URL for the CSV files we want so we’ll do similar steps as above to download these files.\n\n# Similaraly as above, requests.get the CSV URL:\nrectests = requests.get(testscsvurl)\nreccases = requests.get(casescsvurl)"
  },
  {
    "objectID": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#what-to-do-with-the-csv-data",
    "href": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#what-to-do-with-the-csv-data",
    "title": "“Scraping COVID-19 data from data.gouv.fr”",
    "section": "What to do with the CSV data",
    "text": "What to do with the CSV data\nNow that you have the data, what to do with it?\nIt depends on your purpose I guess: * First write the data to a CSV file which you then read * Directly read the data\n\nBy first writing the CSV file to drive\n\n# This will write the data into cases.csv file\n# Of course you need to replace the actual path to the folder you want in the code below:\nwith open(os.path.join(\"/path/to/folder\", \"cases.csv\"), 'wb') as f:\n    f.write(reccases.content)\n\n\n# Same thing for testing data\n# This will write the data into tests.csv file\n# Of course you need to replace the actual path to the folder you want in the code below:\nwith open(os.path.join(\"/path/to/folder\", \"tests.csv\"), 'wb') as f:\n    f.write(rectests.content)\n\n\n# You can then read that csv file to use in your data analysis:\ntests = pd.read_csv('tests.csv', sep=';', dtype={'dep': str, 'jour': str, 'clage_covid': str, 'nb_test': int, 'nb_pos': int, 'nb_test_h': int, 'nb_pos_h': int, 'nb_test_f': int, 'nb_pos_f': int}, parse_dates = ['jour'])\ncases = pd.read_csv('cases.csv', sep=';', dtype={'dep': str, 'jour': str, 'hosp': int, 'rea': int, 'rad': int, 'dc': int}, parse_dates = ['jour'])\n\nNote in the code above I had previously looked through the raw csv data to underdstand how to parse it.\n\n\nDirectly reading the data (bypassing the writing CSV file step)\n\ncases = pd.read_csv(io.StringIO(requests.get(casescsvurl).content.decode('utf-8')), sep=';', dtype={'dep': str, 'jour': str, 'hosp': int, 'rea': int, 'rad': int, 'dc': int}, parse_dates = ['jour'])\ntests = pd.read_csv(io.StringIO(requests.get(testscsvurl).content.decode('utf-8')), sep=';', dtype={'dep': str, 'jour': str, 'hosp': int, 'rea': int, 'rad': int, 'dc': int}, parse_dates = ['jour'])"
  },
  {
    "objectID": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#other-stuff",
    "href": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#other-stuff",
    "title": "“Scraping COVID-19 data from data.gouv.fr”",
    "section": "Other stuff",
    "text": "Other stuff\n\nParsing/Converting URI into readable format\nIt sometimes happends that links are provided in URI (URL symbols encoded into % symbols…)\nYou generally need to convert those back to correct URLs, example below:\n\n# Example URI\ntesturl = 'https%3A%2F%2Fstatic.data.gouv.fr%2Fresources%2Fdonnees-hospitalieres-relatives-a-lepidemie-de-covid-19%2F20200505-190040%2Fdonnees-hospitalieres-covid19-2020-05-05-19h00.csv'\n\n\n# Convert with following line:\nurllib.parse.unquote(testurl)\n\n'https://static.data.gouv.fr/resources/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/20200505-190040/donnees-hospitalieres-covid19-2020-05-05-19h00.csv'\n\n\n\n\nA quick look at French testing data from scratch\nLet’s quickly see how, from scratch, we can use code above to scrape testing data and plot it quickly.\nNote the data only includes city testing centers and does not include hospital testing.\n\n# Use main page URL\nurl_tests = 'https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-tests-de-depistage-de-covid-19-realises-en-laboratoire-de-ville/'\nresponse_tests = requests.get(url_tests)\n\n\n# Find correct CSV file URL\nsouptests = BeautifulSoup(response_tests.text, \"html.parser\")\ntestscsvurl = souptests.find_all('a', class_=\"btn btn-sm btn-primary\")[1].get('href')\n\n\n# Read CSV file into tests variable\nrectests = requests.get(testscsvurl)\ntests = pd.read_csv(io.StringIO(requests.get(testscsvurl).content.decode('utf-8')), sep=';', dtype={'dep': str, 'jour': str, 'hosp': int, 'rea': int, 'rad': int, 'dc': int}, parse_dates = ['jour'])\n\n\n#hide\n!pip install plotly==4.6.0\n\nCollecting plotly==4.6.0\n  Downloading https://files.pythonhosted.org/packages/15/90/918bccb0ca60dc6d126d921e2c67126d75949f5da777e6b18c51fb12603d/plotly-4.6.0-py2.py3-none-any.whl (7.1MB)\n     |████████████████████████████████| 7.2MB 2.4MB/s \nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0)\nInstalling collected packages: plotly\n  Found existing installation: plotly 4.4.1\n    Uninstalling plotly-4.4.1:\n      Successfully uninstalled plotly-4.4.1\nSuccessfully installed plotly-4.6.0\n\n\n\n#collapse_hide\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# We want overall testing for France, se we groupby Day and sum: (filtering for clage_covid = 0 means not differentiated between age groups)\ndf = tests[tests.clage_covid=='0'].groupby(['jour']).sum()\n\nfig = go.Figure(data=[\n    go.Bar(name='Positive tests', x=df.index, y=df.nb_pos, marker_color='red'),\n    go.Bar(name='Total tests', x=df.index, y=df.nb_test, marker_color='blue')\n])\n\nfig.update_layout(\n    title= 'Daily positive and total testing data in France',\n    xaxis_title = 'Date',\n    yaxis_title = 'Number of tests (total and positive)',\n    barmode='group'\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#conclusion",
    "href": "posts/2020-04-15-web-scraping/2020-04-15-web-scraping.html#conclusion",
    "title": "“Scraping COVID-19 data from data.gouv.fr”",
    "section": "Conclusion",
    "text": "Conclusion\nVery easy to incorporate this into a python script to automate.\nThis is only the very basic of scraping, a lot more could be done, maybe in another blog post."
  },
  {
    "objectID": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html",
    "href": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html",
    "title": "“Epidemic modeling - Part 4”",
    "section": "",
    "text": "“Building a new stochastic SEIR model to deal with probability distributions”"
  },
  {
    "objectID": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#motivation-for-write-up",
    "href": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#motivation-for-write-up",
    "title": "“Epidemic modeling - Part 4”",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThis is the 4th part of a multi-part series blog post on modeling in epidemiology.\nThe COVID-19 pandemic has brought a lot of attention to study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points.\nAfter introducing the concepts of compartmentalization and disease dynamics in the first blog post, the second part looked at a deterministic numerical solution for the SEIR model discussed, and the effects of the parameters \\(\\beta\\), \\(\\sigma\\), and \\(\\gamma\\) in parts 1 and 2.\nPart 3 made the argument that most models ignore individual-level disease dynamics in favor of averaging population-level \\(\\sigma\\) and \\(\\gamma\\) parameters and showed some big discrepancies between actual COVID-19 probability distributions for those parameters and those used in research.\nThis 4th part is where I build a numerical SEIR model that takes into account these probability distributions in order to tweak the model as close to COVID-19 data as possible."
  },
  {
    "objectID": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#building-a-stochastic-model",
    "href": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#building-a-stochastic-model",
    "title": "“Epidemic modeling - Part 4”",
    "section": "Building a stochastic model",
    "text": "Building a stochastic model\nAs opposed to the deterministic model from Part 2, this model is going to focus on individual level disease dynamics to model the disease propagation.\nThe basic idea of this model is to have a dataframe with the number of rows equal to the population size (each individual is a row) and two columns: * State column to describe the state of each individual (S, E, I, or R) * Day column to save the day of transition of the individual into that state\nHowever, the population-level rates of transmission still apply here i.e. a person goes from S → E following three points: 1. the number of contacts the person has per unit time (given by \\(r\\)) 2. the chance a given contact is with an I - infectious individual (the higher thenumber of I, the higher the chance) 3. the chance of an S contracting the disease from a contact with an I (given by \\(\\rho\\))\nThis is done stochastically.\nOnce a person becomes E, their progression is unique to them. This progression is calculated in advance for computational reason, but it allows to use the time ditributions we want.\n\n#collapse_hide\n!pip install plotly==4.6.0\nimport pandas as pd\nimport numpy as np\nimport math\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import expon\nfrom scipy.stats import gamma\nfrom scipy.stats import weibull_min\nfrom numpy.random import default_rng\nrng = default_rng()\n\n# Let's build a numerical solution\ndef seir_model(init, parms, days):\n    S_0, E_0, I_0, R_0 = init\n    Epd, Ipd, Rpd = [0], [0], [0]\n    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n    dt=0.1\n    t = np.linspace(0,days,int(days/dt))\n    sigma, beta, gam = parms\n    for _ in t[1:]:\n        next_S = S[-1] - beta*S[-1]*I[-1]*dt\n        Epd.append(beta*S[-1]*I[-1]*dt)\n        next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt\n        Ipd.append(sigma*E[-1]*dt)\n        next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt\n        Rpd.append(gam*I[-1]*dt)\n        next_R = R[-1] + (gam*I[-1])*dt\n        S.append(next_S)\n        E.append(next_E)\n        I.append(next_I)\n        R.append(next_R)\n    return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T\n\nCollecting plotly==4.6.0\n  Downloading https://files.pythonhosted.org/packages/15/90/918bccb0ca60dc6d126d921e2c67126d75949f5da777e6b18c51fb12603d/plotly-4.6.0-py2.py3-none-any.whl (7.1MB)\n     |████████████████████████████████| 7.2MB 4.6MB/s \nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0)\nInstalling collected packages: plotly\n  Found existing installation: plotly 4.4.1\n    Uninstalling plotly-4.4.1:\n      Successfully uninstalled plotly-4.4.1\nSuccessfully installed plotly-4.6.0\n\n\n\nCreating the initial population dataframe\nBelow is a function to create the initial population dataframe: * \\(p\\) is the population number * \\(num_E\\) is the number of people exposed on day 0 * \\(num_I\\) is the number of infectious on day 0 * \\(num_R\\) is the number of people recovered on day 0\n\n#collapse_hide\n# Need this new function for model below:\ndef make_df(p,num_E, num_I, num_R):\n  df = pd.DataFrame(np.full((p,1), 'S').T[0], columns=['State'])\n  df['Day'] = 0\n  tochange=df.loc[rng.choice(p, size=num_E+num_I+num_R, replace=False),'State'].index\n  df.loc[tochange[0:num_E],'State'] = 'E'\n  df.loc[tochange[num_E:num_I+num_E],'State'] = 'I'\n  df.loc[tochange[num_E+num_I:num_E+num_I+num_R],'State'] = 'R'\n  return df\n\n\n\nBuilding the model\n\n#collapse_hide\ndef seir_model_stoch(beta, p, num_E, num_I, num_R, days, T_Latent, T_Infectious):\n\n    # Initialize population dataframe with data given by user\n    df = make_df(p,num_E, num_I, num_R)\n    \n    # This variable is used to track daily value of beta if it varies over time\n    xxbeta=np.array([],dtype=float)\n\n    # Initialize the arrays to return\n    # Below are numbers of S, E, I, R total\n    S=np.array([],dtype=int)\n    E=np.array([],dtype=int)\n    I=np.array([],dtype=int)\n    R=np.array([],dtype=int)\n    # Below are the daily additions in S, E, I, R\n    Spd=np.array([],dtype=int)\n    Epd=np.array([],dtype=int)\n    Ipd=np.array([],dtype=int)\n    Rpd=np.array([],dtype=int)\n\n    b=beta\n    \n    # Stochastic model so use random values to decide on progression\n    rand = np.random.random(size=(p,days))\n\n    # Depending if you want exponential or gamma distribution for T_Latent\n    if T_Latent == 'expon':\n      EtoI = expon.rvs(loc=0,scale=5.2,size=p)\n    else:\n      EtoI = gamma.rvs(1.8,loc=0.9,scale=(5.2-1.8)/0.9,size=p)\n\n    # Depending if you want exponential, gamma, or Weibull distribution for T_Infectious\n    # Uses distributions found on blog part 3\n    if T_Infectious == 'expon':\n      ItoR = expon.rvs(loc=0,scale=28.85,size=p)\n    elif T_Infectious == 'gamma':\n      ItoR = gamma.rvs(4,loc=3,scale=4.25,size=p)    \n    else:\n      ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p)\n\n    # Iterate over every day the simulation is run\n    for j in range(0,days-1):\n\n        # Record daily beta values\n        xxbeta=np.append(beta, b)\n\n        # First we get the index of the individuals that will change state today:\n\n        # Random number tells you which 'S' have been exposed on this day \n        StoE_index = df.loc[(df.State == 'S') & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/p)].index\n\n        # For each row, if a person has been a certain number of days in E, they will go to I\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        EtoI_index = df.loc[(df.State == 'E') & (j-df.Day &gt;= EtoI)].index\n        \n        # Similaraly as above\n        # For each row, if a person has been a certain number of days in I, they will go to R\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        ItoR_index = df.loc[(df.State == 'I') & (j-df.Day &gt;= ItoR)].index\n\n        # Use indexes collected above to populate per day values\n        Epd = np.append(Epd,len(StoE_index))\n        Ipd = np.append(Ipd,len(EtoI_index))\n        Rpd = np.append(Rpd,len(ItoR_index))\n\n        # Now we use the indexes collected above randomly to change the actual population dataframe to the new states\n        df.iloc[ItoR_index] = ['R', j]\n        df.iloc[EtoI_index] = ['I', j]\n        df.iloc[StoE_index] = ['E', j]\n        \n        # Append the S, E, I, and R arrays\n        S=np.append(S,len(np.where(df.State=='S')[0]))\n        E=np.append(E,len(np.where(df.State=='E')[0]))\n        I=np.append(I,len(np.where(df.State=='I')[0]))\n        R=np.append(R,len(np.where(df.State=='R')[0]))\n\n        # Code below for control measures to reduce beta values\n#        if ((I[-1] &gt; 1000) & (Ipd[-1] &gt; 399)): \n#            b = beta2\n#        elif ((I[-1] &gt; 1000) & (Ipd[-1] &lt; 400)): \n#            b = beta3\n                \n    Epd[0]+=num_E\n    Ipd[0]+=num_I\n    Rpd[0]+=num_R\n\n    return S,E,I,R, Epd, Ipd, Rpd, xxbeta\n\n\nSanity check\nLet’s first make sure the stochastic model above gives similar result to the deterministic model previously used in part 2 if we use an exponential distribution for \\(T_{Latent}\\) and \\(T_{Infectious}\\).\n\nE → I\nSo let’s first set all individuals to exposed on day 0 and see the progression to I with exponential and gamma distributions.\n\n#collapse_hide\n# Define parameters for stochastc model\ndays = 20\np = 10000\nnum_E = 10000\nnum_I = 0\nnum_R = 0\n\nbeta_stoch = 0.5*np.ones(days)\n\n# Comparing with previous deterministic model\ninit = 0, p, 0, 0\nsigma = 1/5.2   # 1/5 --&gt; 5 days on average to go from E --&gt; I\nbeta_det = 0.5\ngam = 1/28.85     # 1/11 --&gt; 11 days on average to go from I --&gt; R\nparms = sigma, beta_det, gam\n\n# Run deterministic simulation\nresults_avg = seir_model(init, parms, days)\n\n# Run stochastic simulation with exponential distribution\nresults_stoch_exp = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 'expon', 'expon')\n\n# Run stochastic simulation with gamma distribution\nresults_stoch_gam = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'expon')\n\n\n#collapse_hide\nfig = go.Figure(data=[       \n    go.Scatter(name='Exponential', x=np.arange(len(results_stoch_exp[0])), y=100*(1-results_stoch_exp[1]/p), line={'dash':'dash', 'color':'red'}),\n    go.Scatter(name='Gamma', x=np.arange(len(results_stoch_gam[0])), y=100*(1-results_stoch_gam[1]/p), line={'dash':'dash', 'color':'green'}),\n    go.Scatter(name='Deterministic', x=np.linspace(0,days,days*10), y=100*(1-results_avg.T[1]/p), line={'dash':'dot', 'color':'blue'}), \n])\n\nfig.update_layout(\n    title='Number of E moving to I over time when all population is exposed on day 0',\n    xaxis_title='Days',\n    yaxis_title='Percent of exposed having become infectious',\n    legend=dict(\n        x=1,\n        y=1,\n        traceorder=\"normal\",\n    )\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nSo we can see using the exponential distribution for \\(T_{Latent}\\) in our stochastic model very closely resembles the deterministic model from part 2.\nWe can see using the gamma distribution forces the behaviour of individual-level disease progression also.\n\n\nI → R\nNow let’s set all individuals to infectious on day 0 and see the progression to R with exponential, gamma, and Weibull distributions.\n\n#collapse_hide\n# Define parameters for stochastc model\ndays = 100\np = 10000\nnum_E = 0\nnum_I = 10000\nnum_R = 0\n\nbeta_stoch = 0.5*np.ones(days)\n\n# Comparing with previous average deterministic model\ninit = 0, 0, p, 0\nsigma = 1/5.2   # 1/5 --&gt; 5 days on average to go from E --&gt; I\nbeta_det = 0.5\ngam = 1/28.85     # 1/11 --&gt; 11 days on average to go from I --&gt; R\nparms = sigma, beta_det, gam\n\n# Run deterministic simulation\nresults_avg = seir_model(init, parms, days)\n\n# Run stochastic simulation with exponential distribution\nresults_stoch_exp = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'expon')\n\n# Run stochastic simulation with gamma distribution\nresults_stoch_gam = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'gamma')\n\n# Run stochastic simulation with gamma distribution\nresults_stoch_wei = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'weibull')\n\n\n#collapse_hide\nfig = go.Figure(data=[       \n    go.Scatter(name='Exponential', x=np.arange(len(results_stoch_exp[0])), y=100*(1-results_stoch_exp[2]/p), line={'dash':'dash', 'color':'red'}),\n    go.Scatter(name='Gamma', x=np.arange(len(results_stoch_gam[0])), y=100*(1-results_stoch_gam[2]/p), line={'dash':'dash', 'color':'green'}),\n    go.Scatter(name='Weibull', x=np.arange(len(results_stoch_wei[0])), y=100*(1-results_stoch_wei[2]/p), line={'dash':'dash', 'color':'orange'}),\n    go.Scatter(name='Deterministic', x=np.linspace(0,days,days*10), y=100*(1-results_avg.T[2]/p), line={'dash':'dot', 'color':'blue'}), \n])\n\nfig.update_layout(\n    title='Number of I moving to R over time when all population is infectious on day 0',\n    xaxis_title='Days',\n    yaxis_title='Percent of infectious having become recovered',\n    legend=dict(\n        x=1,\n        y=1,\n        traceorder=\"normal\",\n    )\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nSo we can see using the exponential distribution for \\(\\gamma\\) in our stochastic model very closely resembles the deterministic model from part 2.\nWe can see using the gamma or Weibull distributions forces the behaviour of individual-level disease progression also and results in a vastly different picture for progression from I → R."
  },
  {
    "objectID": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#comparing-deterministic-with-stochastic-seir-models",
    "href": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#comparing-deterministic-with-stochastic-seir-models",
    "title": "“Epidemic modeling - Part 4”",
    "section": "Comparing deterministic with stochastic SEIR models",
    "text": "Comparing deterministic with stochastic SEIR models\nNow that we know our model works, let’s quickly see the effect of stochasticity on the model.\nWe use the deterministic model from blog pat 2 as basis, and so the stochastic model here will use exponential distributions for \\(\\sigma\\) and \\(\\gamma\\).\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 200\np = 10000\nnum_E = 1\nnum_I = 0\nnum_R = 0\nbeta_stoch = 0.5*np.ones(days)\n\n# Define parameters for deterministic model\ninit = 1-(num_E/p)-(num_I/p)-(num_R/p), num_E/p, num_I/p, num_R/p\nsigma = 1/5.2   # 1/5 --&gt; 5 days on average to go from E --&gt; I\nbeta_det = 0.5\ngam = 1/28.85     # 1/11 --&gt; 11 days on average to go from I --&gt; R\nparms = sigma, beta_det, gam\n\n# Run deterministic simulation\nresults_avg = seir_model(init, parms, days)\n\n# Run 3 stochastic simulations\nresults_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 'expon', 'expon')\nresults_stoch2 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 'expon', 'expon')\nresults_stoch3 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 'expon', 'expon')\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='S_det', x=np.linspace(0,days,days*10), y=results_avg.T[0], line={'dash':'solid', 'color':'blue'}, legendgroup=\"det\"),\n    go.Scatter(name='E_det', x=np.linspace(0,days,days*10), y=results_avg.T[1], line={'dash':'solid', 'color':'yellow'}, legendgroup=\"det\"), \n    go.Scatter(name='I_det', x=np.linspace(0,days,days*10), y=results_avg.T[2], line={'dash':'solid', 'color':'red'}, legendgroup=\"det\"),\n    go.Scatter(name='R_det', x=np.linspace(0,days,days*10), y=results_avg.T[3], line={'dash':'solid', 'color':'green'}, legendgroup=\"det\"),\n    go.Scatter(name='S_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='E_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='I_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='R_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='S_stoch2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='E_stoch2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='I_stoch2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='R_stoch2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='S_stoch3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='E_stoch3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={'dash':'dot', 'color':'yellow'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='I_stoch3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='R_stoch3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch3\")\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of stochasticity on Deterministic SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nWe can see very similar curves. The stochasticity appears to influence the time at which the epidemic starts but not the shape of the curves."
  },
  {
    "objectID": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#sigma-exponential-or-gamma-distribution",
    "href": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#sigma-exponential-or-gamma-distribution",
    "title": "“Epidemic modeling - Part 4”",
    "section": "\\(\\sigma\\): exponential or gamma distribution",
    "text": "\\(\\sigma\\): exponential or gamma distribution\nIn this section we want to examine the effect of a gamma distribution has on the SEIR model (we keep exponential distribution for \\(\\gamma\\)).\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 200\np = 10000\nnum_E = 1\nnum_I = 0\nnum_R = 0\nbeta_stoch = 0.5*np.ones(days)\n\n# Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigma\nresults_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 'expon', 'expon')\nresults_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 'expon', 'expon')\nresults_stoch2 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'expon')\nresults_stoch3 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'expon')\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='S_stoch_exp1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[0]/p, line={'dash':'solid', 'color':'blue'}, legendgroup=\"det\"),\n    go.Scatter(name='E_stoch_exp1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[1]/p, line={'dash':'solid', 'color':'yellow'}, legendgroup=\"det\"), \n    go.Scatter(name='I_stoch_exp1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p, line={'dash':'solid', 'color':'red'}, legendgroup=\"det\"),\n    go.Scatter(name='R_stoch_exp1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[3]/p, line={'dash':'solid', 'color':'green'}, legendgroup=\"det\"),\n    go.Scatter(name='S_stoch_exp2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[0]/p, line={'dash':'solid', 'color':'blue'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='E_stoch_exp2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={'dash':'solid','color':'yellow'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='I_stoch_exp2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={'dash':'solid', 'color':'red'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='R_stoch_exp2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[3]/p, line={'dash':'solid', 'color':'green'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='S_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='E_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='I_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='R_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='S_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='E_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={'dash':'dot', 'color':'yellow'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='I_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='R_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch3\")\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of gamma vs. exponential distributed } \\sigma \\text{ on SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nAs you can see here, it is difficult to tell how much the gamma distributed \\(\\sigma\\) differs from the exponential distributed model (other than just timing).\nThe infectious peak might be a little lower and delayed a bit with gama distribution, but it is hard to tell for sure from this.\nThe peak of exposed individuals seems to be a bit higher and delayed with gamma distribution versus exponential distribution."
  },
  {
    "objectID": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#gamma-exponential-gamma-or-weibull-distribution",
    "href": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#gamma-exponential-gamma-or-weibull-distribution",
    "title": "“Epidemic modeling - Part 4”",
    "section": "\\(\\gamma\\): exponential, gamma, or Weibull distribution",
    "text": "\\(\\gamma\\): exponential, gamma, or Weibull distribution\nIn this section we want to examine the effect of having \\(T_{Infectious}\\) be gamma or Weibull distribution on the SEIR model.\n\nExponential vs. Gamma\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 200\np = 10000\nnum_E = 1\nnum_I = 0\nnum_R = 0\nbeta_stoch = 0.5*np.ones(days)\n\n# Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigma\nresults_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'expon')\nresults_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'expon')\nresults_stoch2 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'gamma')\nresults_stoch3 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'gamma')\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='S_stoch_exp1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[0]/p, line={'dash':'solid', 'color':'blue'}, legendgroup=\"det\"),\n    go.Scatter(name='E_stoch_exp1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[1]/p, line={'dash':'solid', 'color':'yellow'}, legendgroup=\"det\"), \n    go.Scatter(name='I_stoch_exp1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p, line={'dash':'solid', 'color':'red'}, legendgroup=\"det\"),\n    go.Scatter(name='R_stoch_exp1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[3]/p, line={'dash':'solid', 'color':'green'}, legendgroup=\"det\"),\n    go.Scatter(name='S_stoch_exp2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[0]/p, line={'dash':'solid', 'color':'blue'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='E_stoch_exp2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={'dash':'solid','color':'yellow'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='I_stoch_exp2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={'dash':'solid', 'color':'red'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='R_stoch_exp2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[3]/p, line={'dash':'solid', 'color':'green'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='S_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='E_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='I_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='R_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='S_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='E_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={'dash':'dot', 'color':'yellow'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='I_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='R_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch3\")\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of gamma vs. exponential distributed } \\gamma \\text{ on SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nAs you can see here, it is a lot easier to differentiate between the two.\nA gamma distributed \\(\\gamma\\) results in a higher peak of infectious people and underlines how using the usual deterministic models can vastly underestimate peak infectious people.\n\n\nGamma vs. Weibull\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 200\np = 10000\nnum_E = 1\nnum_I = 0\nnum_R = 0\nbeta_stoch = 0.5*np.ones(days)\n\n# Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigma\nresults_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'weibull')\nresults_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'weibull')\nresults_stoch2 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'gamma')\nresults_stoch3 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'gamma')\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='S_stoch_wei1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[0]/p, line={'dash':'solid', 'color':'blue'}, legendgroup=\"det\"),\n    go.Scatter(name='E_stoch_wei1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[1]/p, line={'dash':'solid', 'color':'yellow'}, legendgroup=\"det\"), \n    go.Scatter(name='I_stoch_wei1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p, line={'dash':'solid', 'color':'red'}, legendgroup=\"det\"),\n    go.Scatter(name='R_stoch_wei1', x=np.arange(len(results_stoch0[0])), y=results_stoch0[3]/p, line={'dash':'solid', 'color':'green'}, legendgroup=\"det\"),\n    go.Scatter(name='S_stoch_wei2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[0]/p, line={'dash':'solid', 'color':'blue'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='E_stoch_wei2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={'dash':'solid','color':'yellow'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='I_stoch_wei2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={'dash':'solid', 'color':'red'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='R_stoch_wei2', x=np.arange(len(results_stoch1[0])), y=results_stoch1[3]/p, line={'dash':'solid', 'color':'green'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='S_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='E_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='I_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='R_stoch_gam1', x=np.arange(len(results_stoch2[0])), y=results_stoch2[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='S_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[0]/p, line={'dash':'dot', 'color':'blue'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='E_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={'dash':'dot', 'color':'yellow'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='I_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='R_stoch_gam2', x=np.arange(len(results_stoch3[0])), y=results_stoch3[3]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch3\")\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of Weibull vs. gamma distributed } \\gamma \\text{ on SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nOverall both the gamma and Weibull distributions were very close to the actual distribution for COVID-19 \\(T_{Infectious}\\) so it makes sense that the simulations results in similar curbs here."
  },
  {
    "objectID": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#impact-of-distribution-of-t_infectious-on-infectious-peak",
    "href": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#impact-of-distribution-of-t_infectious-on-infectious-peak",
    "title": "“Epidemic modeling - Part 4”",
    "section": "Impact of distribution of \\(T_{Infectious}\\) on Infectious Peak",
    "text": "Impact of distribution of \\(T_{Infectious}\\) on Infectious Peak\nIn the plots above we can see the peak of infectious individuals is higher in the simulations done with Gamma or Weibull distributions than in those done with the exponential distribution.\nNote we have not changed anything for \\(\\beta\\) and in the simulations above we have the following: * Exponential distribution: \\[E[T_{Infectious}] = 28.85\\ days\\] \\[R_0 = \\beta * E[T_{Infectious}] = 14.43\\] * Gamma distribution: \\[E[T_{Infectious}] = 20.05\\ days\\] \\[R_0 = \\beta * E[T_{Infectious}] = 10.03\\] * Weibull distribution: \\[E[T_{Infectious}] = 20.77\\ days\\] \\[R_0 = \\beta * E[T_{Infectious}] = 10.39\\]\nSo while we have a higher \\(R_0\\) when using the exonential distribution for \\(T_{Infectious}\\), the peak of infectious individuals is lower than in the simulations using gamma and Weibull distributions with lower \\(R_0\\).\nWe had previously seen that increasing \\(R_0\\) resulted in high infectious peaks, but this is only true when comparing similar distributions."
  },
  {
    "objectID": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#discussion",
    "href": "posts/2020-03-31-stochastic_model/2020-03-31-stochastic_model.html#discussion",
    "title": "“Epidemic modeling - Part 4”",
    "section": "Discussion",
    "text": "Discussion\nWe can see the actual distribution of \\(\\sigma\\) and \\(\\gamma\\) carry importance in the resulting SEIR models.\n\n\\(R_0\\)\nIn part 1 we saw that \\(R_0\\) was fully characterized by \\(\\beta\\) and \\(\\gamma\\) in the sense that \\[R_0 = \\frac{\\beta}{\\gamma}\\]\nWe can clearly see here however that \\(R_0\\) is not a good enough measure the indicate peak infectious individuals - which is closely related to the peak number of sick individuals which in turn determines required sanitary resources.\nThe actual distribution of \\(T_{Infectious}\\) mus tbe taken into account to estimate true values of peaks.\n\n\nFurther questions\nA couple questions are left to be answered: * How can we control the spread of an epidemic? * How can we evaluate \\(\\beta\\) from the data collected on a population level?\nSee further blog posts."
  },
  {
    "objectID": "posts/2021_05_05_stochastic_HIV_model/2021_05_05_stochastic_HIV_model.html#building-a-stochastic-model",
    "href": "posts/2021_05_05_stochastic_HIV_model/2021_05_05_stochastic_HIV_model.html#building-a-stochastic-model",
    "title": "“HIV modeling”",
    "section": "Building a stochastic model",
    "text": "Building a stochastic model\nThis model is going to focus on individual level disease dynamics to model the disease propagation.\nIt models DHS dataset which contains a homogeneous population between 15 and 49 years old.\nThe basic idea of this model is to have a dataframe with the number of rows equal to the population size (each individual is a row) and two columns: * State column to describe the state of each individual (S, I, or D) * Year column to save the day of transition of the individual into that state * Age column to know the age of the individuals\nHowever, the population-level rates of transmission still apply here i.e. a person goes from S → I following two points: 1. the effective contact rate \\(\\beta\\), which is itself given by: - the number of contacts the person has per unit time (given by \\(r\\)) - the chance of an S contracting the disease from a contact with an I (given by \\(\\rho\\)) 2. the chance a given contact is with an I - infectious individual (the higher the number of I, the higher the chance)\nThis is done stochastically.\nOnce a person becomes I, their progression is unique to them. This progression is calculated in advance for computational reason, but it allows to use the time ditributions we want.\n\n#collapse_hide\n!pip install plotly==4.14.3\nimport pandas as pd\nimport numpy as np\nimport math\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import expon\nfrom scipy.stats import gamma\nfrom scipy.stats import weibull_min\nfrom numpy.random import default_rng\nrng = default_rng()\nimport tqdm\nimport time\n\nCollecting plotly==4.14.3\n  Downloading https://files.pythonhosted.org/packages/1f/f6/bd3c17c8003b6641df1228e80e1acac97ed8402635e46c2571f8e1ef63af/plotly-4.14.3-py2.py3-none-any.whl (13.2MB)\n     |████████████████████████████████| 13.2MB 254kB/s \nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly==4.14.3) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==4.14.3) (1.15.0)\nInstalling collected packages: plotly\n  Found existing installation: plotly 4.4.1\n    Uninstalling plotly-4.4.1:\n      Successfully uninstalled plotly-4.4.1\nSuccessfully installed plotly-4.14.3\n\n\n\nCreating the initial population dataframe\nBelow is a function to create the initial population dataframe: * \\(p\\) is the population number * \\(num_I\\) is the number of infectious on day 0 * \\(num_R\\) is the number of people recovered on day 0\n\n#collapse_hide\n# Need this new function for model below:\ndef make_df(p, num_I, num_R):\n  df = pd.DataFrame(np.full((p,1), 'S').T[0], columns=['State'])\n  df['Year'] = 0\n  df['Age'] = (np.random.random(p)*35+15).astype(int)\n  tochange=df.loc[rng.choice(p, size=num_I+num_R, replace=False),'State'].index\n  df.loc[tochange[0:num_I],'State'] = 'I'\n  df.loc[tochange[num_I:num_I+num_R],'State'] = 'R'\n  return df\n\n\n\nBuilding the model\n\n# Modelling the decrease of beta over time\n\n#np.random.random(size=(p,days))\n#np.log(4)\nj=12\nover = 10\n\n#10/np.cumsum(np.ones(100))\n\nb1 = 0.25  # original beta = beta value before epidemic\nb2 = 0.05 # end beta = beta at the end of epidemic\nb2 + (b1/np.exp((j+(b1*2.9)-over+1)/(b1*27)))\n\n0.19397059336956546\n\n\n\n8#collapse_hide\ndef seir_model_stoch(beta, beta2, p, num_I, num_R, years, T_Infectious, ART, control):\n\n    ################################\n    #### Explanation of inputs  ####\n    ################################\n\n    #### As seen in SSA, beta has a starting value, but after a certain threshold (as soon as incidence or prevalence reaches a certain threshold) behaviours change and beta decreases\n    # beta is initial value of beta at start of epidemic (usually 0.3, but can range from 0.2 to 0.5 as seen in SSA)\n    # beta2 is final value (usually around 0.05)\n    # p is total number of individuals in population\n    # num_I is initial number of PLWHA in population (for simulations start with something between 1 and 10 depending on size of p)\n    # num_R is initial number of people deceased from HIV/AIDS\n    # years is number of years you want to run simulation for\n    # T_infectious is distribution of progression of HIV in an individual (use 'gamma' for HIV)\n    # ART is to emulate ART usage:\n        # ART == 0 means no ART\n        # ART == 1 means ART stops evolution of I to R but does not stop spread from I to S\n        # ART == 2 means ART stops both I to R, and S to I\n    # control sets the threshold at which beta above will decrease\n        # control == 0 means no control i.e. beta never decreases\n        # control == 1 means beta decreases once incidence is 15 per 1 thousand population\n        # control == 2 means beta decreases once incidence is 30 per 1 thousand population\n\n\n    ################################\n    ##### Set up the dataframe #####\n    ################################\n\n    # Initialize population dataframe with data given by user\n    df = make_df(p, num_I, num_R)\n    \n    # This variable is used to track daily value of beta if it varies over time\n    xxbeta=np.array([],dtype=float)\n\n    # Initialize the arrays to return\n    # Below are numbers of S, I, R total\n    S=np.array([],dtype=int)\n    I=np.array([],dtype=int)\n    R=np.array([],dtype=int)\n    # Below are the daily additions in S, I, R\n    Spd=np.array([],dtype=int)\n    Ipd=np.array([],dtype=int)\n    Rpd=np.array([],dtype=int)\n\n    # Beta values to track spread\n    b=beta\n    b2=np.array([],dtype=float)\n    b1=b\n\n    # Signal to initiate decrease of beta\n    over = 0 \n\n    # signal to end transmission and deaths due to ART\n    art1 = 0\n    art2 = 0\n    \n    # Stochastic model so use random values to decide on progression\n    rand = np.random.random(size=(p,years))\n\n    # Depending if you want exponential, gamma, or Weibull distribution for T_Infectious\n    # Uses distributions found on blog part 3\n    if T_Infectious == 'expon':\n      ItoR = expon.rvs(loc=0,scale=10,size=p)\n    elif T_Infectious == 'gamma':\n      ItoR = gamma.rvs(4,loc=3,scale=2,size=p)    \n    else:\n      ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p)\n\n\n    ################################\n    ####### Simulation code ########\n    ################################\n\n    # Iterate over every day the simulation is run\n    for j in range(0,years-1):\n\n        # Record daily beta values\n        xxbeta=np.append(xxbeta, b[j])\n\n        # First we get the index of the individuals that will change state today:\n\n        # Random number tells you which 'S' have been exposed on this day \n        if ART &lt; 2:\n          StoI_index = df.loc[(df.State == 'S') & (df.Age &lt; 49) & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/(len(np.where(df.State=='I')[0])+len(np.where(df.State=='S')[0])))].index\n          StoS_index = df.loc[(df.State == 'S') & (df.Age &lt; 49) & (rand[:,j] &gt;= b[j]*len(np.where(df.State=='I')[0])/(len(np.where(df.State=='I')[0])+len(np.where(df.State=='S')[0])))].index\n        elif ART == 2: \n          if art2 == 0:\n            StoI_index = df.loc[(df.State == 'S') & (df.Age &lt; 49) & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/(len(np.where(df.State=='I')[0])+len(np.where(df.State=='S')[0])))].index\n            StoS_index = df.loc[(df.State == 'S') & (df.Age &lt; 49) & (rand[:,j] &gt;= b[j]*len(np.where(df.State=='I')[0])/(len(np.where(df.State=='I')[0])+len(np.where(df.State=='S')[0])))].index\n          elif art2 == 1:\n            StoI_index = df.loc[(df.State == 'S') & (df.Age &gt; 55)].index # cannot happen so put an impossible condition like df.Age &gt; 55 to emulate\n            StoS_index = df.loc[(df.State == 'S') & (df.Age &lt; 49)].index # anyone S under 49 will stay S\n\n        StoRem_index = df.loc[(df.State == 'S') & (df.Age &gt;= 49)].index\n        \n        # For each row, if a person has been a certain number of years in I, they will go to R (progression to AIDS and death)\n        # This follows ItoR variable which is either exponential or gamma distributed according to above\n        ItoRem_index = df.loc[(df.State == 'I') & (df.Age &gt;= 49)].index\n        if ART == 0: #don't use ART\n          ItoR_index = df.loc[(df.State == 'I') & (j-df.Year &gt;= ItoR) & (df.Age &lt; 49)].index\n          ItoI_index = df.loc[(df.State == 'I') & (j-df.Year &lt; ItoR) & (df.Age &lt; 49)].index\n        elif ART &gt; 0:\n          if art2 == 0:\n            ItoR_index = df.loc[(df.State == 'I') & (j-df.Year &gt;= ItoR) & (df.Age &lt; 49)].index\n            ItoI_index = df.loc[(df.State == 'I') & (j-df.Year &lt; ItoR) & (df.Age &lt; 49)].index\n          elif art2 ==1:\n            ItoR_index = df.loc[(df.State == 'I') & (df.Age &gt; 55)].index # cannot happen so impossible condition\n            ItoI_index = df.loc[(df.State == 'I') & (df.Age &lt; 49)].index            \n\n        RtoRem_index = df.loc[(df.State == 'R') & (df.Age &gt;= 49)].index\n\n        RtoR_index = df.loc[(df.State == 'R') & (df.Age &lt; 49)].index\n\n        # Use indexes collected above to populate per day values\n        Ipd = np.append(Ipd,len(StoI_index))\n        Rpd = np.append(Rpd,len(ItoR_index))\n\n        # Now we use the indexes collected above randomly to change the actual population dataframe to the new states\n        df.loc[ItoR_index, ['State','Year']] = ['S', j]\n        df.loc[ItoR_index, 'Age'] = df.loc[ItoR_index, 'Age'] + 1\n        df.loc[ItoI_index, 'Age'] = df.loc[ItoI_index, 'Age'] + 1\n        \n        df.loc[StoI_index, ['State','Year']] = ['I', j]\n        df.loc[StoI_index, 'Age'] = df.loc[StoI_index, 'Age'] + 1\n        df.loc[StoS_index, 'Age'] = df.loc[StoS_index, 'Age'] + 1\n        \n        df.loc[RtoR_index, 'Age'] = df.loc[RtoR_index, 'Age'] + 1\n\n        df.iloc[ItoRem_index] = ['S', j, 15]\n        df.iloc[StoRem_index] = ['S', j, 15]\n        df.iloc[RtoRem_index] = ['S', j, 15]\n\n        # Append the S, I, and R arrays\n        S=np.append(S,len(np.where(df.State=='S')[0]))\n        I=np.append(I,len(np.where(df.State=='I')[0]))\n        R=np.append(R,len(np.where(df.State=='R')[0]))\n\n        # Code below for control measures to reduce beta values\n        if control == 1:\n          if (I[-1]/p &gt; 0.006):\n            art1 = 1\n            if over == 0:\n              over = j\n          \n          if art1 == 1:\n            if j &gt; over + 15:    \n              art2 = 1\n\n          if over != 0:\n            b = beta2 + (b1/np.exp((j+(b1*2.9)-over+1)/(b1*27)))\n\n        if control == 2:\n          if (I[-1]/p &gt; 0.01):\n            art1 = 1\n            if over == 0:\n              over = j\n          \n          if art1 == 1:\n            if j &gt; over + 15:    \n              art2 = 1\n\n          if over != 0:\n            b = beta2 + (b1/np.exp((j+(b1*2.9)-over+1)/(b1*27)))\n\n\n        xxbeta2 = ((S[j-1]+I[j-1])/I[j-1])*Ipd[j]/S[j-1]\n        b2 = np.append(b2, xxbeta2)\n                \n    Ipd[0]+=num_I\n    Rpd[0]+=num_R\n\n    return S, I, R, Spd, Ipd, Rpd, xxbeta, b2, over"
  },
  {
    "objectID": "posts/2021_05_05_stochastic_HIV_model/2021_05_05_stochastic_HIV_model.html#testing-the-model",
    "href": "posts/2021_05_05_stochastic_HIV_model/2021_05_05_stochastic_HIV_model.html#testing-the-model",
    "title": "“HIV modeling”",
    "section": "Testing the model",
    "text": "Testing the model\n\n#collapse_hide\n# Define parameters for stochastic model\nyears = 50\np = 100000\nnum_E = 0\nnum_I = 50\nnum_R = 0\nbeta_stoch = [0.17,0.17,0.26,0.26,0.36,0.36]\n#beta_stoch = np.linspace(0.2,0.5,num=10)\n#beta_stoch = [0.1,0.1,0.1,0.1,0.1,0.1]\nbeta_stoch2 = [0.05,0.05,0.05,0.05,0.05,0.05]\n#beta_stoch = beta_stoch2\n\ncontrol= [1,1,1,1,2,2]\n\nn = len(beta_stoch)\n\n#results_stoch = []\n\n# Run n stochastic simulations\nfor i in tqdm.tqdm(range(n)):\n  res = seir_model_stoch(beta_stoch[i]*np.ones(years),beta_stoch2[i], p, num_I, num_R, years, 'gamma', 0, control[i])\n  results_stoch.append(res)\n\n100%|██████████| 6/6 [00:41&lt;00:00,  6.95s/it]\n\n\n\n#collapse_hide\nfig = go.Figure()\n\nfor i in range(len(results_stoch)):\n  #fig.add_trace(go.Scatter(name='Beta_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[6], line={'dash':'dot','color':'yellow'}, legendgroup=\"Sim_\"+str(i)))\n  fig.add_trace(go.Scatter(name='Beta_meas'+str(i), x=np.arange(len(results_stoch[i][0])), y=results_stoch[i][7], line={'dash':'dot','color':'yellow'}, legendgroup=\"Sim_\"+str(i)))\n  fig.add_trace(go.Scatter(name='I_stoch'+str(i), x=np.arange(len(results_stoch[i][0])), y=results_stoch[i][1]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"Sim_\"+str(i)))\n  fig.add_trace(go.Bar(name='Ip_stoch'+str(i), x=np.arange(len(results_stoch[i][0])), y=results_stoch[i][4]*10/p, legendgroup=\"Sim_\"+str(i)))\n  fig.add_trace(go.Scatter(name='R_stoch'+str(i), x=np.arange(len(results_stoch[i][0])), y=results_stoch[i][2]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"Sim_\"+str(i)))\n\nfig.update_layout(\n    xaxis_title = 'Years',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Stochastic HIV SIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html",
    "href": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html",
    "title": "Epidemic modeling - Part 1",
    "section": "",
    "text": "This is the 1st part of a multi-part series blog post on modeling in epidemiology.\nThe COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on a few important points.\nIn this first post I want to introduce the concept of compartmentalization and how it forms the basis for studying disease dynamics on the population level."
  },
  {
    "objectID": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#motivation-for-write-up",
    "href": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#motivation-for-write-up",
    "title": "Epidemic modeling - Part 1",
    "section": "",
    "text": "This is the 1st part of a multi-part series blog post on modeling in epidemiology.\nThe COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on a few important points.\nIn this first post I want to introduce the concept of compartmentalization and how it forms the basis for studying disease dynamics on the population level."
  },
  {
    "objectID": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#how-to-model-infectious-diseases-on-population-level",
    "href": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#how-to-model-infectious-diseases-on-population-level",
    "title": "Epidemic modeling - Part 1",
    "section": "How to model infectious diseases on population level ?",
    "text": "How to model infectious diseases on population level ?\n\nCompartments\nWhen modelling infectious diseases, and pandemics in particular, a key ask is to predict the number of infected people at any given time in order to estimate the sanitary resources that will be necessary.\nFrom this simple qestion results the idea of compartmentalization of the population i.e. the division of the population into the two most basic categories:\n\nthose that are infected\nthose that are not\n\nThis is ultimately the foundation for all compartmental models in epidemiology.\nThe nuances between the models then come from how the above two groups are further compartmentalized. That is to say, how we decide the composition of the infected and the not-infected groups.\nFor example, the non-infected group could be further sub-categorized into:\n\nSusceptible\nImmune\n\nAnd the infected group into:\n\nAsymptomatic\nSymptomatic\n\nOr, another option, into:\n\nNo treatment necessary\nRequire treatment:\n\nLocal Doctor visit\nHospitalization\nAdmitted to intensive care unit\n\n\nAs you can see there are many ways to do this, but the more categories you have, the more difficult it might become to model. Usually we determine these subcategories in order to match available data.\n\n\nDynamics\nWhile the compartments describe the state any individual can be in at a certain point in time, the dynamics describe the ways in which the compartments interact with each other.\nI want to underline the separation between disease dynamics on the individual level, and that on the population level below.\n\nIndividual level disease dynamics:\n\nThis describes, on the individual level, the progression of the disease i.e. how one person can go from one state to another (one compartment to another)\nFor example: how does a healthy person become ill and what is the clinical course of the disease for this person?\n\nPopulation-level dynamics:\n\nOn the other hand, the population level dynamics describe, on a population level, how the total number of individuals in each compartment vary over time.\nWe will see more on this in the next blog posts.\n\n\nTwo simple examples - the SIR and SEIR models\nLet’s have a look at a basic compartmental model, first the SIR model.\n\nS –&gt; Susceptible state:\n\nAn S individual is simply someone susceptible to the disease, that is anyone in the population who is healthy and not immune to the disease.\n\nI –&gt; Infectious state:\n\nOnce an individual is exposed to the disease he will develop this disease and become infectious.\n\nR –&gt; Recovered state:\n\nAn individual will either fight off the infection (with the help or not of treatment) or die. These are all included in the R state.\nIn the basic SIR model, anyone R has aquired full and infinite immunity and cannot catch the disease again (of course many variations can be included to reflect more closely a disease).\nIn this write-up and in the following blog posts we will focus on the SEIR models, which are similar to the SIR compartments above with the additional E state between S and I.\n\nE –&gt; Exposed state:\n\nThe exposed state is the state when an individual has been exposed to the disease, but has not become infectious yet.\n\n\nSome important vocabulary\n\n\\(\\underline{Infectious\\ period:}\\)\n\nAlso called the period of communicability, the infectious period is the time during which an individual can transmit the disease to another: \\(T_{Infectious}\\)\n\n\\(\\underline{Clinical\\ infection\\ period:}\\)\n\nThis period corresponds to the period where the infected indvidual shows symptoms: \\(T_{Clinical}\\)\n\n\\(\\underline{Latent\\ period:}\\)\n\nThe latent period is the time between exposure of an individual and the start of the period of communicability of that individual: \\(T_{Latent}\\)\n\n\\(\\underline{Incubation\\ period:}\\)\n\nThe incubation period on the other hand, is the time from exposure of an individual to development of the infection (appearance of disease): \\(T_{Incubation}\\)\nIt should be noted the latent period and incubation period are not necessarily the same.\n\n\\(\\underline{T_{Latent} &lt; T_{Incubation}}\\):\n\nIn this case, an individual who has been exposed becomes infectious before the development of disease.\nWe call this a subclinical infection and during that time the individual is called an asymptomatic carrier.\n\n\\(\\underline{T_{Latent} &gt; T_{Incubation}:}\\)\n\nIn other cases, the latent period can be longer than the incubation period, eg: smallpox.\n\n\\(\\underline{T_{Latent} + T_{Infectious} &gt; T_{Incubation} + T_{Clinical}:}\\)\n\nAnother case of subclinical infection resulting in asymptomatic carriers occurs when the end of clinical infection (of disease) happens earlier than the end of the period of communicability (see Wikipedia figure below)\nOverall, these asymptomatic carriers can be a significant difficutly to overcome epidemics.\n\n\n\\(\\underline{Basic\\ reproduction\\ number:}\\)\n\nThe basic reproduction number \\(R_0\\) is the measure of secondary infections in a susceptible population.\nIn other words, it is the number of people that each infectious individual will infect over the time of their infectious period.\nExample:\nIf an infectious individual infects 3 other individuals over the course of his infection, his \\(R_0\\) is 3.\nThis number is a very important element in the spreading dynamics (see derivation below)."
  },
  {
    "objectID": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#a-closer-look-at-the-seir-model",
    "href": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#a-closer-look-at-the-seir-model",
    "title": "Epidemic modeling - Part 1",
    "section": "A closer look at the SEIR model",
    "text": "A closer look at the SEIR model\n\nIndividual-level disease dynamic\nAs explained above, the individual-level disease dynamic describes the progression of disease within an individual i.e. the progression of an individual from one state to another.\n\nIn the models used here (SEIR model), an individual starts at S (although an initial exposed or infectious person is injected into the population at time t=0).\nIf exposed to the disease he will move into the state E.\nAfter which he will move to the I state with probability 1, but in a time unique to himself.\nAgain after which he will move into the state R with probability 1, and again in a time unique to him.\nFrom state R he will stay in state R (either dead or has aquired full and inifite immunity).\n\nLet’s have a closer look:\n\nS → E\nThe chances of an individual going from S → E depends on three things:\n\nthe number of contacts the person has per unit time (given by \\(r\\))\nthe chance a given contact is with an I - infectious individual (the higher the number of I in the population, the higher the chance)\nthe chance of an S contracting the disease from a contact with an I (given by \\(\\rho\\))\n\n\n\nE → I\nThe latent period\nAll people exposed will eventually develop disease.\nHowever, individually, a person might go from E to I on the first day, or after 10 days, this is unique to the individual.\nEvery additional day following exposure the probability of this individual to go from E → I increases (we will have a look at the probability distribution and its importance later).\n\n\nI → R\nThe period of communicability\nSimilarly, all infectious people will recover (or die).\nAgain, individually, a person might go from I to R in 5 days or in 15 days, this time is the recovery time and is proper to the individual.\n\n\n\nPopulation level dynamics\nMost basic models tend to disregard the notion of individual dynamics above in favor of poopulation level dynamics.\nThat is to say the models tend to model disease on a population level without looking at the specific pogression of disease within the individuals and using averages instead (although the S → E uses the same logic as above).\nBelow is an explanation for such an SEIR model with its mathematical formulation.\nNote no births or deaths are included.\n\nS → E\nAs stated above, going from S to E on a particular day depends on these three characteristics:\n\nthe proportion of infectious people in the population on that day: \\(i(t) = \\frac{I(t)}{N}\\)\nthe number of contacts an individual has per day: \\(r\\)\nthe chance for an S to contract the disease after contact with an I: \\(\\rho\\)\n\nWe can combine the last two into \\(\\beta = r \\rho\\)\nOn a population-level however, the number of S that will become E also depends on the proportion of S in the population (of course if there are no S, no one will become E of course).\nSo we add the following requirement:\n\nthe proportion of susceptible people in the population on that day: \\(s(t) = \\frac{S(t)}{N}\\)\n\nSo the change in the number of S in a population on a given day is equal to:\n\\[ - \\beta i(t) s(t)\\]\n(note the negative sign to indicate the number of S is diminishing as they become exposed)\nHence we can formulate this mathematically as follows:\n\nDiscrete-time: \\[\\Delta S = -\\beta I S\\Delta T\\]\nContinuous-time: \\[\\frac{ds(t)}{dt}=-\\beta i(t) s(t)\\]\n\n\n\nE → I\nWe have seen above how each individual goes from E to I.\nOn a population level, the number of E changes in two ways:\n\nnew additions following S → E\nreduction following E → I\n\nWe already know the number from S → E is:\n\\[\\beta i(t) s(t)\\]\nSo how can we model the number of E → I?\nWhile individually this is a bit more complicated to model and pertains to the specific probability distribution of the latent period, on a population level we can use the average time it takes - this is what most models do (part 3 of this blog post will show why this is wrong for COVID-19).\nLet’s say average latent period is\n\\[\\frac{1}{\\sigma}\\]\nthen we know that every unit time that goes by, we have\n\\[\\sigma E\\]\nindividuals that transition from E → I.\nMathematically, we write this as :\n\nDiscrete-time: \\[\\Delta E = (\\beta I S-\\sigma E) \\Delta T\\]\nContinuous-time: \\[\\frac{de(t)}{dt}=\\beta i(t) s(t) - \\sigma e(t)\\]\n\n\n\nI → R\nSimilarly as above, we have seen above how each individual goes from I to R but this does not tell us about the population level dynamics.\nOn a population level, the number of I changes in two ways:\n\nnew additions following E → I\nreduction following I → R\n\nWe know the number from E → I is:\n\\[\\sigma e(t)\\]\nHow can we model the number of I → R?\nAgain, while individually this is complicated, on a population level, how about averaging out the period of infectiousness, this is what most models do.\nLet’s say average time of infectiousness is\n\\[\\frac{1}{\\gamma}\\]\nThen we have :\n\nDiscrete-time: \\[\\Delta I = (\\sigma E - \\gamma I) \\Delta T\\]\nContinuous-time: \\[\\frac{di(t)}{dt}=\\sigma e(t) - \\gamma i(t)\\]\n\n\n\nR → R\nFinally, it is simple to model the number of individuals in R state with the following equation:\n\nDiscrete-time: \\[\\Delta R = \\gamma I \\Delta T\\]\nContinuous-time: \\[\\frac{dr(t)}{dt}=\\gamma i(t)\\]"
  },
  {
    "objectID": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#effective-and-basic-reproduction-numbers-r-and-r_0-respectively",
    "href": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#effective-and-basic-reproduction-numbers-r-and-r_0-respectively",
    "title": "Epidemic modeling - Part 1",
    "section": "Effective and Basic Reproduction Numbers: \\(R\\) and \\(R_0\\) respectively",
    "text": "Effective and Basic Reproduction Numbers: \\(R\\) and \\(R_0\\) respectively\nAs stated above, \\(R_0\\) is the measure of secondary infections. Let’s have a look how we can characterize it.\n\nUnderstanding how the infection spreads\nAny individual in state I (infectious) will contaminate others according to the following:\n\nNumber of contacts the individual has per day given by: \\(r_i\\)\nProbability to infect an S after contact given by: \\(\\rho_i\\)\nProbability of a contact being with an S given by: \\(\\frac{S(t)}{N} = s(t)\\)\nThe period of infectiousness of the individual given by [\\(j_i, j_i+\\frac{1}{\\tau_i}\\)] (where \\(j_i\\) is the first day of infectiousness for that individual and \\(\\frac{1}{\\tau_i}\\) is that individuals’ time of infectiousness)\n\nRemember \\(r_i\\rho_i=\\beta_i\\)\n\n\nDerivation of \\(R\\) for each individual\nLet’s call the measure of \\(R\\) for any individual \\(R_i\\).\nFrom the parameters above we can write \\(R_i\\) for each infectious individual as the sum of secondary infections per day of infectiousness as below:\n\nDiscrete-time: \\[R_i = \\sum_{Day=j_i}^{j_i+\\frac{1}{\\tau_i}} \\beta_i \\frac{S(Day)}{N}\\] *Continuous-time: \\[R_i = \\int_{j_i}^{j_i+\\frac{1}{\\tau_i}} \\beta_i s(t) dt\\]\n\n\n\nFinding \\(R_0\\) of each individual by making assumptions\n\\(R_{0,i}\\) is the measure of \\(R_i\\) in a susceptible population, i.e. when: \\[S = N\\]\nIn other words: \\[R_{0,i} = R_i ~ \\frac{N}{S}\\]\nIf we make the following assumptions:\n\ns(t) is constant over the course of infectiousness of an individual: \\[s(t) = s(t+\\frac{1}{\\tau_i})\\]\n\\(\\beta_i\\) is a constant and does not vary over the course of time (no control measures)\n\nThen the equation for \\(R_{0,i}\\) reduces to the following: \\[R_{0,i} = [\\beta_i]_{j_i}^{j_i+\\frac{1}{\\tau_i}} = \\frac{\\beta_i}{\\tau_i}\\]\nWe can see the basic reproduction number of an individual is fully characterized by the \\(\\beta_i\\) and the \\(\\tau_i\\) of that individual.\n\n\n\\(R_0\\) for a population\nTo a generalize to a population-level, we can simply find the expected value for the equation above: \\[R_0 = E[R_{0,i}] = \\frac{E[\\beta_i]}{E[\\tau_i]}\\]\nAssuming:\n\n\\(E[\\beta_i] = \\beta\\)\n\\(E[\\tau_i] = \\gamma\\)\n\nWe can write: \\[R_0 = \\frac{\\beta}{\\gamma}\\]"
  },
  {
    "objectID": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#herd-immunity",
    "href": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#herd-immunity",
    "title": "Epidemic modeling - Part 1",
    "section": "Herd Immunity",
    "text": "Herd Immunity\nAs just described, \\(R\\) is the measure of secondary infections, and \\(R_0\\) is the measure of secondary infections in a susceptible population where \\(R=R_0~s(t)\\).\nIt is easy to understand that if each infectious individual contaminates less than 1 other individual on average (\\(R &lt; 1\\)) then the number of exposed, and eventually infectious, individuals will diminish and tend to 0.\nOn the other hand, if each infectious individual contaminates more than 1 other individual (\\(R &gt; 1\\)) then the number of infectious individuals will rise (chance of epidemic).\nMathematical formulation:\n\\[\\frac{d~e(t)}{dt} = \\beta~i(t)~s(t) - \\gamma~i(t)\\] \\[\\leftrightarrow\\frac{d~e(t)}{dt} = R_0~\\gamma~i(t)~s(t) - \\gamma~i(t)\\] \\[\\leftrightarrow\\frac{d~e(t)}{dt} = \\gamma~i(t)~(R_0~s(t) - 1)=\\gamma~i(t)~(R - 1)\\]\nAnd so we find that in a population where \\(\\gammaĩ(t)&gt;0\\): \\[\\frac{d~e(t)}{dt} = 0\\] \\[\\leftrightarrow R-1 = 0\\] \\[\\leftrightarrow R = 1\\]\nIf \\(R&lt;1\\) then \\(\\frac{d~e(t)}{dt} &lt; 0\\)\nHerd immunity threshold:\nThe herd immunity threshold is the point at which enough of the population is immune to the disease (not susceptible) in order to have \\(R &lt; 1\\) and can be calculated as follows: \\[R = R_0 ~ s(t)\\]\nWe know the proportion of the population immune to the disease is: \\[Immune(t) = 1-s(t)\\] \\[\\leftrightarrow\\ s(t)=1-Immune(t)\\]\nThe threshold of \\(R = 1\\) is achieved when: \\[R_0 ~s(t) = 1\\] \\[\\leftrightarrow\\ R_0 ~(1-Immune(t)) = 1\\] \\[\\leftrightarrow\\ 1-Immune(t) = \\frac{1}{R_0}\\] \\[\\leftrightarrow\\ Immune(t) = 1-\\frac{1}{R_0}\\]\nWhen the proportion of immune individuals in a population reaches \\(1-\\frac{1}{R_0}\\) then \\(R\\) will become smaller than 1 and the number of infectious individuals will diminish and tend to 0."
  },
  {
    "objectID": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#conclusion",
    "href": "posts/2020-03-15-compartmentalization/2020-03-15-compartmentalization.html#conclusion",
    "title": "Epidemic modeling - Part 1",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a brief introduction to compartmentalization models and the dynamics associated with them.\nOf course these sort of derivations can be done for many different types of comprtaments and their relevant dynamics, but the SEIR is simple enough to understand and model quickly."
  },
  {
    "objectID": "posts/2021-05-07-stochastic_HIV_model/2021-05-07-stochastic_HIV_model.html#motivation-for-write-up",
    "href": "posts/2021-05-07-stochastic_HIV_model/2021-05-07-stochastic_HIV_model.html#motivation-for-write-up",
    "title": "“HIV modeling”",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThis is the 4th part of a multi-part series blog post on modeling in epidemiology.\nThe COVID-19 pandemic has brought a lot of attention to study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points.\nAfter introducing the concepts of compartmentalization and disease dynamics in the first blog post, the second part looked at a deterministic numerical solution for the SEIR model discussed, and the effects of the parameters \\(\\beta\\), \\(\\sigma\\), and \\(\\gamma\\) in parts 1 and 2.\nPart 3 made the argument that most models ignore individual-level disease dynamics in favor of averaging population-level \\(\\sigma\\) and \\(\\gamma\\) parameters and showed some big discrepancies between actual COVID-19 probability distributions for those parameters and those used in research.\nThis 4th part is where I build a numerical SEIR model that takes into account these probability distributions in order to tweak the model as close to COVID-19 data as possible."
  },
  {
    "objectID": "posts/2021-05-07-stochastic_HIV_model/2021-05-07-stochastic_HIV_model.html#building-a-stochastic-model",
    "href": "posts/2021-05-07-stochastic_HIV_model/2021-05-07-stochastic_HIV_model.html#building-a-stochastic-model",
    "title": "“HIV modeling”",
    "section": "Building a stochastic model",
    "text": "Building a stochastic model\nAs opposed to the deterministic model from Part 2, this model is going to focus on individual level disease dynamics to model the disease propagation.\nThe basic idea of this model is to have a dataframe with the number of rows equal to the population size (each individual is a row) and two columns: * State column to describe the state of each individual (S, E, I, or R) * Day column to save the day of transition of the individual into that state\nHowever, the population-level rates of transmission still apply here i.e. a person goes from S → E following three points: 1. the number of contacts the person has per unit time (given by \\(r\\)) 2. the chance a given contact is with an I - infectious individual (the higher thenumber of I, the higher the chance) 3. the chance of an S contracting the disease from a contact with an I (given by \\(\\rho\\))\nThis is done stochastically.\nOnce a person becomes E, their progression is unique to them. This progression is calculated in advance for computational reason, but it allows to use the time ditributions we want.\n\n#collapse_hide\n!pip install plotly==4.14.3\nimport pandas as pd\nimport numpy as np\nimport math\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import expon\nfrom scipy.stats import gamma\nfrom scipy.stats import weibull_min\nfrom numpy.random import default_rng\nrng = default_rng()\n\n# Let's build a numerical solution\ndef seir_model(init, parms, days):\n    S_0, E_0, I_0, R_0 = init\n    Epd, Ipd, Rpd = [0], [0], [0]\n    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n    dt=0.1\n    t = np.linspace(0,days,int(days/dt))\n    sigma, beta, gam = parms\n    for _ in t[1:]:\n        next_S = S[-1] - beta*S[-1]*I[-1]*dt\n        Epd.append(beta*S[-1]*I[-1]*dt)\n        next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt\n        Ipd.append(sigma*E[-1]*dt)\n        next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt\n        Rpd.append(gam*I[-1]*dt)\n        next_R = R[-1] + (gam*I[-1])*dt\n        S.append(next_S)\n        E.append(next_E)\n        I.append(next_I)\n        R.append(next_R)\n    return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T\n\nCollecting plotly==4.14.3\n  Downloading https://files.pythonhosted.org/packages/1f/f6/bd3c17c8003b6641df1228e80e1acac97ed8402635e46c2571f8e1ef63af/plotly-4.14.3-py2.py3-none-any.whl (13.2MB)\n     |████████████████████████████████| 13.2MB 303kB/s \nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly==4.14.3) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==4.14.3) (1.15.0)\nInstalling collected packages: plotly\n  Found existing installation: plotly 4.4.1\n    Uninstalling plotly-4.4.1:\n      Successfully uninstalled plotly-4.4.1\nSuccessfully installed plotly-4.14.3\n\n\n\nCreating the initial population dataframe\nBelow is a function to create the initial population dataframe: * \\(p\\) is the population number * \\(num_E\\) is the number of people exposed on day 0 * \\(num_I\\) is the number of infectious on day 0 * \\(num_R\\) is the number of people recovered on day 0\n\n#collapse_hide\n# Need this new function for model below:\ndef make_df(p, num_I, num_R):\n  df = pd.DataFrame(np.full((p,1), 'S').T[0], columns=['State'])\n  df['Year'] = 0\n  df['Age'] = (np.random.random(p)*35+15).astype(int)\n  tochange=df.loc[rng.choice(p, size=num_I+num_R, replace=False),'State'].index\n  df.loc[tochange[0:num_I],'State'] = 'I'\n  df.loc[tochange[num_I:num_I+num_R],'State'] = 'R'\n  return df\n\n\n\nBuilding the model\n\n#np.random.random(size=(p,days))\n#np.log(4)\nj=11\nover = 10\n#10/np.cumsum(np.ones(100))\n0.05 + (0.3/np.exp((j+1-over)/10))\n\n0.29561922592339457\n\n\n\n#collapse_hide\ndef seir_model_stoch(beta, beta2, p, num_I, num_R, years, T_Infectious, ART, control):\n\n    # Initialize population dataframe with data given by user\n    df = make_df(p, num_I, num_R)\n    \n    # This variable is used to track daily value of beta if it varies over time\n    xxbeta=np.array([],dtype=float)\n\n    # Initialize the arrays to return\n    # Below are numbers of S, I, R total\n    S=np.array([],dtype=int)\n    I=np.array([],dtype=int)\n    R=np.array([],dtype=int)\n    # Below are the daily additions in S, I, R\n    Spd=np.array([],dtype=int)\n    Ipd=np.array([],dtype=int)\n    Rpd=np.array([],dtype=int)\n\n    b=beta\n    #b2=beta[0]\n    b2=np.array([],dtype=float)\n    b1=b\n\n    # signal diminshing beta\n    over = 0 \n\n    # signal end of deaths due to ART\n    art1 = 0\n    art2 = 0\n\n    \n    # Stochastic model so use random values to decide on progression\n    rand = np.random.random(size=(p,years))\n\n    # Depending if you want exponential, gamma, or Weibull distribution for T_Infectious\n    # Uses distributions found on blog part 3\n    if T_Infectious == 'expon':\n      ItoR = expon.rvs(loc=0,scale=10,size=p)\n    elif T_Infectious == 'gamma':\n      ItoR = gamma.rvs(4,loc=3,scale=2,size=p)    \n    else:\n      ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p)\n\n    # Iterate over every day the simulation is run\n    for j in range(0,years-1):\n\n        # Record daily beta values\n        xxbeta=np.append(xxbeta, b[j])\n\n        # First we get the index of the individuals that will change state today:\n\n        # Random number tells you which 'S' have been exposed on this day \n        #StoE_index = df.loc[(df.State == 'S') & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/p)].index\n        if ART &lt; 2:\n          StoI_index = df.loc[(df.State == 'S') & (df.Age &lt; 49) & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/(len(np.where(df.State=='I')[0])+len(np.where(df.State=='S')[0])))].index\n          StoS_index = df.loc[(df.State == 'S') & (df.Age &lt; 49) & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/(len(np.where(df.State=='I')[0])+len(np.where(df.State=='S')[0])))].index\n        elif ART == 2:\n          if art2 == 0:\n            StoI_index = df.loc[(df.State == 'S') & (df.Age &lt; 49) & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/(len(np.where(df.State=='I')[0])+len(np.where(df.State=='S')[0])))].index\n            StoS_index = df.loc[(df.State == 'S') & (df.Age &lt; 49) & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/(len(np.where(df.State=='I')[0])+len(np.where(df.State=='S')[0])))].index\n          elif art2 == 1:\n            StoI_index = df.loc[(df.State == 'S') & (df.Age &gt; 55)].index\n            StoS_index = df.loc[(df.State == 'S') & (df.Age &lt; 49)].index\n\n        StoRem_index = df.loc[(df.State == 'S') & (df.Age == 49)].index\n\n        # For each row, if a person has been a certain number of days in E, they will go to I\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        #EtoI_index = df.loc[(df.State == 'E') & (j-df.Day &gt;= EtoI)].index\n        \n        # Similaraly as above\n        # For each row, if a person has been a certain number of days in I, they will go to R\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        ItoRem_index = df.loc[(df.State == 'I') & (df.Age == 49)].index\n        if ART == 0: #don't use ART\n          ItoR_index = df.loc[(df.State == 'I') & (j-df.Year &gt;= ItoR) & (df.Age &lt; 49)].index\n          ItoI_index = df.loc[(df.State == 'I') & (j-df.Year &lt; ItoR) & (df.Age &lt; 49)].index\n        elif ART &gt; 0:\n          if art2 == 0:\n            ItoR_index = df.loc[(df.State == 'I') & (j-df.Year &gt;= ItoR) & (df.Age &lt; 49)].index\n            ItoI_index = df.loc[(df.State == 'I') & (j-df.Year &lt; ItoR) & (df.Age &lt; 49)].index\n          elif art2 ==1:\n            ItoR_index = df.loc[(df.State == 'I') & (df.Age &gt; 49)].index\n            ItoI_index = df.loc[(df.State == 'I') & (df.Age &lt; 49)].index            \n\n        RtoRem_index = df.loc[(df.State == 'R') & (df.Age == 49)].index\n        RtoR_index = df.loc[(df.State == 'R') & (df.Age &lt; 49)].index\n\n        # Use indexes collected above to populate per day values\n        #Epd = np.append(Epd,len(StoE_index))\n        #Ipd = np.append(Ipd,len(EtoI_index))\n        Ipd = np.append(Ipd,len(StoI_index))\n        Rpd = np.append(Rpd,len(ItoR_index))\n\n        # Now we use the indexes collected above randomly to change the actual population dataframe to the new states\n        df.iloc[ItoRem_index] = ['S', j, 15]\n        df.loc[ItoR_index, ['State','Year']] = ['S', j]\n        df.loc[ItoR_index, 'Age'] = df.loc[ItoR_index, 'Age'] + 1\n        df.loc[ItoI_index, 'Age'] = df.loc[ItoI_index, 'Age'] + 1\n        df.iloc[StoRem_index] = ['S', j, 15]\n        df.loc[StoI_index, ['State','Year']] = ['I', j]\n        df.loc[StoI_index, 'Age'] = df.loc[StoI_index, 'Age'] + 1\n        df.loc[StoS_index, 'Age'] = df.loc[StoS_index, 'Age'] + 1\n        \n        df.iloc[RtoRem_index] = ['S', j, 15]\n        df.loc[RtoR_index, 'Age'] = df.loc[RtoR_index, 'Age'] + 1\n        \n        \n        # Append the S, I, and R arrays\n        S=np.append(S,len(np.where(df.State=='S')[0]))\n        I=np.append(I,len(np.where(df.State=='I')[0]))\n        R=np.append(R,len(np.where(df.State=='R')[0]))\n\n        # Code below for control measures to reduce beta values\n        if control == 1:\n          if (I[-1]/p &gt; 0.015):\n            art1 = 1\n            if over == 0:\n              over = j\n          \n          if art1 == 1:\n            if j &gt; over + 15:    \n            #if Ipd[-2] &gt; Ipd[-1]:\n              art2 = 1\n\n          if over != 0:\n            #b = beta2+(b1/np.exp((j+3-over)/15))\n            b = beta2+(b1/np.exp((j+1-over)/10))\n\n        if control == 2:\n          if (I[-1]/p &gt; 0.3):\n            art1 = 1\n            if over == 0:\n              over = j\n              #print(over)\n          \n          if art1 == 1:\n            if j &gt; over + 15:    \n            #if Ipd[-2] &gt; Ipd[-1]:\n              art2 = 1\n\n          if over != 0:\n            #b = beta2+(b1/np.exp((j+3-over)/15))\n            b = beta2+(b1/np.exp((j+1-over)/10))\n\n\n        xxbeta2 = ((S[j-1]+I[j-1])/I[j-1])*Ipd[j]/S[j-1]\n        #xxbeta2 = 0.5\n        #print(xxbeta2)\n        b2 = np.append(b2, xxbeta2)\n                \n    #Epd[0]+=num_E\n    Ipd[0]+=num_I\n    Rpd[0]+=num_R\n\n    #return S,E,I,R, Epd, Ipd, Rpd, xxbeta\n    return S, I, R, Spd, Ipd, Rpd, xxbeta, b2, over"
  },
  {
    "objectID": "posts/2021-05-07-stochastic_HIV_model/2021-05-07-stochastic_HIV_model.html#testing-the-model",
    "href": "posts/2021-05-07-stochastic_HIV_model/2021-05-07-stochastic_HIV_model.html#testing-the-model",
    "title": "“HIV modeling”",
    "section": "Testing the model",
    "text": "Testing the model\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 200\np = 10000\nnum_E = 0\nnum_I = 1\nnum_R = 0\nbeta_stoch = 0.3*np.ones(days)\nbeta_stoch2 = 0.05\n\n# Run 3 stochastic simulations\nresults_stoch1 = seir_model_stoch(beta_stoch,beta_stoch2, p, num_I, num_R, years, 'gamma', 0, 1)\nresults_stoch2 = seir_model_stoch(beta_stoch, beta_stoch2, p, num_I, num_R, years, 'gamma', 0, 1)\nresults_stoch3 = seir_model_stoch(beta_stoch, beta_stoch2, p, num_I, num_R, years, 'gamma', 0, 2)\nresults_stoch4 = seir_model_stoch(beta_stoch, beta_stoch2, p, num_I, num_R, years, 'gamma', 0, 2)\n\n\nresults_stoch1[8]\n\n26\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Beta_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[6], line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='Beta_meas1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[7], line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='I_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch1\"),\n    go.Bar(name='Ip_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[4]*10/p, legendgroup=\"stoch1\"),\n    go.Scatter(name='R_stoch1', x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch1\"),\n    go.Scatter(name='Beta_stoch2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[6], line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='Beta_meas2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[7], line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='I_stoch2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch2\"),\n    go.Bar(name='Ip_stoch2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[4]*10/p, legendgroup=\"stoch2\"),\n    go.Scatter(name='R_stoch2', x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch2\"),\n    go.Scatter(name='Beta_stoch3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[6], line={'dash':'dot', 'color':'yellow'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='Beta_meas3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[7], line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='I_stoch3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch3\"),\n    go.Bar(name='Ip_stoch3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[4]*10/p, legendgroup=\"stoch3\"),\n    go.Scatter(name='R_stoch3', x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='Beta_stoch4', x=np.arange(len(results_stoch4[0])), y=results_stoch4[6], line={'dash':'dot', 'color':'yellow'}, legendgroup=\"stoch4\"),\n    go.Scatter(name='Beta_meas4', x=np.arange(len(results_stoch4[0])), y=results_stoch4[7], line={'dash':'dot','color':'yellow'}, legendgroup=\"stoch4\"),\n    go.Scatter(name='I_stoch4', x=np.arange(len(results_stoch4[0])), y=results_stoch4[1]/p, line={'dash':'dot', 'color':'red'}, legendgroup=\"stoch4\"),\n    go.Bar(name='Ip_stoch4', x=np.arange(len(results_stoch4[0])), y=results_stoch4[4]*10/p, legendgroup=\"stoch4\"),\n    go.Scatter(name='R_stoch4', x=np.arange(len(results_stoch4[0])), y=results_stoch4[2]/p, line={'dash':'dot', 'color':'green'}, legendgroup=\"stoch4\")\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of stochasticity on Deterministic SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html",
    "href": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html",
    "title": "“Epidemic modeling - Part 5”",
    "section": "",
    "text": "“Further discussion on \\(R_0\\), distribution of \\(T_{Infectious}\\), and their impact on peak infectious in SEIR model”"
  },
  {
    "objectID": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html#motivation-for-write-up",
    "href": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html#motivation-for-write-up",
    "title": "“Epidemic modeling - Part 5”",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThis is the 5th part of a multi-part series blog post on modeling in epidemiology.\nThe COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points.\nThe 1st part of the blog series showed an epidemic could occur when \\(R_0 &gt; 1\\) and that it was fully characerized by the average \\(\\beta\\) and the average \\(T_{Infectious}\\).\nWe have also seen that the higher \\(R_0\\), the faster and the higher the peak infectious will be.\nThe latest blog post however showed the importance of the distribution of \\(T_{Infectious}\\) to simulate the SEIR model and how it impacted the spread of the disease and the peak of infectious individuals. Even with lower \\(R_0\\) values, the infectious peak were higher when using Gamma or Weibull distributions for \\(T_{Infectious}\\).\nThis 5th installment examines this discrepancy further.\n\n#collapse_hide\n!pip install plotly==4.6.0\nimport pandas as pd\nimport numpy as np\nimport math\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import expon\nfrom scipy.stats import gamma\nfrom scipy.stats import weibull_min\nfrom numpy.random import default_rng\nrng = default_rng()\n\n# Let's build a numerical solution\ndef seir_model(init, parms, days):\n    S_0, E_0, I_0, R_0 = init\n    Epd, Ipd, Rpd = [0], [0], [0]\n    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n    dt=0.1\n    t = np.linspace(0,days,int(days/dt))\n    sigma, beta, gam = parms\n    for _ in t[1:]:\n        next_S = S[-1] - beta*S[-1]*I[-1]*dt\n        Epd.append(beta*S[-1]*I[-1]*dt)\n        next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt\n        Ipd.append(sigma*E[-1]*dt)\n        next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt\n        Rpd.append(gam*I[-1]*dt)\n        next_R = R[-1] + (gam*I[-1])*dt\n        S.append(next_S)\n        E.append(next_E)\n        I.append(next_I)\n        R.append(next_R)\n    return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T\n\nRequirement already satisfied: plotly==4.6.0 in /usr/local/lib/python3.6/dist-packages (4.6.0)\nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0)\n\n\n\n#collapse_hide\n# Need this new function for model below:\ndef make_df(p,num_E, num_I, num_R):\n  df = pd.DataFrame(np.full((p,1), 'S').T[0], columns=['State'])\n  df['Day'] = 0\n  tochange=df.loc[rng.choice(p, size=num_E+num_I+num_R, replace=False),'State'].index\n  df.loc[tochange[0:num_E],'State'] = 'E'\n  df.loc[tochange[num_E:num_I+num_E],'State'] = 'I'\n  df.loc[tochange[num_E+num_I:num_E+num_I+num_R],'State'] = 'R'\n  return df\n\n\n#collapse_hide\ndef seir_model_stoch(beta, p, num_E, num_I, num_R, days, T_Latent, T_Infectious):\n\n    # Initialize population dataframe with data given by user\n    df = make_df(p,num_E, num_I, num_R)\n    \n    # This variable is used to track daily value of beta if it varies over time\n    xxbeta=np.array([],dtype=float)\n\n    # Initialize the arrays to return\n    # Below are numbers of S, E, I, R total\n    S=np.array([],dtype=int)\n    E=np.array([],dtype=int)\n    I=np.array([],dtype=int)\n    R=np.array([],dtype=int)\n    # Below are the daily additions in S, E, I, R\n    Spd=np.array([],dtype=int)\n    Epd=np.array([],dtype=int)\n    Ipd=np.array([],dtype=int)\n    Rpd=np.array([],dtype=int)\n\n    b=beta\n    \n    # Stochastic model so use random values to decide on progression\n    rand = np.random.random(size=(p,days))\n\n    # Depending if you want exponential or gamma distribution for T_Latent\n    if T_Latent == 'expon':\n      EtoI = expon.rvs(loc=0,scale=5.2,size=p)\n    else:\n      EtoI = gamma.rvs(1.8,loc=0.9,scale=(5.2-1.8)/0.9,size=p)\n\n    # Depending if you want exponential, gamma, or Weibull distribution for T_Infectious\n    # Uses distributions found on blog part 3\n    if T_Infectious == 'expon':\n      ItoR = expon.rvs(loc=0,scale=28.85,size=p)\n    elif T_Infectious == 'gamma':\n      ItoR = gamma.rvs(4,loc=3,scale=4.25,size=p)    \n    else:\n      ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p)\n\n    # Iterate over every day the simulation is run\n    for j in range(0,days-1):\n\n        # Record daily beta values\n        xxbeta=np.append(beta, b)\n\n        # First we get the index of the individuals that will change state today:\n\n        # Random number tells you which 'S' have been exposed on this day \n        StoE_index = df.loc[(df.State == 'S') & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/p)].index\n\n        # For each row, if a person has been a certain number of days in E, they will go to I\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        EtoI_index = df.loc[(df.State == 'E') & (j-df.Day &gt;= EtoI)].index\n        \n        # Similaraly as above\n        # For each row, if a person has been a certain number of days in I, they will go to R\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        ItoR_index = df.loc[(df.State == 'I') & (j-df.Day &gt;= ItoR)].index\n\n        # Use indexes collected above to populate per day values\n        Epd = np.append(Epd,len(StoE_index))\n        Ipd = np.append(Ipd,len(EtoI_index))\n        Rpd = np.append(Rpd,len(ItoR_index))\n\n        # Now we use the indexes collected above randomly to change the actual population dataframe to the new states\n        df.loc[ItoR_index, 'State'] = 'R'\n        df.loc[EtoI_index, 'State'] = 'I'\n        df.loc[StoE_index, 'State'] = 'E'\n        df.loc[ItoR_index, 'Day'] = j\n        df.loc[EtoI_index, 'Day'] = j\n        df.loc[StoE_index, 'Day'] = j\n        df.loc[ItoR_index, 'DayR'] = j\n        df.loc[EtoI_index, 'DayI'] = j\n        df.loc[StoE_index, 'DayE'] = j\n        \n        # Append the S, E, I, and R arrays\n        S=np.append(S,len(np.where(df.State=='S')[0]))\n        E=np.append(E,len(np.where(df.State=='E')[0]))\n        I=np.append(I,len(np.where(df.State=='I')[0]))\n        R=np.append(R,len(np.where(df.State=='R')[0]))\n\n        # Code below for control measures to reduce beta values\n#        if ((I[-1] &gt; 1000) & (Ipd[-1] &gt; 399)): \n#            b = beta2\n#        elif ((I[-1] &gt; 1000) & (Ipd[-1] &lt; 400)): \n#            b = beta3\n                \n    Epd[0]+=num_E\n    Ipd[0]+=num_I\n    Rpd[0]+=num_R\n\n    return S,E,I,R, Epd, Ipd, Rpd, xxbeta, df"
  },
  {
    "objectID": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html#peak-of-infectious-individuals",
    "href": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html#peak-of-infectious-individuals",
    "title": "“Epidemic modeling - Part 5”",
    "section": "Peak of infectious individuals",
    "text": "Peak of infectious individuals\nLet’s first try to characterize when the peak of infectious indiviuals occurs in the SEIR model.\nThe peak of infectious individuals occurs when the number of individuals that recover per day (\\(R_{pd}\\)) becomes greater than the number of new infectious individuals per day (\\(I_{pd}\\)), i.e. when: \\[R_{pd} \\geq I_{pd}\\]\n\nConvolution and daily numbers\nMathematical derivation of \\(I_{pd}\\):\nIn the stochastic model, the number of new infectious individuals per day is the sum of the daily new exposures per day of the previous days multiplied by the probability that they become infectious after so many days.\nWe write: \\[I_{pd}[j] = \\sum_{n_L=0}^{M_L-1}h_L[n_L]~E_{pd}[j-n_L]\\]\nMathematically, this means \\(I_{pd}[j]\\) is the result of convolution of \\(E_{pd}\\) and an impulse response \\(h_L[n]\\) where \\(h_L[n]\\) describes the distribution of \\(T_{Latent}\\), and we can write: \\[I_{pd}[j] = h_L[j]\\circledast E_{pd}[j]\\]\nFrom this derivation, we can see the \\(I_{pd}\\) depends on the distribution of \\(T_{Latent}\\) and \\(E_{pd}\\) (the number of new exposures per day) of the previous days.\nWhen initial conditions are the same, this means \\(T_{Infectious}\\) has no impact on \\(I_{pd}\\) and so the rate of new infectious individuals will look the same between the two models (exponential or gamma distributed \\(T_{Infectious}\\)).\nMathematical derivation of \\(R_{pd}\\):\nWe can similarly describe \\(R_{pd}[j]\\) as the result of convolution of \\(I_{pd}\\) and an impulse response \\(h_I[n]\\) where \\(h_I[n]\\) describes the distribution of \\(T_{Infectious}\\).\nIn other words: \\[R_{pd}[j] = \\sum_{n_I=0}^{M_I-1}h_I[n_I]~I_{pd}[j-n_I] = h_I[j]\\circledast I_{pd}[j]\\]\n\n\nWhen does the peak occur?\nIt occurs on day j where the threshold below is reached: \\[R_{pd}[j] = I_{pd}[j]\\] \\[\\leftrightarrow \\sum_{n_I=0}^{M_I-1}h_I[n_I]~I_{pd}[j-n_I] = \\sum_{n_L=0}^{M_L-1}h_L[n_L]~E_{pd}[j-n_L]\\]\nWhile this is complicated to solve analytically, we can look at the distribution of \\(T_{Infectious}\\) and get some clues as to what might happen with different distributions."
  },
  {
    "objectID": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html#i_pd-and-r_pd-for-t_infectious-sim-explambda-vs.-t_infectious-sim-weibulllambda-kgamma",
    "href": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html#i_pd-and-r_pd-for-t_infectious-sim-explambda-vs.-t_infectious-sim-weibulllambda-kgamma",
    "title": "“Epidemic modeling - Part 5”",
    "section": "\\(I_{pd}\\) and \\(R_{pd}\\) for \\(T_{Infectious} \\sim Exp(\\lambda)\\) vs. \\(T_{Infectious} \\sim Weibull(\\lambda, k,\\gamma)\\)",
    "text": "\\(I_{pd}\\) and \\(R_{pd}\\) for \\(T_{Infectious} \\sim Exp(\\lambda)\\) vs. \\(T_{Infectious} \\sim Weibull(\\lambda, k,\\gamma)\\)\n\n\\(T_{Infectious} \\sim Exp(28.85)\\) vs. \\(T_{Infectious} \\sim Weibull(2.3, 20.11, 2)\\)\nLet’s first have another look at the difference between these two distributions:\n\n#collapse_hide\nlocw=2\nwk = 2.3\nwl = (20-locw)/(math.log(2)**(1/wk))\n\nloce=0\nscalee=28.85-loce\n\np=10000\n\ndf = pd.DataFrame({\n    'Exponential': expon.rvs(loc=loce, scale=scalee,size=p),\n    'Weibull': weibull_min.rvs(wk, loc=locw, scale=wl,size=p)\n    })\n\nfig = px.histogram(df.stack().reset_index().rename(columns={\"level_1\": \"Distribution\"}), x=0, color=\"Distribution\", marginal='box')\nfig.update_layout(\n    title={\n        'text':'Exponential vs. Weibull distributions',\n        'x':0.5,\n        'xanchor':'center'\n    },\n    barmode='overlay',\n    xaxis_title='Days',\n    yaxis_title='Count',\n    legend=dict(\n        x=1,\n        y=0,\n        traceorder=\"normal\",\n    )\n)\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\nImpact on \\(I_{pd}\\) \\(R_{pd}\\):\nLet’s see how the two distributions of \\(T_{Infectious}\\) result in different curbs for \\(I_{pd}\\) and \\(R_{pd}\\).\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 200\np = 10000\nnum_E = 1\nnum_I = 0\nnum_R = 0\nbeta_stoch = 0.5*np.ones(days)\n\n# Run 2 stochastic simulations, 1 with exponential gamma, 1 with weibull gamma\nresults_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 'expon')\nresults_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 1)\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Ipd_Exp', x=np.arange(len(results_stoch0[0])), y=results_stoch0[5], line={'dash':'dashdot', 'color':'red'}, legendgroup=\"stoch4\"),\n    go.Scatter(name='Rpd_Exp', x=np.arange(len(results_stoch0[0])), y=results_stoch0[6], line={'dash':'dashdot', 'color':'green'}, legendgroup=\"stoch4\"),\n    go.Scatter(name='Ipd_Weibull', x=np.arange(len(results_stoch1[0])), y=results_stoch1[5], line={'dash':'solid', 'color':'red'}, legendgroup=\"stoch3\"),\n    go.Scatter(name='Rpd_Weibull', x=np.arange(len(results_stoch1[0])), y=results_stoch1[6], line={'dash':'solid', 'color':'green'}, legendgroup=\"stoch3\")\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Effect of Exponential vs. Weibull distributed } T_{Infectious} \\text{ on } I_{pd} \\text{ and } R_{pd}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\nAnalysis:\nAs discussed above, we can see here \\(I_{pd}\\) are very similar between the two models.\nHowever, there are two key takeaways from these graphs:\n\nRise of total infectious individuals:\n\nThe Exponential distribution leads to more recoveries early on as the \\(R_{pd}\\) for the Exp distribution leads the \\(R_{pd}\\) of the Weibull distribution by ~6 days in the earl recovery period.\nIntuitively, we can thus understand that total infectious people will increase slower with the Eponential distribution than the Weibull distribution (since more people recover faster in the Exp model), and so the peak of infectious individuals will be earlier with the Weibull distribution.\n\nRecoveries:\n\nWhile the peak appears faster with a Weibull distribution, we also see that after the initial delay, the Weibull distribution leads to larger number of recoveries and so the total number of infectious individuals will tend to 0 much quicker than with the Exp distribution."
  },
  {
    "objectID": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html#discussion",
    "href": "posts/2020-04-01-diagnosing-R0-and-peak-infectious/2020-04-01-diagnosing-R0-and-peak-infectious.html#discussion",
    "title": "“Epidemic modeling - Part 5”",
    "section": "Discussion",
    "text": "Discussion\nWhile \\(R_0\\) seemed to be a good measure of the spread of disease and a good predictor of the peak of infectious individuals in a population, we find here that the actual distribution of \\(T_{Infectious}\\) plays a crucial role in detemining the dynamics of the SEIR model.\nThat is to say that while estimating \\(R_0\\) is important in times of epidemics or pandemics, finding the actual distributions of \\(T_{Latent}\\) and \\(T_{Infetious}\\) are equally important in predicting the impact of the disease."
  },
  {
    "objectID": "posts/2020-04-02-convolution/2020-04-02-convolution.html",
    "href": "posts/2020-04-02-convolution/2020-04-02-convolution.html",
    "title": "“Epidemic modeling - Part 7”",
    "section": "",
    "text": "#collapse_hide\n!pip install plotly==4.9.0\nimport pandas as pd\nimport numpy as np\n\nimport math\n\nfrom scipy import signal\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom scipy.stats import expon\nfrom scipy.stats import gamma\nfrom scipy.stats import weibull_min\n\nfrom numpy.random import default_rng\nrng = default_rng()\n\n# Let's build a numerical solution\ndef seir_model(init, parms, days):\n    S_0, E_0, I_0, R_0 = init\n    Epd, Ipd, Rpd = [0], [0], [0]\n    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n    dt=0.1\n    t = np.linspace(0,days,int(days/dt))\n    sigma, beta, gam = parms\n    for _ in t[1:]:\n        next_S = S[-1] - beta*S[-1]*I[-1]*dt\n        Epd.append(beta*S[-1]*I[-1]*dt)\n        next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt\n        Ipd.append(sigma*E[-1]*dt)\n        next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt\n        Rpd.append(gam*I[-1]*dt)\n        next_R = R[-1] + (gam*I[-1])*dt\n        S.append(next_S)\n        E.append(next_E)\n        I.append(next_I)\n        R.append(next_R)\n    return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T\n\nCollecting plotly==4.9.0\n  Downloading https://files.pythonhosted.org/packages/bf/5f/47ab0d9d843c5be0f5c5bd891736a4c84fa45c3b0a0ddb6b6df7c098c66f/plotly-4.9.0-py2.py3-none-any.whl (12.9MB)\n     |████████████████████████████████| 12.9MB 313kB/s \nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.9.0) (1.15.0)\nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.9.0) (1.3.3)\nInstalling collected packages: plotly\n  Found existing installation: plotly 4.4.1\n    Uninstalling plotly-4.4.1:\n      Successfully uninstalled plotly-4.4.1\nSuccessfully installed plotly-4.9.0\n#collapse_hide\n# Need this new function for model below:\ndef make_df(p,num_E, num_I, num_R):\n  df = pd.DataFrame(np.full((p,1), 'S').T[0], columns=['State'])\n  df['Day'] = 0\n  tochange=df.loc[rng.choice(p, size=num_E+num_I+num_R, replace=False),'State'].index\n  df.loc[tochange[0:num_E],'State'] = 'E'\n  df.loc[tochange[num_E:num_I+num_E],'State'] = 'I'\n  df.loc[tochange[num_E+num_I:num_E+num_I+num_R],'State'] = 'R'\n  return df\n#collapse_hide\ndef seir_model_stoch(beta, p, num_E, num_I, num_R, days):\n\n    # Initialize population dataframe with data given by user\n    df = make_df(p,num_E, num_I, num_R)\n    \n    # This variable is used to track daily value of beta if it varies over time\n    xxbeta=np.array([],dtype=float)\n\n    # Initialize the arrays to return\n    # Below are numbers of S, E, I, R total\n    S=np.array([],dtype=int)\n    E=np.array([],dtype=int)\n    I=np.array([],dtype=int)\n    R=np.array([],dtype=int)\n    # Below are the daily additions in S, E, I, R\n    Spd=np.array([],dtype=int)\n    Epd=np.array([],dtype=int)\n    Ipd=np.array([],dtype=int)\n    Rpd=np.array([],dtype=int)\n\n    b=beta\n    \n    # Stochastic model so use random values to decide on progression\n    rand = np.random.random(size=(p,days))\n\n    # Depending if you want exponential or gamma distribution for T_Latent\n    EtoI = gamma.rvs(1.8,loc=0.9,scale=(5.2-1.8)/0.9,size=p)\n\n    # Depending if you want exponential, gamma, or Weibull distribution for T_Infectious\n    # Uses distributions found on blog part 3\n    ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p)\n\n    # Iterate over every day the simulation is run\n    for j in range(0,days-1):\n\n        # Record daily beta values\n        xxbeta=np.append(beta, b)\n\n        # First we get the index of the individuals that will change state today:\n\n        # Random number tells you which 'S' have been exposed on this day \n        StoE_index = df.loc[(df.State == 'S') & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/p)].index\n\n        # For each row, if a person has been a certain number of days in E, they will go to I\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        EtoI_index = df.loc[(df.State == 'E') & (j-df.Day &gt;= EtoI)].index\n        \n        # Similaraly as above\n        # For each row, if a person has been a certain number of days in I, they will go to R\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        ItoR_index = df.loc[(df.State == 'I') & (j-df.Day &gt;= ItoR)].index\n\n        # Use indexes collected above to populate per day values\n        Epd = np.append(Epd,len(StoE_index))\n        Ipd = np.append(Ipd,len(EtoI_index))\n        Rpd = np.append(Rpd,len(ItoR_index))\n\n        # Now we use the indexes collected above randomly to change the actual population dataframe to the new states\n        df.loc[ItoR_index, 'State'] = 'R'\n        df.loc[EtoI_index, 'State'] = 'I'\n        df.loc[StoE_index, 'State'] = 'E'\n        df.loc[ItoR_index, 'Day'] = j\n        df.loc[EtoI_index, 'Day'] = j\n        df.loc[StoE_index, 'Day'] = j\n        df.loc[ItoR_index, 'DayR'] = j\n        df.loc[EtoI_index, 'DayI'] = j\n        df.loc[StoE_index, 'DayE'] = j\n        \n        # Append the S, E, I, and R arrays\n        S=np.append(S,len(np.where(df.State=='S')[0]))\n        E=np.append(E,len(np.where(df.State=='E')[0]))\n        I=np.append(I,len(np.where(df.State=='I')[0]))\n        R=np.append(R,len(np.where(df.State=='R')[0]))\n\n        # Code below for control measures to reduce beta values\n#        if ((I[-1] &gt; 1000) & (Ipd[-1] &gt; 399)): \n#            b = beta2\n#        elif ((I[-1] &gt; 1000) & (Ipd[-1] &lt; 400)): \n#            b = beta3\n                \n    Epd[0]+=num_E\n    Ipd[0]+=num_I\n    Rpd[0]+=num_R\n\n    return S,E,I,R, Epd, Ipd, Rpd, xxbeta, df"
  },
  {
    "objectID": "posts/2020-04-02-convolution/2020-04-02-convolution.html#motivation-for-write-up",
    "href": "posts/2020-04-02-convolution/2020-04-02-convolution.html#motivation-for-write-up",
    "title": "“Epidemic modeling - Part 7”",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThis is the 7th part of a multi-part series blog post on modeling in epidemiology.\nThe goal of this 7th installment is to expand on the notions seen in part 5.\nMost notably, the idea is to expand on the notion of convolution and deconvolution and to see how it can be useful to describe an epidemic."
  },
  {
    "objectID": "posts/2020-04-02-convolution/2020-04-02-convolution.html#available-data",
    "href": "posts/2020-04-02-convolution/2020-04-02-convolution.html#available-data",
    "title": "“Epidemic modeling - Part 7”",
    "section": "Available data",
    "text": "Available data\nIf you look at the the COVID-19 trackers around the web, or even mine, you can get a sense of what data is available for study.\nGenerally speaking we have the following: * Total number of positive individuals * Daily number of newly diagnosed individuals * Daily number of recovered individuals * Daily number of deaths\nResearch has also shown some vague numbers on both \\(T_{Latent}\\) and \\(T_{Infectious}\\).\nAbout the data:\n\nWe don’t have data for exposure - how can we get it using deconvolution ??\nThe data for some regions can be very sparse"
  },
  {
    "objectID": "posts/2020-04-02-convolution/2020-04-02-convolution.html#convolution",
    "href": "posts/2020-04-02-convolution/2020-04-02-convolution.html#convolution",
    "title": "“Epidemic modeling - Part 7”",
    "section": "Convolution",
    "text": "Convolution\n\nConvolution to get \\(I_{pd}\\)\nWe have seen that daily new infectious individuals is given by:\n\\[I_{pd}[j] = \\sum_{n_L=0}^{M_L-1}h_L[n_L]~E_{pd}[j-n_L]= h_L[j]\\circledast E_{pd}[j]\\]\nWhere \\(h_L[j]\\) describes the distribution of \\(T_{Latent}\\).\n\n\nConvolution to get \\(R_{pd}\\)\nSimilarly, we have seen that daily new recovered individuals (deaths + recoveries in fact) is given by:\n\\[R_{pd}[j] = \\sum_{n_I=0}^{M_I-1}h_I[n_I]~I_{pd}[j-n_I]= h_I[j]\\circledast I_{pd}[j]\\]\nWhere \\(h_I[j]\\) describes the distribution of \\(T_{Infectious}\\)."
  },
  {
    "objectID": "posts/2020-04-02-convolution/2020-04-02-convolution.html#finding-h_lj-and-h_ij",
    "href": "posts/2020-04-02-convolution/2020-04-02-convolution.html#finding-h_lj-and-h_ij",
    "title": "“Epidemic modeling - Part 7”",
    "section": "Finding \\(h_L[j]\\) and \\(h_I[j]\\)",
    "text": "Finding \\(h_L[j]\\) and \\(h_I[j]\\)\n\n\\(h_L[j]\\) is simply the probability of an individual having a latent period of j days\n\\(h_I[j]\\) is similarly the probability of an individual having an infectious period of j days\n\n\n#collapse_show\ndays = np.arange(100)\ncdf = pd.DataFrame({\n    'T_Latent': gamma.cdf(days, 1.8,loc=0.9,scale=(5.2-1.8)/0.9), \n    'T_Infectious': weibull_min.cdf(days, 2.3,loc=2,scale=20.11)\n    })\nh_L = cdf.diff().T_Latent\nh_I = cdf.diff().T_Infectious\nh_L[0] = 0\nh_I[0] = 0"
  },
  {
    "objectID": "posts/2020-04-02-convolution/2020-04-02-convolution.html#convolution-in-practice",
    "href": "posts/2020-04-02-convolution/2020-04-02-convolution.html#convolution-in-practice",
    "title": "“Epidemic modeling - Part 7”",
    "section": "Convolution in practice",
    "text": "Convolution in practice\n\nComparing \\(I_{pd}\\) from the model to \\(I_{pd}\\) obtained by convolving \\(R_{pd}\\)\nFirst run the SEIR model to obtain the actual \\(I_{pd}\\):\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 300\np = 10000\nnum_E = 1\nnum_I = 0\nnum_R = 0\nbeta_stoch = 0.5*np.ones(days)\n\n# Run 2 stochastic simulations\nresults_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days)\n\nNow obtain \\(h_L[j]\\circledast E_{pd}[j]\\):\n\n#collapse_hide\nIpd=signal.fftconvolve(h_L, results_stoch0[4], mode='full')\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Ipd_Actual', x=np.arange(len(results_stoch0[5])), y=results_stoch0[5]),\n    go.Scatter(name='Ipd_convolved', x=np.arange(len(Ipd)), y=Ipd)\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Actual } I_{pd} \\text{ vs. } h_L[j]\\circledast E_{pd}[j]$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\nComparing \\(R_{pd}\\) from the model to \\(R_{pd}\\) obtained by convolving \\(I_{pd}\\)\nWe can use the actual \\(R_{pd}\\) from the SEIR model above.\nNow obtain \\(h_I[j]\\circledast I_{pd}[j]\\):\n\n#collapse_hide\nRpd=signal.fftconvolve(h_I, results_stoch0[5], mode='full')\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Rpd_Actual', x=np.arange(len(results_stoch0[6])), y=results_stoch0[6]),\n    go.Scatter(name='Rpd_convolved', x=np.arange(len(Rpd)), y=Rpd)\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Actual } R_{pd} \\text{ vs. } h_I[j]\\circledast I_{pd}[j]$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/2020-04-02-convolution/2020-04-02-convolution.html#de-convolution",
    "href": "posts/2020-04-02-convolution/2020-04-02-convolution.html#de-convolution",
    "title": "“Epidemic modeling - Part 7”",
    "section": "De-convolution",
    "text": "De-convolution\nWe get very close matches when convlving, can we obtain similar results by deconvolving?\n\nProblems with de-convolution\nIlll-posed\nUnfortunately de-convolution is not as straightforward as the convolution.\nA quick recap of the math, in the frequency domain we have (Fourier or z transform) \\[F \\{ f ∗ g \\} = F \\{ f \\} ~ F \\{ g \\}\\] And so if we have: \\[y=\\ f ∗ g\\] Then: \\[F \\{ y \\} = F \\{ f \\} ~ F \\{ g \\}\\] And: \\[g = F^{-1} \\left(\\frac{F\\{y\\}}{F\\{f\\}} \\right) \\]\nBut this last equation is often ill-posed.\nNoise in data\nWe can see above that daily new data varies a lot from day to day due to the stochasticity of the model. This results in high-frequency decomposition in the frequency domain.\nInitial conditions and boundary-value problems\nBoundary values refer to the values on day 0 and the last day we have data for. Due to the way data is reported on JHU, we someimes have crayz values here, most commonly a value of 0 for the latest day if the data isn0t reported on time for example. These gaps can create issues for deconvolution.\nIn the next blog post I go around this by simplz averaging the last 3 days instead of using only the last day.\n\n\nSolutions to these problems\n####Iterative deconvoluton\nMultiple iterative deconvolution algorithms exist: Lucy_Richardson, Gold, or Van Cittert among others.\nLR has already been used to get daily incidence of the Spanish flu based on death records in Philadelphia at the time for example 1.\nAn adaptation of those above is used here with the basics below (see code for details):\nLet’s use the equation for \\(I_{pd}\\) as an example: \\[I_{pd}[j] = \\sum_{n_L=0}^{M_L-1}h_L[n_L]~E_{pd}[j-n_L]= h_L[j]\\circledast E_{pd}[j]\\]\nThe idea is that after an initial guess for \\(E_{pd}\\) we can iteraively find a better one.\nWith the \\(n^{th}\\) guess for \\(E_{pd}\\) written as \\(E_{pd, n}\\) we have: \\[I_{pd}[j]= h_L[j]\\circledast E_{pd, n}[j]\\] \\[\\leftrightarrow 0 = I_{pd}[j] - h_L[j]\\circledast E_{pd, n}[j]\\] \\[\\leftrightarrow E_{pd,n+1} = E_{pd,n} + I_{pd}[j] - h_L[j]\\circledast E_{pd, n}[j]\\]\nWhere \\(O[j] = I_{pd}[j] - h_L[j]\\circledast E_{pd, n}[j]\\) is the error term.\nHence we want to minimze O[j].\nInitial guess\nWe know our \\(h_L\\) and \\(h_I\\) are in part simply delay functions, so an initial first guess is to simply use the same signal delayed by the time difference caused by the impulse response.\n\nLow-pass filter\nTo go around those high-frequency notes in the data, we can siply use a lowpass filter implemented as a butterworth filter below:\nThe basic effect is that it smoothes the daily values as we have when convolving from the previous state daily values.\n\n#collapse_show\ndef lowpass(x, fc=0.05):\n  fs = 1  # Sampling frequency\n  t = np.arange(len(x)) #select number of days done in SEIR model\n  signala = x\n\n  #fc = 0.05  # Cut-off frequency of the filter\n  w = fc / (fs / 2) # Normalize the frequency\n  b, a = signal.butter(5, w, 'low')\n  return signal.filtfilt(b, a, signala)"
  },
  {
    "objectID": "posts/2020-04-02-convolution/2020-04-02-convolution.html#deconvolution-in-practice",
    "href": "posts/2020-04-02-convolution/2020-04-02-convolution.html#deconvolution-in-practice",
    "title": "“Epidemic modeling - Part 7”",
    "section": "Deconvolution in practice",
    "text": "Deconvolution in practice\n\nCoding the iterative deconvolution\n\n#collapse_show\n# Let's define an iteration function:\ndef iter_deconv(alpha, impulse_response, input_signal, delay, comparator):\n  conv=signal.fftconvolve(impulse_response, input_signal, mode='full')\n  correction=np.roll(comparator-conv[:len(comparator)], delay)\n  input_signal=np.floor(lowpass(alpha*correction+input_signal))\n  input_signal[input_signal&lt;0]=0\n  return input_signal\n\n# Define a function to return MSE between two signals as a measure of goodness of fit\ndef msecalc(A, B):\n  return ((A - B)**2).mean(axis=0)\n\n\n\nComparing \\(E_{pd}\\) from the model to \\(E_{pd}\\) obtained by de-convolving \\(I_{pd}\\)\n\n#collapse_show\n#regularization parameter\nalpha=2\n\n# Setup up the resultant Ipd we want to compare our guess with\nIpd=np.floor(lowpass(results_stoch0[5]))\nIpd[Ipd&lt;0]=0\n\n# Find delay caused by h_L\ndelay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode='full').argmax()\n\n# We want initial guess to simply be the result of the convolution delayed\ninitial_guess=np.roll(Ipd,delay)\nEnext = initial_guess\n\n# AN array to record MSE between result we want and our iterated guess\nmse=np.array([])\nmse=np.append(mse, 100000)\nmse=np.append(mse, msecalc(Ipd, signal.fftconvolve(h_L, Enext, mode='full')[:len(Ipd)]))\n\nitercount=0\nwhile mse[-1] &lt; mse[-2]:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  mse=np.append(mse, msecalc(Ipd, signal.fftconvolve(h_L, Enext, mode='full')[:len(Ipd)]))\n  print(\"Iteration #\" + str(itercount) +\": MSE= \"+str(mse[itercount]))\nprint(\"Iteration #\" + str(itercount+1) +\": MSE= \"+str(mse[-1])+\" so we use the result of the previous iteration.\")\n\nIteration #1: MSE= 128.253113864647\nIteration #2: MSE= 29.12630153955291\nIteration #3: MSE= 20.66117839168287\nIteration #4: MSE= 14.827667889972124\nIteration #5: MSE= 11.903142433438639\nIteration #6: MSE= 9.752160210380993\nIteration #7: MSE= 7.427715483375404\nIteration #8: MSE= 6.485147598975144\nIteration #9: MSE= 4.774163310379314\nIteration #10: MSE= 4.219980970725028\nIteration #11: MSE= 3.2288913917683457\nIteration #12: MSE= 2.9803815164048726\nIteration #13: MSE= 2.384262857175685\nIteration #14: MSE= 2.239378745592882\nIteration #15: MSE= 1.8410465823606756\nIteration #16: MSE= 1.858781105754766 so we use the result of the previous iteration.\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='E_pd', x=np.arange(150), y=results_stoch0[4]),\n    go.Scatter(name='Epd=lowpass(E_pd)', x=np.arange(150), y=lowpass(results_stoch0[4])),\n    go.Scatter(name='Epd=deconv(I_pd)', x=np.arange(150), y=Enext),\n    go.Scatter(name='I_pd', x=np.arange(150), y=results_stoch0[5]),\n    go.Scatter(name='Ipd=lowpass(I_pd)', x=np.arange(150), y=lowpass(results_stoch0[5])),\n    go.Scatter(name='Ipd=conv(E_pd)', x=np.arange(150), y=signal.fftconvolve(h_L, lowpass(results_stoch0[4]), mode='full')),\n    go.Scatter(name='Ipd=conv(deconv(Ipd))', x=np.arange(150), y=signal.fftconvolve(h_L, Enext, mode='full')[:len(Ipd)])\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Actual } E_{pd} \\text{ vs. deconvolution of } I_{pd}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nThe iterative deconvolution seems to work nicely and we get close results.\n\n\nComparing \\(I_{pd}\\) from the model to \\(I_{pd}\\) obtained by de-convolving \\(R_{pd}\\)\nLet’s try the same thing but using \\(R_{pd}\\) to get \\(I_{pd}\\):\n\n#collapse_hide\n#regularization parameter\nalpha=2\n\n# Setup up the resultant Ipd we want to compare our guess with\nRpd=np.floor(lowpass(results_stoch0[6]))\nRpd[Rpd&lt;0]=0\n\n# Find delay caused by h_I\ndelay=Rpd.argmax()-signal.fftconvolve(Rpd, h_I, mode='full').argmax()\n\n# We want initial guess to simply be the result of the convolution delayed\ninitial_guess=np.roll(Rpd,delay)\nInext = initial_guess\n\n\n# AN array to record MSE between result we want and our iterated guess\nmse=np.array([])\nmse=np.append(mse, 100000)\nmse=np.append(mse, msecalc(Rpd, signal.fftconvolve(h_I, Inext, mode='full')[:len(Rpd)]))\n\nitercount=0\nwhile mse[-1] &lt; mse[-2]:\n  itercount=itercount+1\n  Inext=iter_deconv(alpha, h_I, Inext, delay, Rpd)\n  mse=np.append(mse, msecalc(Rpd, signal.fftconvolve(h_I, Inext, mode='full')[:len(Rpd)]))\n  print(\"Iteration #\" + str(itercount) +\": MSE= \"+str(mse[itercount]))\nprint(\"Iteration #\" + str(itercount+1) +\": MSE= \"+str(mse[-1])+\" so we use the result of the previous iteration.\")\n\nIteration #1: MSE= 166.72190714942613\nIteration #2: MSE= 25.569889450476406\nIteration #3: MSE= 13.843314263045501\nIteration #4: MSE= 8.628213904576954\nIteration #5: MSE= 8.000003180274252\nIteration #6: MSE= 5.92374081714056\nIteration #7: MSE= 5.79711361370541\nIteration #8: MSE= 4.215547918598343\nIteration #9: MSE= 4.298363884056574 so we use the result of the previous iteration.\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Ipd', x=np.arange(150), y=results_stoch0[5]),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=np.arange(150), y=lowpass(results_stoch0[5])),\n    go.Scatter(name='Ipd=deconv(Rpd)', x=np.arange(150), y=Inext),\n    go.Scatter(name='Rpd', x=np.arange(150), y=results_stoch0[6]),\n    go.Scatter(name='Rpd=conv(Inext)', x=np.arange(150), y=signal.fftconvolve(h_I, Inext, mode='full')),\n    go.Scatter(name='Rpd=lowpass(Rpd)', x=np.arange(150), y=lowpass(results_stoch0[6])),\n    go.Scatter(name='Rpd=conv(deconv(Rpd))', x=np.arange(150), y=signal.fftconvolve(h_I, Inext, mode='full')[:len(Rpd)])    \n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Actual } I_{pd} \\text{ vs. deconvolution of } R_{pd}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nAgain our algorithm works nicely."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html",
    "href": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html",
    "title": "“Testing - what to be aware of”",
    "section": "",
    "text": "“A common case used to identify gaps in knowledge on serology testing”"
  },
  {
    "objectID": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#motivation-for-write-up",
    "href": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#motivation-for-write-up",
    "title": "“Testing - what to be aware of”",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThe real-world motivation for this write-up can be found under Story Time section, but I first wanted to give a bit of theoretical background here.\nThe importance of testing has been greatly talked about these last few weeks/months with the emergence of the COVID-19 pandemic with numerous articles being published, all underlining the importance of testing. The part emphasized is the fact that early testing allows for quick isolation of sick individuals and tracing of their potential contacts, and thus limiting the potential for spread.\nThe kind of test for this are called virologic testing and test directly for the presence of virus in an individual (active infection). This is done with Nucleic Acid Tests, or NAT, usually after amplification of the very small amount of genetic material present via Polymerase Chain Reaction. Results are available within hours or days and require diagnostic machinery and specialists.\nKnowing who has been infected is also important as it could allow already recovered patients (who are thought to gain immunity from COVID-19) to return safely to work and live basically normally. Tests that check for past infections exist, and are called serology or antibody tests. They check for specific antibodies that match those deveopped during an immune response response against SARS-CoV-2.\nThis is all good in theory, but with a disease that can cause such serious conditions as COVID-19 can, we need to be sure a positive test means for certain that person is now immmune, or we risk allowing individuals with false positives to return to normal when they should not, and continue the damaging spread of the disease.\nThe aim of this short right-up is to clear up some misconceptions around testing protocols, discuss the importance of false positives, false negatives, and its importance to guiding public health policies. The idea is basically to answer the following questions:\n\nHow many tests should return positive for a person to be, say 95% or 99% person sure he is now immune?\nWhat if a different test is negative?"
  },
  {
    "objectID": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#specificity-sensitivity-false-positives-false-negatives",
    "href": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#specificity-sensitivity-false-positives-false-negatives",
    "title": "“Testing - what to be aware of”",
    "section": "Specificity, Sensitivity, False positives, False negatives?",
    "text": "Specificity, Sensitivity, False positives, False negatives?\nAs briefly explained above, neither virological and serological tests are infallible. False positives i.e. healthy individuals with a positive test, and false negatives i.e. infected indiviuals with negative tests, can, and do happen.\nThere are numerous reasons how and why this can happen, but that is not the point of this write-up. Here, we acknowledge the fact non-perfect tests are a reality and establish testing protocol to deal with that fact.\nThankfully, before being shipped out, the various laboratories test their tests. They are able to characterize them rather precisely and give an indiction of how useful they may be with two important values: * Specificity * Sensitivity\n\nSpecificity\nSpecificity is the true negative rate - i.e. the percentage of healthy people correctly identified as such (for antibody testing, it is the percentage of people not having antibodies correctly identified as such).\nIn other words, if a test was used on 100 people who do not have antibodies, the number of people correctly identified as not hvaing antibodies is the specificity.\nA perfect test with 100% specificity, means there are no false positives. This has major implications in the current context of COVID-19 pandemic as having an anitbody test with 100% specificity would allow immune people to know so for certain (as long as research showed antibodies gave immunity).\nMathematically, we pose specificity as follows:\n\\(Specificity = \\frac{True\\ negatives}{True\\ negatives + False\\ posiives}\\)\n\n\nSensitivity\nSensitivity is the true positive rate - i.e. the percentage of infected people correctly identified as such (for antibody tests, it is the percentage of people having antibodies correctly identified as such).\nIn other words, if an antibody test was used on 100 people with antibodies, the number of people correctly identified as having anitbodies is the sensitivity.\nA perfect test with 100% sensitivity, means there are no false negatives.\nMathematically, we pose specificity as follows:\n\\(Sensitivity = \\frac{True\\ positives}{True\\ positives + False\\ negatives}\\)\n\n\nPrevalence\nPrevalence is simply the proportion of a population that has a certain characteistic. In the current context of antibody testing, the prevalence will be defined as the proportion of people who have antibody conferring immunity to COVID-19 (i.e. the proportion that has had the disease).\n\\(Prevalence = \\frac{\\#\\ People\\ with\\ antibodies}{Total\\ number\\ of\\ people}\\)\nWhere \\(Total\\ number\\ of\\ people\\) is simply \\(\\#\\ People\\ with\\ antibodies + \\# People\\ without\\ antibodies\\)"
  },
  {
    "objectID": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#story-time---part-1",
    "href": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#story-time---part-1",
    "title": "“Testing - what to be aware of”",
    "section": "Story time - Part 1",
    "text": "Story time - Part 1\nSpecificity, sensitivity, prevalence, false negatives, false positives.. This is all good, but it can be a bit abstract outside of a specific testing context.\nLet’s use the current COVID-19 pandemic as an example.\nAntibody tests are finally becoming available to the general population, and you want to know if you’ve had the disease (developped antibodies against it).\n\nNow let’s say you had influenza like symptoms back in January or February, would you expect a positive or negative result on the test?\nWhat if you haven’t been sick but want to check out of curiosity, what result would you expect?\nIf it does come back positive, how certain would you be that you actually have those antibodies and it wasn’t a false positive?\nYou decide to use a second test to make sure, again it comes positive. Now how certain are you that you have antibodies?\nOut of extreme precaution you decide to try a test from another laboratory (different specificity and sensitivity), and this time the test comes back negative. It’s become a bit more complex to evaluate your situation now.\nSo how about another test from this second laboratory? Again, negative.. Two positives, two negatives - what can you make of this information?\n\nHowever far fetched this scenario may seem, it is exactly what happened to this Florida physician:\n\ntwitter: https://twitter.com/HandtevyMD/status/1245832946612711424\n\nThere are two questions that come out of this story:\n\nAfter those 4 tests, what is the probability that Dr. Antevy has those antibodies - or more generally, can we calculate the probability of someone having antibodies given their test results?\nWhat should be the threshold of such a probability to minimize the risk of someone without antibodies going out in nature thinking he does ? (obviously if someone has 10 positive tests in a row, it seems sure enough that person has antibodies) This pushes for the need of rigorous testing protocol."
  },
  {
    "objectID": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#calculating-probabilites-given-test-results",
    "href": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#calculating-probabilites-given-test-results",
    "title": "“Testing - what to be aware of”",
    "section": "Calculating probabilites given test results",
    "text": "Calculating probabilites given test results\nClearly, our objective is to calculate the probability that a person has antibodies, or:\n\\(P(seropositive)\\)\n\nConditional probabilities\nBaye’s theorem describes probabilities when given evidence.\nSay a person has had some COVID-19 symptoms (dry cough, fever, loss of smell, slight fever) a few weeks ago. He might say there is a 75% chance that he had contracted COVID-19, and 25% chance it was another disease. In this case:\n\\(P(seropositive) = 0.75\\)\nNow this person goes to get an antibody test. What is the probability he is seropositive given a positive or negative result? Baye’s theorem allows us to write it as follows:\n\\(P(seropositive\\ |\\ positive\\ test) = \\frac{P(positive\\ test\\ |\\ seropositive)\\ *\\ P(seropositive)}{P(positive\\ test)}\\)\nand\n\\(P(seropositive\\ |\\ negative\\ test) = \\frac{P(negative\\ test\\ |\\ seropositive)\\ *\\ P(seropositive)}{P(negative\\ test)}\\)\nNote:\n\\(P(seropositive)\\) is called the prior.\n\\(P(seropositive\\ |\\ positive\\ test)\\) and \\(P(seropositive\\ |\\ negative\\ test)\\) are called the posterior.\n\n\n\\(P(Positive\\ test)\\)\nLet’s have a look at the probability of getting a positive test - there are 2 ways to get a positive result :\n\nA false positive\nA true positive\n\n\\(P(False\\ positive) = P(Positive\\ test\\ |\\ seronegative)*P(seronegative)\\)\nAnd\n\\(P(True\\ positive) = P(Positive\\ test\\ |\\ seropositive)*P(seropositive)\\)\nSo:\n\\(P(Positive\\ test) = P(Positive\\ test\\ |\\ seropositive)*P(seropositive) + P(Positive\\ test\\ |\\ seronegative)*P(seronegative)\\)\n\n\nSensitivity and Specificity revisited\nEarlier we saw:\n\\(Sensitivity = \\frac{True\\ positives}{True\\ positives + False\\ negatives}\\)\nAnd that\n\\(Specificity = \\frac{True\\ negatives}{True\\ negatives + False\\ positives}\\)\nBut we can rewrite these equations as follows:\n\\(Sensitivity = P(Positive\\ test\\ |\\ seropositive)\\)\nAnd\n\\(Specificity = P(Negative\\ test\\ |\\ seronegative) = 1-P(Positive\\ test\\ |\\ seronegative)\\)\n\n\nRe-writing the posterior probability\nUsing Baye’s rule and the calculations above we can re-write the posterior equations as follows:\n\\(P(seropositive\\ |\\ Positive\\ test) = \\frac{Sensitivity*P(seropositive)}{Sensitivity*P(seropositive)+ (1-Specificity)*(1-P(seropositive))}\\)\nAnd:\n\\(P(seronegative\\ |\\ Negative\\ test) = \\frac{Specificity*(1-P(seropositive))}{Specificity*(1-P(seropositive))+(1-Sensitivity)*P(seropositive)}\\)\n\n\nThe role of prevalence in these calculations\nThe equations above describe the probability for an individual given a test result and their prior probability. This prior probability can be estimated given presence or not of symptoms, contact with other infected individuals, location, other diagnostics, etc…\nHowever, on a population level, if we were to test a random individual, this prior becomes the prevalence and for a random individual, the equations become:\n\\(P(seropositive\\ |\\ Positive\\ test) = \\frac{Sensitivity*Prevalence}{Sensitivity*Prevalence+(1-Specificity)*(1-Prevalence)}\\)\nAnd:\n\\(P(seronegative\\ |\\ Negative\\ test) = \\frac{Specificity*(1-Prevalence)}{Specificity*(1-Prevalence)+(1-Sensitivity)*Prevalence}\\)"
  },
  {
    "objectID": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#serology-testing-simulation",
    "href": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#serology-testing-simulation",
    "title": "“Testing - what to be aware of”",
    "section": "Serology testing simulation",
    "text": "Serology testing simulation\nLet’s see what these equations look like in practice.\n\n#hide\n!pip install plotly==4.6.0\n\nCollecting plotly==4.6.0\n  Downloading https://files.pythonhosted.org/packages/15/90/918bccb0ca60dc6d126d921e2c67126d75949f5da777e6b18c51fb12603d/plotly-4.6.0-py2.py3-none-any.whl (7.1MB)\n     |████████████████████████████████| 7.2MB 2.4MB/s \nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0)\nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3)\nInstalling collected packages: plotly\n  Found existing installation: plotly 4.4.1\n    Uninstalling plotly-4.4.1:\n      Successfully uninstalled plotly-4.4.1\nSuccessfully installed plotly-4.6.0\n\n\n\n#collapse_hide\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\n#collapse_hide\n# Let's write a function to output the posterior probability given prior, test result, and test characteristics (sensitivity and specificity)\ndef Pposterior(Pprior, test_res, Sn, Sp):\n  if test_res:\n    return ((Sn * Pprior) / (Sn * Pprior + (1-Sp) * (1-Pprior)))\n  else:\n    return (1-((Sp * (1-Pprior))/(1-(Sn * Pprior + (1-Sp) * (1-Pprior)))))\n\nSay we have an antibody test with 90% sensitivity and 90% specificity - meaning we have 90% true positives and 90% true negatives, we obtain a graph as below:\n\n#collapse_hide\n\n# Below is the prior probability of being infected:\nnum=10000\nPprior = np.linspace((1/num),(num-1)/num,num=num)\n\n# Graph the results\nfig = go.Figure(data=[\n    go.Scatter(name='Test negative', x=100*Pprior, y=100*Pposterior(Pprior, False, 0.9, 0.9), line_color=\"green\"),\n    go.Scatter(name='Test positive', x=100*Pprior, y=100*Pposterior(Pprior, True, 0.9, 0.9), line_color=\"red\"),\n    go.Scatter(name='No test', x=100*Pprior, y=100*Pprior, line_color=\"blue\")\n])\n\nfig.update_layout(\n    xaxis_title = 'Prior probability of being infected',\n    yaxis_title = 'Posterior probability of being infected given test result&lt;br&gt;Specificity=90.0&lt;br&gt;Sensitivity=90.0'\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nIf you hover the mouse over the lines you can see the exact numbers.\nAs you can see, a positive or negative test does give more information than no test, but it doesn’t quite give you certainty."
  },
  {
    "objectID": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#story-time---part-2",
    "href": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#story-time---part-2",
    "title": "“Testing - what to be aware of”",
    "section": "Story time - Part 2",
    "text": "Story time - Part 2\nLet’s circle back to our Dr. Antevy with his two positive tests and the two negative tests.\nPrior to any tests, he was about 50% certain of having contracted COVID-19 based on his assesment of his symptoms, location, contact with other people, etc..\nLet’s go through his test results to see what his posterior probability of having antibodies is.\n\n#collapse_hide\n# Let's make a new function for multiple tests in a row\n\ndef PposteriorM(Pprior, test_res):\n  x = Pprior\n  for tr, sn, sp in test_res:\n    if tr == 1:\n      x = (sn * x) / (sn * x + (1-sp) * (1-x))\n    elif tr == 0:\n      x = (1-((sp * (1-x))/(1-(sn * x + (1-sp) * (1-x)))))\n  return x\n\nLet’s say these are the characteristics of the tests he used:\n\nTest 1 and 2:\nSpecificity = 0.90\nSensitivity = 0.99\nTest 3 and 4:\nSpecificity = 0.97\nSensitivity = 0.95\n\nSo a highly sensitive first test followed by a rather good allround test, a bit more specific than the first.\n\n#collapse_hide\n\n# Below is the prior probability of being infected:\nnum=10000\nPprior = np.linspace((1/num),(num-1)/num,num=num)\n\n# Test characteristics\ntest_results = [(1, 0.99, 0.90),(1, 0.99, 0.90),(0,0.95,0.97),(0,0.95,0.97)]\n\n# Graph the results\nfig = go.Figure(data=[\n    go.Scatter(name='1 - 1st positive test', x=100*Pprior, y=100*PposteriorM(Pprior, [test_results[0]])),\n    go.Scatter(name='2 - 2nd positive test', x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:2])),\n    go.Scatter(name='3 - 1st negative test', x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:3])),\n    go.Scatter(name='4 - 2nd negative test', x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:4]))    \n])\n\nfig.update_layout(\n    xaxis_title = 'Prior probability of being infected',\n    yaxis_title = 'Posterior probability of being infected given test results'\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nSo let’s go through step-by-step:\n\nBefore any test, he was about 50% sure he contracted COVID-19\nAfter the 1st positive test, this goes up to 90.8% sure\nAfter the 2nd positive test, up to 99.0% sure\nBut the 1st negative test drops it back to 83.5%\nAnd the 2nd negative all the way down to 20.7%\n\nWhat if this was done on a random person in France for example, and all 4 tests were positive.\nThen the prior would be the prevalence in France (0.2%) instead of 50%, and the step by step would be as follows: * Before any test, about 0.20% * After 1st positive: still only 1.9% chance of being seropositive\n* After 2nd positive test: only 16.4% chance of seropositive * After 3rd positive: 86% * And after 4th positive test 99.5%\nSo it took about 4 positive tests for a random person in France to become confident enough to be seropositive."
  },
  {
    "objectID": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#discussion",
    "href": "posts/2020-04-12-on-testing/2020-04-12-on-testing.html#discussion",
    "title": "“Testing - what to be aware of”",
    "section": "Discussion",
    "text": "Discussion\nThe results above strongly underline the need for clear testing protocols and clear understanding of the interpretation of test results.\nWtih a disease that can be so devastating as COVID-19, a few things should be kept in mind: * A high treshold should be used to hedge the risk a false positive * Multiple tests should be taken * Multiple tests with different characteristics (ideally at least one with high sensitivity, and one with high specificity)"
  },
  {
    "objectID": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html",
    "href": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html",
    "title": "“Epidemic modeling - Part 6”",
    "section": "",
    "text": "“A quick overview of control measures in times of pandemic/epidemic”"
  },
  {
    "objectID": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#motivation-for-write-up",
    "href": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#motivation-for-write-up",
    "title": "“Epidemic modeling - Part 6”",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThis is the 6th part of a multi-part series blog post on modeling in epidemiology.\nThe COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points.\nFollowing the first 5 parts of this blog series, we are left wondering what possible measures can be put in place to control the epidemic.\nThis 6th installment focuses on this and attempts to elucidate the subject."
  },
  {
    "objectID": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#why-do-we-need-control-measures-during-an-epidemic",
    "href": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#why-do-we-need-control-measures-during-an-epidemic",
    "title": "“Epidemic modeling - Part 6”",
    "section": "Why do we need control measures during an epidemic?",
    "text": "Why do we need control measures during an epidemic?\nIn the previous sections we have seen that without control measures in place i.e. when injecting an exposed person into the simulation, depending on the values of \\(\\beta\\) and \\(\\gamma\\), the virus will spread until it has infected everyone.\nThere are many reasons why this is bad with a virus as virulent as SARS-CoV-2: * Even with a low Case Fatality Rate (CFR), the total death toll will be unacceptable * The strain on sanitary resources of peak sick people will lead to an increase in all-cause deaths * The strain on sanitary resources will lead to increased long-term morbidity * Many economic implications * Many many others…"
  },
  {
    "objectID": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#control-measures-the-basics",
    "href": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#control-measures-the-basics",
    "title": "“Epidemic modeling - Part 6”",
    "section": "Control measures: the basics",
    "text": "Control measures: the basics\nWhile the previous sections might have left us wondering what we can possibly do to control the pread of the epidemic, the evidence left from those studies in fact leave us with a lot of clues as to where to begin.\n\nOnly one way to stop an epidemic\nWe have seen there is only one way to stop an epidemic, and that is by having: \\[R \\leq 1\\] \\[\\leftrightarrow R_0~s(t) \\leq 1\\] \\[\\leftrightarrow \\frac{\\beta}{\\gamma}~s(t) \\leq 1\\]\n\n\nTwo methods to save lives\nFrom the equation above, we see an epidemic will either continue until herd immunity is reached: \\[s(t) \\leq \\frac{1}{R_0}\\] \\[\\leftrightarrow Immune(t) \\geq 1-\\frac{1}{R_0}\\]\nOr until measures are in place so that: \\[R_0 \\leq \\frac{1}{s(t)}\\]\nIn the worst case scenario where \\(s(t) = 1\\) (completely susceptible population), we need: \\[R_0 \\leq 1\\] \\[\\leftrightarrow \\beta \\leq \\gamma\\]\nFrom this results two main ideas of control:\n\nReaching herd immunity all while controlling peak infectious individuals to within hospital and sanitary resources so as to limit morbidity and mortality\nReducing \\(\\beta\\) to smaller than \\(\\gamma\\) to stop the epidemic even before herd immunity\n\nWhile the difference between the two is noted here, in practice they are effectively the same.\nOption 1 is the same as option 2, with the difference that in 1 we don’t quite have \\[R\\leq 1\\] and so the difference between the two results from different levels of imlementation.\n\n\nControl measures in practice\nWe have seen three things influence the total number of people that end up infected with the virus: * Proportion of Susceptible in population \\(s(t)\\) * Value \\(\\beta\\) * Value of \\(\\gamma\\) (\\(T_{Infectious}\\))\nWe have also seen that the peak of infectious individuals could be affected by: * Value of \\(\\sigma\\) (\\(T_{Latent}\\))\nAnd indeed, reducing the peak of infectious at any point comes down to either: * Reducing the number of S: * vaccination * prophylactic treatment when potentially exposed * Reducing \\(\\beta = r * \\rho\\) by: 1. reducing \\(r\\) i.e. reducing the average number of contacts a person has per day: * lockdown measures * work from home * closing places where people gather (restaurants, bars, places of worship, etc) 2. reducing \\(\\rho\\) - reducing the pobability of transmitting infection from an infectious to a susceptible via: * physical distancing * hygiene measures * wearing personal protective equipment (PPE i.e. masks, gloves, etc) * Reducing \\(\\gamma\\) by: * Isolation of the sick (mass testing of symptomatic and immediate isolation of positive cases) * Contact-tracing: tracing and quarantining all contacts from infectious people as to quarantine potential exposed before they become infectious * Chemotherapy: treatment to shorten duration of sickness and infectious period * Increasing \\(\\sigma\\) by: * prophylactic treatment when potentially exposed\nAll these are the same requirements to reduce \\(R \\leq 1\\).\nLet’s try to quantify the impact of each measure below.\n\n#collapse_hide\n!pip install plotly==4.8.2\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import expon\nfrom scipy.stats import gamma\nfrom scipy.stats import weibull_min\nfrom numpy.random import default_rng\nrng = default_rng()\n\nCollecting plotly==4.8.2\n  Downloading https://files.pythonhosted.org/packages/27/99/9794bcd22fae2e12b689759d53fe26939a4d11b8b44b0b7056e035c64529/plotly-4.8.2-py2.py3-none-any.whl (11.5MB)\n     |████████████████████████████████| 11.5MB 3.3MB/s \nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.8.2) (1.12.0)\nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.8.2) (1.3.3)\nInstalling collected packages: plotly\n  Found existing installation: plotly 4.4.1\n    Uninstalling plotly-4.4.1:\n      Successfully uninstalled plotly-4.4.1\nSuccessfully installed plotly-4.8.2\n\n\n\n#collapse_hide\n# Need this new function for model below:\ndef make_df(p,num_E, num_I, num_R):\n  df = pd.DataFrame(np.full((p,1), 'S').T[0], columns=['State'])\n  df['Day'] = 0\n  tochange=df.loc[rng.choice(p, size=num_E+num_I+num_R, replace=False),'State'].index\n  df.loc[tochange[0:num_E],'State'] = 'E'\n  df.loc[tochange[num_E:num_I+num_E],'State'] = 'I'\n  df.loc[tochange[num_E+num_I:num_E+num_I+num_R],'State'] = 'R'\n  return df\n\n\n#collapse_hide\n# regular Stoachastic SEIR model below:\ndef seir_model_stoch_ctrl(beta, p, num_E, num_I, num_R, days, isolation, iso_n, contact_tracing, lockdown):\n\n    # Initialize population dataframe with data given by user\n    df = make_df(p,num_E, num_I, num_R)\n    \n    # This variable is used to track daily value of beta\n    xxbeta=np.array([],dtype=float)\n\n    # Initialize the arrays to return\n    # Below are numbers of S, E, I, R total\n    S=np.array([],dtype=int)\n    E=np.array([],dtype=int)\n    I=np.array([],dtype=int)\n    R=np.array([],dtype=int)\n    # Below are the daily additions in S, E, I, R\n    Spd=np.array([],dtype=int)\n    Epd=np.array([],dtype=int)\n    Ipd=np.array([],dtype=int)\n    Rpd=np.array([],dtype=int)\n\n    b=beta\n    beta2=b/10\n    beta3=b/10\n\n    lockdown_date=0\n    \n    # Stochastic model so use random values to decide on progression\n    rand = np.random.random(size=(p,days))\n\n    # Depending if you want exponential or gamma distribution for sigma\n    EtoI = gamma.rvs(1.8,loc=0.9,scale=(5.2-1.8)/0.9,size=p)\n\n    # Depending if you want exponential or gamma distribution for gamma and if you have isolation or not\n    # Uses distributiosn found on blog part 3\n    if isolation is True:\n      ItoR = iso_n*np.ones(p)\n    else:\n      ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p)   \n\n\n    # Iterate over every day the simulation is run\n    for j in range(0,days):\n\n        # Record daily beta values\n        xxbeta=np.append(xxbeta, b[j])\n\n        # First we get the index of the individuals that will change state today:\n\n        # Random number tells you which 'S' have been exposed on this day \n        StoE_index = df.loc[(df.State == 'S') & (rand[:,j] &lt; b[j]*len(np.where(df.State=='I')[0])/p)].index\n\n        # For each row, if a person has been a certain number of days in E, they will go to I\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        EtoI_index = df.loc[(df.State == 'E') & (j-df.Day &gt;= EtoI)].index\n        \n        # Similaraly as above\n        # For each row, if a person has been a certain number of days in I, they will go to R\n        # This follows EtoI variable which is either exponential or gamma distributed according to above\n        ItoR_index = df.loc[(df.State == 'I') & (j-df.Day &gt;= ItoR)].index\n\n        # Use indexes collected above to populate per day values\n        Epd = np.append(Epd,len(StoE_index))\n        Ipd = np.append(Ipd,len(EtoI_index))\n        Rpd = np.append(Rpd,len(ItoR_index))\n\n        # Append the S, E, I, and R arrays\n        S=np.append(S,len(np.where(df.State=='S')[0]))\n        E=np.append(E,len(np.where(df.State=='E')[0]))\n        I=np.append(I,len(np.where(df.State=='I')[0]))\n        R=np.append(R,len(np.where(df.State=='R')[0]))\n\n        # Now we use the indexes collected above randomly to change the actual population dataframe to the new states\n        df.iloc[ItoR_index] = ['R', j]\n        df.iloc[EtoI_index] = ['I', j]\n        df.iloc[StoE_index] = ['E', j]\n\n        # Code below for control measures to reduce beta values\n        if lockdown is True:\n          if ((I[-1] &gt; 100) & (Ipd[-1] &gt; 39)):\n            if lockdown_date == 0:\n              lockdown_date = j+1\n            b = beta2\n          elif ((I[-1] &gt; 100) & (Ipd[-1] &lt; 40)): \n            b = beta3\n        \n    Epd[0]+=num_E\n    Ipd[0]+=num_I\n    Rpd[0]+=num_R\n\n    return S,E,I,R, Epd, Ipd, Rpd, xxbeta, lockdown_date"
  },
  {
    "objectID": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#reducing-the-proportion-of-susceptible-in-the-population",
    "href": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#reducing-the-proportion-of-susceptible-in-the-population",
    "title": "“Epidemic modeling - Part 6”",
    "section": "Reducing the proportion of Susceptible in the population",
    "text": "Reducing the proportion of Susceptible in the population\nAs we have seen, reducing the proportion of susceptible in the population helps reduce the impact of the epidemic.\nIn the first blog post of the series, we derived the threshold for herd immunity as being \\(1-\\frac{1}{R_0}\\).\nIn our simulations we have \\(R_0 = \\frac{\\beta}{\\gamma} = \\frac{0.5}{\\frac{1}{20.62}} = 10.31\\)\nAnd so, the herd immunity threshold in our simulation should be: \\[HIT = \\frac{9.31}{10.31}~100\\% = 90.3\\%\\]\nLet’s run the model with the initial condition that 92% are in the R state already.\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 300\np = 10000\nnum_E = 1\nnum_I = 0\n\n# Run 2 simulations, one above HIT, and one below:\nnum_R1 = 8000 \nnum_R2 = 9200\nbeta_stoch = 0.5*np.ones(days)\n\n# Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigmalation, iso_n, contact_tracing, lockdown\nresults_stoch0 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R1, days, isolation=False, iso_n=iso_n, contact_tracing=False, lockdown=False)\nresults_stoch1 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R2, days, isolation=False, iso_n=iso_n1, contact_tracing=False, lockdown=False)\n\nNameError: ignored\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='I_below_HIT', x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p),\n    go.Scatter(name='I_above_HIT', x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p),\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of herd immunity on SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#reducing-beta",
    "href": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#reducing-beta",
    "title": "“Epidemic modeling - Part 6”",
    "section": "Reducing \\(\\beta\\)",
    "text": "Reducing \\(\\beta\\)\nSee blog post 2 for effect of \\(\\beta\\) on the SEIR model."
  },
  {
    "objectID": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#reducing-gamma",
    "href": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#reducing-gamma",
    "title": "“Epidemic modeling - Part 6”",
    "section": "Reducing \\(\\gamma\\)",
    "text": "Reducing \\(\\gamma\\)\n\nIsolating positive tests\nSay you isolate positive tests and are able to test everyone.\nWhat if tests are positive \\(n\\) days after people are infectious ?\nYou can isolate people after \\(n\\) days and this effectively reduces \\(T_{Infectious}\\) to \\(n\\) days.\nLet’s plot the total infectious individuals with: * n = 10 days * n = 7 days * n = 5 days * n = 2 days\n\n#collapse_hide\n# Define parameters for stochastic model\ndays = 200\np = 10000\nnum_E = 1\nnum_I = 0\nnum_R = 0\nbeta_stoch = 0.5*np.ones(days)\n\n# Isolate after iso_n days\niso_n1 = 10 # Test and isolate all infectious after 10 days\niso_n2 = 7\niso_n3 = 5\niso_n4 = 2\n\n# Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigmalation, iso_n, contact_tracing, lockdown\nresults_stoch0 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=False, iso_n=iso_n, contact_tracing=False, lockdown=False)\nresults_stoch1 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=True, iso_n=iso_n1, contact_tracing=False, lockdown=False)\nresults_stoch2 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=True, iso_n=iso_n2, contact_tracing=False, lockdown=False)\nresults_stoch3 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=True, iso_n=iso_n3, contact_tracing=False, lockdown=False)\nresults_stoch4 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=True, iso_n=iso_n4, contact_tracing=False, lockdown=False)\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='I_no_isolation', x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p),\n    go.Scatter(name='I_iso10', x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p),\n    go.Scatter(name='I_iso7', x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p),\n    go.Scatter(name='I_iso5', x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p),\n    go.Scatter(name='I_iso2', x=np.arange(len(results_stoch4[0])), y=results_stoch4[2]/p),\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Proportion of population',\n    title={\n        'text':r'$\\text{Effect of early testing and isolation on SEIR model}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\nClearly, early testing and isolation of cases reduces transmission and this the peak of infectious individuals. The faster we do this the better.\nIf we catch infectious individuals early enough, we can completely control the epidemic (see I_iso_2 above)."
  },
  {
    "objectID": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#discussion",
    "href": "posts/2020-04-02-control-measures/2020-04-02-control-measures.html#discussion",
    "title": "“Epidemic modeling - Part 6”",
    "section": "Discussion",
    "text": "Discussion\nThis was a quick introduction to the basics of control measures and their impact on the SEIR model.\nWhile there are many different possible approaches, the accumulation of the different solutions is the way to achieve control over an epidemic.\nMaybe I will make an interactive app to see the effect of each effect."
  },
  {
    "objectID": "posts/2020-03-12-COVID-19-Tracker/2020-03-12-COVID-19-Tracker.html#link-to-tracker",
    "href": "posts/2020-03-12-COVID-19-Tracker/2020-03-12-COVID-19-Tracker.html#link-to-tracker",
    "title": "COVID-19 Tracker Map",
    "section": "Link to tracker",
    "text": "Link to tracker\nFollow this link."
  },
  {
    "objectID": "posts/2020-03-12-COVID-19-Tracker/2020-03-12-COVID-19-Tracker.html#about",
    "href": "posts/2020-03-12-COVID-19-Tracker/2020-03-12-COVID-19-Tracker.html#about",
    "title": "COVID-19 Tracker Map",
    "section": "About",
    "text": "About\nThere are already many great ways to visualize the spread of sars-cov-2 around the world. I’ve made another one for family and friends that has the world on one page and France on another.\nYou can click on the countries for more details on the world map.\nLikewise you can click on specific departments to see more detail on the French map.\nThe world map uses data aggregated by the John Hopkins University CSSE.\nThe french map uses hospitalization and intensive care data from Sante Publique France."
  },
  {
    "objectID": "posts/2020-03-12-COVID-19-Tracker/2020-03-12-COVID-19-Tracker.html#future-blog-post-about-building-the-site",
    "href": "posts/2020-03-12-COVID-19-Tracker/2020-03-12-COVID-19-Tracker.html#future-blog-post-about-building-the-site",
    "title": "COVID-19 Tracker Map",
    "section": "Future blog post about building the site",
    "text": "Future blog post about building the site\nThe site is built using Plotly Dash and deployed to Heroku.\nI will likely make a blog post detailing how to build the app and deploy it."
  },
  {
    "objectID": "posts/2020-06-02-real-world/2020-06-02-real-world.html",
    "href": "posts/2020-06-02-real-world/2020-06-02-real-world.html",
    "title": "“Epidemic modeling - Part 8”",
    "section": "",
    "text": "#collapse_hide\n# This code wrangles the data from JHU\n!pip install plotly==4.9.0\n!pip install dash==1.16.0\nimport pandas as pd\nimport numpy as np\n\nimport math\n\nfrom scipy import signal\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom scipy.stats import expon\nfrom scipy.stats import gamma\nfrom scipy.stats import weibull_min\n\nfrom numpy.random import default_rng\nrng = default_rng()\n\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\n\nimport datetime\n\n\n# Import confirmed cases\nconf_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n\n#Import deaths data\ndeaths_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n\n# Import recovery data\nrec_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\n\n#iso_alpha = pd.read_csv('https://raw.githubusercontent.com/jeffufpost/sars-cov-2-world-tracker/master/data/iso_alpha.csv', index_col=0, header=0).T.iloc[0]\niso_alpha = pd.read_csv('https://raw.githubusercontent.com/jeffufpost/sars-cov-2-world-tracker/master/data/iso_alpha.csv', index_col=0, header=0)\n\n# Wrangle the data\n\n#print(\"Wrangling data by country.......\")\n# Consolidate countries (ie. frenc dom tom are included in France, etc..)\nconf_df = conf_df.groupby(\"Country/Region\")\nconf_df = conf_df.sum().reset_index()\nconf_df = conf_df.set_index('Country/Region')\n\ndeaths_df = deaths_df.groupby(\"Country/Region\")\ndeaths_df = deaths_df.sum().reset_index()\ndeaths_df = deaths_df.set_index('Country/Region')\n\nrec_df = rec_df.groupby(\"Country/Region\")\nrec_df = rec_df.sum().reset_index()\nrec_df = rec_df.set_index('Country/Region')\n\n# Remove Lat and Long columns\nconf_df = conf_df.iloc[:,2:]\ndeaths_df = deaths_df.iloc[:,2:]\nrec_df = rec_df.iloc[:,2:]\n\n# Convert country names to correct format for search with pycountry\nconf_df = conf_df.rename(index={'Congo (Brazzaville)': 'Congo', 'Congo (Kinshasa)': 'Congo, the Democratic Republic of the', 'Burma': 'Myanmar', 'Korea, South': 'Korea, Republic of', 'Laos': \"Lao People's Democratic Republic\", 'Taiwan*': 'Taiwan', \"West Bank and Gaza\":\"Palestine, State of\"})\n# Convert country names to correct format for search with pycountry\ndeaths_df = deaths_df.rename(index={'Congo (Brazzaville)': 'Congo', 'Congo (Kinshasa)': 'Congo, the Democratic Republic of the', 'Burma': 'Myanmar', 'Korea, South': 'Korea, Republic of', 'Laos': \"Lao People's Democratic Republic\", 'Taiwan*': 'Taiwan', \"West Bank and Gaza\":\"Palestine, State of\"})\n# Convert country names to correct format for search with pycountry\nrec_df = rec_df.rename(index={'Congo (Brazzaville)': 'Congo', 'Congo (Kinshasa)': 'Congo, the Democratic Republic of the', 'Burma': 'Myanmar', 'Korea, South': 'Korea, Republic of', 'Laos': \"Lao People's Democratic Republic\", 'Taiwan*': 'Taiwan', \"West Bank and Gaza\":\"Palestine, State of\"})\n\n# Convert dates to datime format\nconf_df.columns = pd.to_datetime(conf_df.columns).date\ndeaths_df.columns = pd.to_datetime(deaths_df.columns).date\nrec_df.columns = pd.to_datetime(rec_df.columns).date\n\n# Create a per day dataframe\n#print(\"Creating new per day dataframes......\")\n# Create per day dataframes for cases, deaths, and recoveries - by pd.DatafRame.diff\nconf_df_pd = conf_df.diff(axis=1)\ndeaths_df_pd = deaths_df.diff(axis=1)\nrec_df_pd = rec_df.diff(axis=1)\n\n#print(\"Create infected dataframe = conf - deaths - recoveries\")\ninf_df = conf_df - deaths_df - rec_df\n\nconf_df_pd.iloc[:,0] = 0\nrec_df_pd.iloc[:,0] = 0\ndeaths_df_pd.iloc[:,0] = 0\ninf_df.iloc[:,0] = 0\n\n#print(\"Adding dataframes of 1st, 2nd, and 3rd derivatives of number of infected\")\nfirstdev = inf_df.apply(np.gradient, axis=1)\nseconddev = firstdev.apply(np.gradient)\nthirddev = seconddev.apply(np.gradient)\n\n#print(\"Create series of first date above 100 confirmed cases.....\")\n# Create a column containing date at which 100 confirmed cases were reached, NaN if not reached yet\nfda100 = conf_df[conf_df &gt; 100].apply(pd.Series.first_valid_index, axis=1)\n\n# Create dataframe for probability plot\nprobevent = iso_alpha.join(inf_df)\nprobevent['prev'] = probevent.iloc[:,-1] / probevent['SP.POP.TOTL']\n\nRequirement already satisfied: plotly==4.9.0 in /usr/local/lib/python3.6/dist-packages (4.9.0)\nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.9.0) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.9.0) (1.15.0)\nRequirement already satisfied: dash==1.16.0 in /usr/local/lib/python3.6/dist-packages (1.16.0)\nRequirement already satisfied: dash-core-components==1.12.0 in /usr/local/lib/python3.6/dist-packages (from dash==1.16.0) (1.12.0)\nRequirement already satisfied: dash-renderer==1.8.0 in /usr/local/lib/python3.6/dist-packages (from dash==1.16.0) (1.8.0)\nRequirement already satisfied: flask-compress in /usr/local/lib/python3.6/dist-packages (from dash==1.16.0) (1.5.0)\nRequirement already satisfied: dash-html-components==1.1.1 in /usr/local/lib/python3.6/dist-packages (from dash==1.16.0) (1.1.1)\nRequirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from dash==1.16.0) (4.9.0)\nRequirement already satisfied: Flask&gt;=1.0.2 in /usr/local/lib/python3.6/dist-packages (from dash==1.16.0) (1.1.2)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from dash==1.16.0) (0.16.0)\nRequirement already satisfied: dash-table==4.10.1 in /usr/local/lib/python3.6/dist-packages (from dash==1.16.0) (4.10.1)\nRequirement already satisfied: brotli in /usr/local/lib/python3.6/dist-packages (from flask-compress-&gt;dash==1.16.0) (1.0.9)\nRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly-&gt;dash==1.16.0) (1.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly-&gt;dash==1.16.0) (1.15.0)\nRequirement already satisfied: Jinja2&gt;=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask&gt;=1.0.2-&gt;dash==1.16.0) (2.11.2)\nRequirement already satisfied: itsdangerous&gt;=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask&gt;=1.0.2-&gt;dash==1.16.0) (1.1.0)\nRequirement already satisfied: Werkzeug&gt;=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask&gt;=1.0.2-&gt;dash==1.16.0) (1.0.1)\nRequirement already satisfied: click&gt;=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask&gt;=1.0.2-&gt;dash==1.16.0) (7.1.2)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2&gt;=2.10.1-&gt;Flask&gt;=1.0.2-&gt;dash==1.16.0) (1.1.1)\n#collapse_hide\n# This code is the lowpass filter \ndef lowpass(x, fc=0.05, pad=0): # starts with cutoff freq at 0.05 but can be adjusted, should use 0.02 for France for example\n  \n  # Pad with last value\n  if pad == 1:\n    i=0\n    while i &lt; 100:\n      x=np.append(x, (x[-1]+x[-2]+x[-3])/3)\n      i=i+1\n\n  fs = 1  # Sampling frequency\n  t = np.arange(len(x.T)) #select number of days done in SEIR model\n  signala = x.T\n\n  #fc = 0.05  # Cut-off frequency of the filter\n  w = fc / (fs / 2) # Normalize the frequency\n  b, a = signal.butter(5, w, 'low')\n  return signal.filtfilt(b, a, signala), w\n#collapse_hide\n# This code creates the impulse responses\ndays = np.arange(100)\ncdf = pd.DataFrame({\n    'T_Latent': gamma.cdf(days, 1.8,loc=0.9,scale=(5.2-1.8)/0.9), \n    'T_Infectious': weibull_min.cdf(days, 2.3,loc=2,scale=20.11)\n    })\nh_L = cdf.diff().T_Latent\nh_I = cdf.diff().T_Infectious\nh_L[0] = 0\nh_I[0] = 0\n#collapse_hide\n# This code is for the iterative deconvolution\n# Let's define an iteration function:\ndef iter_deconv(alpha, impulse_response, input_signal, delay, comparator):\n  conv=signal.fftconvolve(impulse_response, input_signal, mode='full')\n  correction=np.roll(comparator-conv[:len(comparator)], delay)\n  input_signal=np.floor(lowpass(alpha*correction+input_signal, 0.05, 0)[0])\n  input_signal[input_signal&lt;0]=0\n  return input_signal\n\n# Define a function to return MSE between two signals as a measure of goodness of fit\ndef msecalc(A, B):\n  return ((A - B)**2).mean(axis=0)"
  },
  {
    "objectID": "posts/2020-06-02-real-world/2020-06-02-real-world.html#motivation-for-write-up",
    "href": "posts/2020-06-02-real-world/2020-06-02-real-world.html#motivation-for-write-up",
    "title": "“Epidemic modeling - Part 8”",
    "section": "Motivation for write-up",
    "text": "Motivation for write-up\nThis is the 8th part of a multi-part series blog post on modeling in epidemiology.\nWhile we have studied in detail the stochastc SEIR model, we have not compared this to actual real-world data.\nThe goal of this 8th installment is to expand on the 7th blog post and use deconvolution techniques on real world data to estimate the various parameters.\nMost notably, we want to calculate \\(R\\) from available data from JHU, let’s see how to go about this."
  },
  {
    "objectID": "posts/2020-06-02-real-world/2020-06-02-real-world.html#data-available",
    "href": "posts/2020-06-02-real-world/2020-06-02-real-world.html#data-available",
    "title": "“Epidemic modeling - Part 8”",
    "section": "Data available",
    "text": "Data available\nWe will use the JHU data which is updated daily and displayed graphically on my tracker here.\nA lot of countries have had difficulty reporting reliable data, but a few have done so rather well.\nWe will have a closer look at these contries: * Austria * France * Germany * Switzerland * United States\nThe JHU datasets include the following data: * Daily cumulative confirmed cases * Daily cumulative recoveries * Daily cumulative deaths\nFrom which we can calculate the following: * Daily new confirmed cases: \\(I_{pd}\\) * Daily new recoveries * Daily new deaths * Current number of cases: \\(I\\)"
  },
  {
    "objectID": "posts/2020-06-02-real-world/2020-06-02-real-world.html#analysis",
    "href": "posts/2020-06-02-real-world/2020-06-02-real-world.html#analysis",
    "title": "“Epidemic modeling - Part 8”",
    "section": "Analysis",
    "text": "Analysis\n\nWe have seen previously how we can get \\(E_{pd}\\) from deconvolution of \\(I_{pd}\\).\nWe also know that: \\[E_{pd} = \\beta ~ I ~ \\frac{S}{N}\\] \\[\\leftrightarrow \\beta = \\frac{N ~ E_{pd}}{I ~ S}\\]\nAnd we know: \\[R_0 = \\frac{\\beta}{\\gamma}\\] and \\[R=R_0\\frac{S}{N}\\]\nSo: \\[E_{pd}=\\gamma~R~I\\] \\[\\leftrightarrow R = \\frac{E_{pd}}{\\gamma~I}\\]\n\nThis means we can find \\(R\\) by deconvolution of \\(I_{pd}\\).\nOur aim is to find this graphically in the real-world data."
  },
  {
    "objectID": "posts/2020-06-02-real-world/2020-06-02-real-world.html#real-world-analysis",
    "href": "posts/2020-06-02-real-world/2020-06-02-real-world.html#real-world-analysis",
    "title": "“Epidemic modeling - Part 8”",
    "section": "Real-world analysis",
    "text": "Real-world analysis\n\nGeneric analysis\nLet’s first have a look at the genral way we will go about the analysis (i.e. not country specific).\n\nWe have daily data \\(I_{pd}\\) and \\(R_{pd}\\)\nWe have our assumed \\(T_L\\) and \\(T_I\\)\n\nNote, depending on the data we have, \\(R_{pd}\\) can be: * sum of deaths and recoveries * only deaths * only recoveries\nAnalysis steps: 1. The first thing we want to check is whether \\(h_I\\circledast I_{pd} [j]\\) gives us something close to \\(R_{pd}\\) If not, why not? (what kind of \\(R_{pd}\\) should we use?) 2. Can we get an estimated \\(E_{pd}\\) by deconvolution of \\(I_{pd}\\) ? What cutoff frequency should we use ? 3. What can that tell us about \\(R\\) and \\(\\beta\\) ?\n\n\nAustria\nLets first have a look at data from Austria\nUse fc = 0.05\n\n1. Checking \\(h_I\\)\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Bar(name='Ipd', x=conf_df_pd.loc['Austria'].index, y=conf_df_pd.loc['Austria']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=conf_df_pd.loc['Austria'].index, y=lowpass(conf_df_pd.loc['Austria'], 0.05, 1)[0]),\n    go.Bar(name='Rpd', x=rec_df_pd.loc['Austria'].index, y=rec_df_pd.loc['Austria']),\n    go.Scatter(name='Rpd=lowpass(Rpd)', x=rec_df_pd.loc['Austria'].index, y=lowpass(rec_df_pd.loc['Austria'], 0.05, 1)[0]),\n    go.Scatter(name='Rpd=conv(Ipd)', x=conf_df_pd.loc['Austria'].index, y=signal.fftconvolve(h_I, conf_df_pd.loc['Austria'], mode='full'))\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Austria: Actual } R_{pd} \\text{ vs. } h_I[j]\\circledast I_{pd}[j]$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nWe can see the actual \\(R_{pd}\\) leads \\(h_I[j]\\circledast I_{pd}[j]\\) by about 5 days.\nThere is a 20 day lag between peak \\(I_{pd}\\) and \\(h_I[j]\\circledast I_{pd}[j]\\) as is expected since \\(E[T_I] = 20.11\\).\nBut there is only a 15 day lag between peak \\(I_{pd}\\) and actual \\(R_{pd}\\).\nThere are a few possibilites for why this is the case, including that maybe we haven’t assumed the correct distribution for \\(T_I\\).\nHowever there is another reason why this could be. Testing, especially in the early days, took time, and it took time for a patient showing symptoms before he could be tested. This may simply explain the 5 day difference.\n\n\n2. Estimating \\(E_{pd}\\) by deconvolution of \\(I_{pd}\\)\n\n#collapse_hide\n\n#Settting up for deconvolution of Ipd\n\n#regularization parameter\nalpha=2\n\n# Setup up the resultant Ipd we want to compare our guess with\nIpd=np.floor(lowpass(conf_df_pd.loc['Austria'], 0.05, 1)[0])\nIpd[Ipd&lt;0]=0\n\n\n# Pad with last value\ni=0\nwhile i &lt; 100:\n  Ipd=np.append(Ipd, Ipd[-1])\n  i=i+1\n\n# Find delay caused by h_L\ndelay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode='full').argmax()\n\n# We want initial guess to simply be the result of the convolution delayed\ninitial_guess=np.roll(Ipd,delay)\nEnext = initial_guess\n\n# AN array to record MSE between result we want and our iterated guess\nmse=np.array([])\nmse=np.append(mse, 10000000)\nmse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['Austria'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Austria'])]))\n\nitercount=0\nwhile mse[-1] &lt; mse[-2]:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['Austria'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Austria'])]))\n  print(\"Iteration #\" + str(itercount) +\": MSE= \"+str(mse[itercount]))\nprint(\"Iteration #\" + str(itercount+1) +\": MSE= \"+str(mse[-1])+\" so we use the result of the previous iteration.\")\n\nIteration #1: MSE= 602.3866158407542\nIteration #2: MSE= 137.5220623354217\nIteration #3: MSE= 95.28871087778828\nIteration #4: MSE= 84.74331769219238\nIteration #5: MSE= 58.26385652331345\nIteration #6: MSE= 57.88558587706432\nIteration #7: MSE= 45.65200740982705\nIteration #8: MSE= 47.295703271338205 so we use the result of the previous iteration.\n\n\n\n#collapse_hide\n# We can keep going the iteration until lowest MSE\n\n#change alpha if you like\n#alpha=2\n\ni=0\nwhile i &lt; 10:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  print(msecalc(Ipd[:len(conf_df_pd.loc['Austria'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Austria'])]))\n  i=i+1\n\n35.024578436645875\n34.802404169985934\n35.024578436645875\n34.802404169985934\n35.024578436645875\n34.802404169985934\n35.024578436645875\n34.802404169985934\n35.024578436645875\n34.802404169985934\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['Austria'].index, y=Enext),\n    go.Scatter(name='Ipd=conv(deconv(Ipd))', x=inf_df.loc['Austria'].index, y=signal.fftconvolve(h_L, Enext, mode='full')),\n    go.Bar(name='Ipd', x=inf_df.loc['Austria'].index, y=conf_df_pd.loc['Austria']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=inf_df.loc['Austria'].index, y=lowpass(conf_df_pd.loc['Austria'], 0.05, 1)[0])\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Austria: Actual } I_{pd} \\text{ vs. convolution of deconvolution of } I_{pd}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nWe can see that our estimate for \\(\\hat{E}_{pd}\\) must be close to the reality of \\(E_{pd}\\) as \\(I_{pd}\\) is almost identical to \\(\\hat{E}_{pd} \\circledast h_L\\).\nOf course, this holds only as long as our estimate of \\(h_L\\) is close to reality.\n\n\n3. \\(\\beta\\) and \\(R\\) from \\(E_{pd}\\) and \\(I\\)\nAs described above:\n\\[R = \\frac{E_{pd}}{\\gamma~I}\\]\n\n#collapse_hide\n\n# Calculate R\ngam = 1/15 # As we say gamma is 1/20.11\nR = Enext[:len(inf_df.loc['Austria'])]*(1/gam)/inf_df.loc['Austria']\n\nfig = go.Figure(data=[    \n    go.Scatter(name='R', x=inf_df.loc['Austria'].index, y=R),\n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['Austria'].index, y=Enext),\n    go.Scatter(name='Inf', x=inf_df.loc['Austria'].index, y=inf_df.loc['Austria']),\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'R',\n    title={\n        'text':r'$\\text{Austria: R }$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\n\nFrance\nBetter to use fc = 0.02 here\n\n1. Checking \\(h_I\\)\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Bar(name='Ipd', x=conf_df_pd.loc['France'].index, y=conf_df_pd.loc['France']),\n    go.Scatter(name='Ipd=lowpass2(Ipd)', x=conf_df_pd.loc['France'].index, y=lowpass(conf_df_pd.loc['France'], 0.02, 1)[0]),\n    go.Bar(name='Rpd', x=rec_df_pd.loc['France'].index, y=rec_df_pd.loc['France']),\n    go.Scatter(name='Rpd=lowpass(Rpd)', x=rec_df_pd.loc['France'].index, y=lowpass(rec_df_pd.loc['France'], 0.05, 1)[0]),\n    go.Scatter(name='Rpd=conv(Ipd)', x=conf_df_pd.loc['France'].index, y=signal.fftconvolve(h_I, conf_df_pd.loc['France'], mode='full'))\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{France: Actual } R_{pd} \\text{ vs. } h_I[j]\\circledast I_{pd}[j]$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nData is not very good, reason for using fc = 0.02 and it helps below:\n\n\n2. Estimating \\(E_{pd}\\) by deconvolution of \\(I_{pd}\\)\n\n#collapse_hide\n\n#Settting up for deconvolution of Ipd\n\n#regularization parameter\nalpha=2\n\n# Setup up the resultant Ipd we want to compare our guess with\nIpd=np.floor(lowpass(conf_df_pd.loc['France'], 0.02, 1)[0])\nIpd[Ipd&lt;0]=0\n\n\n# Pad with last value\n#i=0\n#while i &lt; 100:\n#  Ipd=np.append(Ipd, Ipd[-1])\n#  i=i+1\n\n  # Pad with last value\ni=0\nwhile i &lt; 100:\n  Ipd=np.append(Ipd, (Ipd[-1]+Ipd[-2]+Ipd[-3])/3)\n  i=i+1\n\n# Find delay caused by h_L\ndelay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode='full').argmax()\n\n# We want initial guess to simply be the result of the convolution delayed\ninitial_guess=np.roll(Ipd,delay)\nEnext = initial_guess\n\n# AN array to record MSE between result we want and our iterated guess\nmse=np.array([])\nmse=np.append(mse, 10000000)\nmse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['France'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['France'])]))\n\nitercount=0\nwhile mse[-1] &lt; mse[-2]:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['France'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['France'])]))\n  print(\"Iteration #\" + str(itercount) +\": MSE= \"+str(mse[itercount]))\nprint(\"Iteration #\" + str(itercount+1) +\": MSE= \"+str(mse[-1])+\" so we use the result of the previous iteration.\")\n\nIteration #1: MSE= 20606.281237526222\nIteration #2: MSE= 13526.285668534732\nIteration #3: MSE= 10574.377475718808\nIteration #4: MSE= 8110.23542907448\nIteration #5: MSE= 7027.411896952288\nIteration #6: MSE= 6166.3965954792575\nIteration #7: MSE= 5621.1228898175195\nIteration #8: MSE= 5225.169278268174\nIteration #9: MSE= 4785.661680709303\nIteration #10: MSE= 4545.017011595534\nIteration #11: MSE= 4158.892779723382\nIteration #12: MSE= 4001.5961767280764\nIteration #13: MSE= 3669.5048358308395\nIteration #14: MSE= 3563.157463148674\nIteration #15: MSE= 3289.181492376996\nIteration #16: MSE= 3218.1987768323515\nIteration #17: MSE= 2988.8539703808074\nIteration #18: MSE= 2944.054349881666\nIteration #19: MSE= 2751.886100263989\nIteration #20: MSE= 2722.317605737591\nIteration #21: MSE= 2558.207523350096\nIteration #22: MSE= 2538.612302912937\nIteration #23: MSE= 2392.481423469871\nIteration #24: MSE= 2381.424278845392\nIteration #25: MSE= 2252.452737178472\nIteration #26: MSE= 2248.127189416129\nIteration #27: MSE= 2132.364086049821\nIteration #28: MSE= 2132.3036292501274\nIteration #29: MSE= 2030.0926377297346\nIteration #30: MSE= 2034.180323331884 so we use the result of the previous iteration.\n\n\n\n#collapse_hide\n# We can keep going the iteration until lowest MSE\n\n#change alpha if you like\nalpha=2\n\ni=0\nwhile i &lt; 10:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  print(msecalc(Ipd[:len(conf_df_pd.loc['France'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['France'])]))\n  i=i+1\n\n188.30632450243417\n188.7970643390086\n188.30632450243417\n188.7970643390086\n188.30632450243417\n188.7970643390086\n188.30632450243417\n188.7970643390086\n188.30632450243417\n188.7970643390086\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['France'].index, y=Enext),\n    go.Scatter(name='Ipd=conv(deconv(Ipd))', x=inf_df.loc['France'].index, y=signal.fftconvolve(h_L, Enext, mode='full')),\n    go.Bar(name='Ipd', x=inf_df.loc['France'].index, y=conf_df_pd.loc['France']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=inf_df.loc['France'].index, y=lowpass(conf_df_pd.loc['France'], 0.02, 1)[0])\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{France: Actual } I_{pd} \\text{ vs. convolution of deconvolution of } I_{pd}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\n3. \\(\\beta\\) and \\(R\\) from \\(E_{pd}\\) and \\(I\\)\nAs described above:\n\\[R = \\frac{E_{pd}}{\\gamma~I}\\]\n\n#collapse_hide\n\n# Calculate R\ngam = 1/20.11 # As we say gamma is 1/20.11\nR = Enext[:len(inf_df.loc['France'])]*(1/gam)/inf_df.loc['France']\n\nfig = go.Figure(data=[    \n    go.Scatter(name='R', x=inf_df.loc['France'].index, y=R),\n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['France'].index, y=Enext),\n    go.Scatter(name='Inf', x=inf_df.loc['France'].index, y=inf_df.loc['France']),\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'R',\n    title={\n        'text':r'$\\text{France: R }$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nAfter March 15th we see a rapi decline in \\(R\\) until \\(R&lt;1\\) since April 12th, however the last 2 weeks of May have seen an increase and it is now close to 1 in early June.\n\n\n\nGermany\nUse fc = 0.05\n\n1. Checking \\(h_I\\)\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Bar(name='Ipd', x=conf_df_pd.loc['Germany'].index, y=conf_df_pd.loc['Germany']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=conf_df_pd.loc['Germany'].index, y=lowpass(conf_df_pd.loc['Germany'], 0.05, 1)[0]),\n    go.Bar(name='Rpd', x=rec_df_pd.loc['Germany'].index, y=rec_df_pd.loc['Germany']),\n    go.Scatter(name='Rpd=lowpass(Rpd)', x=rec_df_pd.loc['Germany'].index, y=lowpass(rec_df_pd.loc['Germany'], 0.05, 1)[0]),\n    go.Scatter(name='Rpd=conv(Ipd)', x=conf_df_pd.loc['Germany'].index, y=signal.fftconvolve(h_I, conf_df_pd.loc['Germany'], mode='full'))\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Germany: Actual } R_{pd} \\text{ vs. } h_I[j]\\circledast I_{pd}[j]$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nWe can see the actual \\(R_{pd}\\) lags \\(h_I[j]\\circledast I_{pd}[j]\\) by about 10 days.\nAlthough they are pretty close still.\n\n\n2. Estimating \\(E_{pd}\\) by deconvolution of \\(I_{pd}\\)\n\n#collapse_hide\n\n#Settting up for deconvolution of Ipd\n\n#regularization parameter\nalpha=2\n\n# Setup up the resultant Ipd we want to compare our guess with\nIpd=np.floor(lowpass(conf_df_pd.loc['Germany'], 0.05, 1)[0])\nIpd[Ipd&lt;0]=0\n\n\n# Pad with last value\ni=0\nwhile i &lt; 100:\n  Ipd=np.append(Ipd, Ipd[-1])\n  i=i+1\n\n# Find delay caused by h_L\ndelay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode='full').argmax()\n\n# We want initial guess to simply be the result of the convolution delayed\ninitial_guess=np.roll(Ipd,delay)\nEnext = initial_guess\n\n# AN array to record MSE between result we want and our iterated guess\nmse=np.array([])\nmse=np.append(mse, 10000000)\nmse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['Germany'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Germany'])]))\n\nitercount=0\nwhile mse[-1] &lt; mse[-2]:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['Germany'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Germany'])]))\n  print(\"Iteration #\" + str(itercount) +\": MSE= \"+str(mse[itercount]))\nprint(\"Iteration #\" + str(itercount+1) +\": MSE= \"+str(mse[-1])+\" so we use the result of the previous iteration.\")\n\nIteration #1: MSE= 19182.550164399338\nIteration #2: MSE= 5638.48775032635\nIteration #3: MSE= 3246.2710584964543\nIteration #4: MSE= 2220.8292019558216\nIteration #5: MSE= 1734.0114023879353\nIteration #6: MSE= 1406.1497289577862\nIteration #7: MSE= 1184.4461818454367\nIteration #8: MSE= 1030.202328804179\nIteration #9: MSE= 896.2409457402528\nIteration #10: MSE= 809.4351741573042\nIteration #11: MSE= 717.199971974659\nIteration #12: MSE= 665.2713091464134\nIteration #13: MSE= 597.8790194675906\nIteration #14: MSE= 563.7280585832057\nIteration #15: MSE= 514.1662272707379\nIteration #16: MSE= 488.615995245544\nIteration #17: MSE= 450.7962144016111\nIteration #18: MSE= 432.2572271458473\nIteration #19: MSE= 404.57784046391833\nIteration #20: MSE= 390.7347771267719\nIteration #21: MSE= 369.1301222679626\nIteration #22: MSE= 358.17563961008307\nIteration #23: MSE= 340.32015028482033\nIteration #24: MSE= 330.90810363317763\nIteration #25: MSE= 317.76318508683784\nIteration #26: MSE= 310.0059778323316\nIteration #27: MSE= 299.59228791783625\nIteration #28: MSE= 293.56728048512196\nIteration #29: MSE= 285.3683914705494\nIteration #30: MSE= 280.481496274307\nIteration #31: MSE= 273.53802530941925\nIteration #32: MSE= 269.521074875222\nIteration #33: MSE= 264.4061918959138\nIteration #34: MSE= 260.3478006318621\nIteration #35: MSE= 255.7827146378307\nIteration #36: MSE= 253.58018538676694\nIteration #37: MSE= 249.94632990679477\nIteration #38: MSE= 247.73167727107818\nIteration #39: MSE= 244.17252878049058\nIteration #40: MSE= 242.3365776903206\nIteration #41: MSE= 240.3281460532849\nIteration #42: MSE= 237.68591618463944\nIteration #43: MSE= 236.09886767500404\nIteration #44: MSE= 234.00127371907027\nIteration #45: MSE= 232.41433578896027\nIteration #46: MSE= 229.92902289872004\nIteration #47: MSE= 229.06701011549225\nIteration #48: MSE= 226.27080566070512\nIteration #49: MSE= 226.32595033054216 so we use the result of the previous iteration.\n\n\n\n#collapse_hide\n# We can keep going the iteration until lowest MSE\n\n#change alpha if you like\nalpha=2\n\ni=0\nwhile i &lt; 10:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  print(msecalc(Ipd[:len(conf_df_pd.loc['Germany'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Germany'])]))\n  i=i+1\n\n205.9650472667082\n208.05908470510605\n205.9650472667082\n208.05908470510605\n205.9650472667082\n208.05908470510605\n205.9650472667082\n208.05908470510605\n205.9650472667082\n208.05908470510605\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['Germany'].index, y=Enext),\n    go.Scatter(name='Ipd=conv(deconv(Ipd))', x=inf_df.loc['Germany'].index, y=signal.fftconvolve(h_L, Enext, mode='full')),\n    go.Bar(name='Ipd', x=inf_df.loc['Germany'].index, y=conf_df_pd.loc['Germany']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=inf_df.loc['Germany'].index, y=lowpass(conf_df_pd.loc['Germany'], 0.05, 1)[0])\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Germany: Actual } I_{pd} \\text{ vs. convolution of deconvolution of } I_{pd}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nWe can see that our estimate for \\(\\hat{E}_{pd}\\) must be close to the reality of \\(E_{pd}\\) as \\(I_{pd}\\) is almost identical to \\(\\hat{E}_{pd} \\circledast h_L\\).\nOf course, this holds only as long as our estimate of \\(h_L\\) is close to reality.\n\n\n3. \\(\\beta\\) and \\(R\\) from \\(E_{pd}\\) and \\(I\\)\nAs described above:\n\\[R = \\frac{E_{pd}}{\\gamma~I}\\]\n\n#collapse_hide\n\n# Calculate R\ngam = 1/20.11 # As we say gamma is 1/20.11\nR = Enext[:len(inf_df.loc['Germany'])]*(1/gam)/inf_df.loc['Germany']\n\nfig = go.Figure(data=[    \n    go.Scatter(name='R', x=inf_df.loc['Germany'].index, y=R),\n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['Germany'].index, y=Enext),\n    go.Scatter(name='Inf', x=inf_df.loc['Germany'].index, y=inf_df.loc['Germany']),\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'R',\n    title={\n        'text':r'$\\text{Germany: R }$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nWe can see \\(R&lt;1\\) since April 3rd, however the peak \\(E_{pd}\\) is slightly ahead of that March 23rd.\n\n\n\nSwitzerland\n\n1. Checking \\(h_I\\)\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Bar(name='Ipd', x=conf_df_pd.loc['Switzerland'].index, y=conf_df_pd.loc['Switzerland']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=conf_df_pd.loc['Switzerland'].index, y=lowpass(conf_df_pd.loc['Switzerland'], 0.05, 1)[0]),\n    go.Bar(name='Rpd', x=rec_df_pd.loc['Switzerland'].index, y=rec_df_pd.loc['Switzerland']),\n    go.Scatter(name='Rpd=lowpass(Rpd)', x=rec_df_pd.loc['Switzerland'].index, y=lowpass(rec_df_pd.loc['Switzerland'], 0.05, 1)[0]),\n    go.Scatter(name='Rpd=conv(Ipd)', x=conf_df_pd.loc['Switzerland'].index, y=signal.fftconvolve(h_I, conf_df_pd.loc['Switzerland'], mode='full'))\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Switzerland: Actual } R_{pd} \\text{ vs. } h_I[j]\\circledast I_{pd}[j]$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nWe can see the actual \\(R_{pd}\\) leads \\(h_I[j]\\circledast I_{pd}[j]\\) by about 7 days.\n\n\n2. Estimating \\(E_{pd}\\) by deconvolution of \\(I_{pd}\\)\n\n#collapse_hide\n\n#Settting up for deconvolution of Ipd\n\n#regularization parameter\nalpha=2\n\n# Setup up the resultant Ipd we want to compare our guess with\nIpd=np.floor(lowpass(conf_df_pd.loc['Switzerland'], 0.05, 1)[0])\nIpd[Ipd&lt;0]=0\n\n\n# Pad with last value\ni=0\nwhile i &lt; 100:\n  Ipd=np.append(Ipd, Ipd[-1])\n  i=i+1\n\n# Find delay caused by h_L\ndelay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode='full').argmax()\n\n# We want initial guess to simply be the result of the convolution delayed\ninitial_guess=np.roll(Ipd,delay)\nEnext = initial_guess\n\n# AN array to record MSE between result we want and our iterated guess\nmse=np.array([])\nmse=np.append(mse, 10000000)\nmse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['Switzerland'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Switzerland'])]))\n\nitercount=0\nwhile mse[-1] &lt; mse[-2]:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['Russia'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Switzerland'])]))\n  print(\"Iteration #\" + str(itercount) +\": MSE= \"+str(mse[itercount]))\nprint(\"Iteration #\" + str(itercount+1) +\": MSE= \"+str(mse[-1])+\" so we use the result of the previous iteration.\")\n\nIteration #1: MSE= 864.5294935911032\nIteration #2: MSE= 232.47979293395576\nIteration #3: MSE= 173.17995448248922\nIteration #4: MSE= 142.7278443215308\nIteration #5: MSE= 102.95126255601593\nIteration #6: MSE= 97.14103443111384\nIteration #7: MSE= 73.32554749188243\nIteration #8: MSE= 73.40930143937898 so we use the result of the previous iteration.\n\n\n\n#collapse_hide\n# We can keep going the iteration until lowest MSE\n\n#change alpha if you like\nalpha=2\n\ni=0\nwhile i &lt; 10:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  print(msecalc(Ipd[:len(conf_df_pd.loc['Switzerland'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['Switzerland'])]))\n  i=i+1\n\n35.2303147406422\n35.62189380732824\n35.229698402255934\n35.613648731822856\n35.229698402255934\n35.613648731822856\n35.229698402255934\n35.613648731822856\n35.229698402255934\n35.613648731822856\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['Switzerland'].index, y=Enext),\n    go.Scatter(name='Ipd=conv(deconv(Ipd))', x=inf_df.loc['Switzerland'].index, y=signal.fftconvolve(h_L, Enext, mode='full')),\n    go.Bar(name='Ipd', x=inf_df.loc['Switzerland'].index, y=conf_df_pd.loc['Switzerland']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=inf_df.loc['Switzerland'].index, y=lowpass(conf_df_pd.loc['Switzerland'], 0.05, 1)[0])\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{Switzerland: Actual } I_{pd} \\text{ vs. convolution of deconvolution of } I_{pd}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nWe can see that our estimate for \\(\\hat{E}_{pd}\\) must be close to the reality of \\(E_{pd}\\) as \\(I_{pd}\\) is almost identical to \\(\\hat{E}_{pd} \\circledast h_L\\).\nOf course, this holds only as long as our estimate of \\(h_L\\) is close to reality.\n\n\n3. \\(\\beta\\) and \\(R\\) from \\(E_{pd}\\) and \\(I\\)\nAs described above:\n\\[R = \\frac{E_{pd}}{\\gamma~I}\\]\n\n#collapse_hide\n\n# Calculate R\ngam = 1/20.11 # As we say gamma is 1/20.11\nR = Enext[:len(inf_df.loc['Switzerland'])]*(1/gam)/inf_df.loc['Switzerland']\n\nfig = go.Figure(data=[    \n    go.Scatter(name='R', x=inf_df.loc['Switzerland'].index, y=R),\n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['Switzerland'].index, y=Enext),\n    go.Scatter(name='Inf', x=inf_df.loc['Switzerland'].index, y=inf_df.loc['Switzerland']),\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'R',\n    title={\n        'text':r'$\\text{Switzerland: R }$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\n\nUnited States\n\n1. Checking \\(h_I\\)\n\n#collapse_hide\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Bar(name='Ipd', x=conf_df_pd.loc['US'].index, y=conf_df_pd.loc['US']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=conf_df_pd.loc['US'].index, y=lowpass(conf_df_pd.loc['US'], 0.05, 1)[0]),\n    go.Bar(name='Rpd', x=rec_df_pd.loc['US'].index, y=rec_df_pd.loc['US']),\n    go.Scatter(name='Rpd=lowpass(Rpd)', x=rec_df_pd.loc['US'].index, y=lowpass(rec_df_pd.loc['US'], 0.05, 1)[0]),\n    go.Scatter(name='Rpd=conv(Ipd)', x=conf_df_pd.loc['US'].index, y=signal.fftconvolve(h_I, conf_df_pd.loc['US'], mode='full'))\n])\n\nfig.update_layout(\n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{US: Actual } R_{pd} \\text{ vs. } h_I[j]\\circledast I_{pd}[j]$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nData is not very good.\n\n\n2. Estimating \\(E_{pd}\\) by deconvolution of \\(I_{pd}\\)\n\n#collapse_hide\n\n#Settting up for deconvolution of Ipd\n\n#regularization parameter\nalpha=2\n\n# Setup up the resultant Ipd we want to compare our guess with\nIpd=np.floor(lowpass(conf_df_pd.loc['US'], 0.05, 1)[0])\nIpd[Ipd&lt;0]=0\n\n\n# Pad with last value\ni=0\nwhile i &lt; 100:\n  Ipd=np.append(Ipd, Ipd[-1])\n  i=i+1\n\n# Find delay caused by h_L\ndelay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode='full').argmax()\n\nif (np.abs(delay)&gt;20):\n  delay = -15\n\n# We want initial guess to simply be the result of the convolution delayed\ninitial_guess=np.roll(Ipd,delay)\nEnext = initial_guess\n\n# AN array to record MSE between result we want and our iterated guess\nmse=np.array([])\nmse=np.append(mse, 10000000)\nmse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['US'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['US'])]))\n\nitercount=0\nwhile mse[-1] &lt; mse[-2]:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc['US'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['US'])]))\n  print(\"Iteration #\" + str(itercount) +\": MSE= \"+str(mse[itercount]))\nprint(\"Iteration #\" + str(itercount+1) +\": MSE= \"+str(mse[-1])+\" so we use the result of the previous iteration.\")\n\nIteration #1: MSE= 1302852.439800496\nIteration #2: MSE= 561276.1316916037\nIteration #3: MSE= 392265.42495631176\nIteration #4: MSE= 318143.92169379967\nIteration #5: MSE= 272333.0641150245\nIteration #6: MSE= 233533.32988425402\nIteration #7: MSE= 202941.06092517954\nIteration #8: MSE= 179715.6638386193\nIteration #9: MSE= 159674.82822623185\nIteration #10: MSE= 144041.10302527325\nIteration #11: MSE= 129965.13942407207\nIteration #12: MSE= 118818.8007805406\nIteration #13: MSE= 108715.87370526823\nIteration #14: MSE= 100634.66153928495\nIteration #15: MSE= 93446.25080919724\nIteration #16: MSE= 87452.88477850877\nIteration #17: MSE= 82485.3263728532\nIteration #18: MSE= 77847.33446293023\nIteration #19: MSE= 74602.08153637181\nIteration #20: MSE= 70781.73306311587\nIteration #21: MSE= 68800.8673361004\nIteration #22: MSE= 65387.86700604501\nIteration #23: MSE= 64292.13662703821\nIteration #24: MSE= 61036.839684856765\nIteration #25: MSE= 60524.93042299802\nIteration #26: MSE= 57292.73138597839\nIteration #27: MSE= 57176.48413202111\nIteration #28: MSE= 53959.94059590202\nIteration #29: MSE= 54120.03347178106 so we use the result of the previous iteration.\n\n\n\n#collapse_hide\n# We can keep going the iteration until lowest MSE\n\n#change alpha if you like\nalpha=2\n\ni=0\nwhile i &lt; 10:\n  itercount=itercount+1\n  Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd)\n  print(msecalc(Ipd[:len(conf_df_pd.loc['US'])], signal.fftconvolve(h_L, Enext, mode='full')[:len(conf_df_pd.loc['US'])]))\n  i=i+1\n\n33182.952031012894\n33274.1027461995\n33182.952031012894\n33274.1027461995\n33182.952031012894\n33274.1027461995\n33182.952031012894\n33274.1027461995\n33182.952031012894\n33274.1027461995\n\n\n\n#collapse_hide\nfig = go.Figure(data=[    \n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['US'].index, y=Enext),\n    go.Scatter(name='Ipd=conv(deconv(Ipd))', x=inf_df.loc['US'].index, y=signal.fftconvolve(h_L, Enext, mode='full')),\n    go.Bar(name='Ipd', x=inf_df.loc['US'].index, y=conf_df_pd.loc['US']),\n    go.Scatter(name='Ipd=lowpass(Ipd)', x=inf_df.loc['US'].index, y=lowpass(conf_df_pd.loc['US'], 0.05, 1)[0])\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'Count',\n    title={\n        'text':r'$\\text{US: Actual } I_{pd} \\text{ vs. convolution of deconvolution of } I_{pd}$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\n3. \\(\\beta\\) and \\(R\\) from \\(E_{pd}\\) and \\(I\\)\nAs described above:\n\\[R = \\frac{E_{pd}}{\\gamma~I}\\]\n\n#collapse_hide\n\n# Calculate R\ngam = 1/20.11 # As we say gamma is 1/20.11\nR = Enext[:len(inf_df.loc['US'])]*(1/gam)/inf_df.loc['US']\n\nfig = go.Figure(data=[    \n    go.Scatter(name='R', x=inf_df.loc['US'].index, y=R),\n    go.Scatter(name='Epd=deconv(Ipd)', x=inf_df.loc['US'].index, y=Enext),\n    go.Scatter(name='Inf', x=inf_df.loc['US'].index, y=inf_df.loc['US']),\n])\n\nfig.update_layout(\n    \n    xaxis_title = 'Day',\n    yaxis_title = 'R',\n    title={\n        'text':r'$\\text{US: R }$',\n        'x':0.5,\n        'xanchor':'center'\n    }\n)\n\nfig.show()\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nAfter March 15th we see a rapi decline in \\(R\\) until \\(R&lt;1\\) since April 12th, however the last 2 weeks of May have seen an increase and it is now close to 1 in early June.\nWe can see \\(R\\) declined rapidly after March 12th to \\(R&lt;1\\) on May 30th, but has since grown back to close to 1."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scattered Thoughts",
    "section": "",
    "text": "“HIV modeling”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\n“Epidemic modeling - Part 5”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\n“HIV modeling”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\n“Epidemic modeling - Part 7”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\n“Epidemic modeling - Part 4”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\n“Scraping COVID-19 data from data.gouv.fr”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\n“Testing - what to be aware of”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\n“Epidemic modeling - Part 6”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\n“Epidemic modeling - Part 8”\n\n\n\n\n\n\n\n\n\n\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\nEpidemic modeling - Part 3\n\n\n\n\n\n\n\nprobability distributions\n\n\nmodeling\n\n\nSEIR\n\n\nepidemiology\n\n\n\n\nExamining the major flaw of the deterministic SEIR model\n\n\n\n\n\n\nMar 25, 2020\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\nEpidemic modeling - Part 2\n\n\n\n\n\n\n\nmodeling\n\n\ncompartmentalization\n\n\nSEIR\n\n\nepidemiology\n\n\ndisease dynamics\n\n\n\n\nA deterministic numerical SEIR model\n\n\n\n\n\n\nMar 18, 2020\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\nEpidemic modeling - Part 1\n\n\n\n\n\n\n\nmodeling\n\n\ncompartmentalization\n\n\nSEIR\n\n\nepidemiology\n\n\ndisease dynamics\n\n\n\n\nCompartmentalization models and disease dynamics\n\n\n\n\n\n\nMar 15, 2020\n\n\nJeffrey Post\n\n\n\n\n\n\n  \n\n\n\n\nCOVID-19 Tracker Map\n\n\n\n\n\n\n\nCOVID-19\n\n\nDashboards\n\n\nPlotly\n\n\n\n\nA link to my COVID-19 map tracker\n\n\n\n\n\n\nMar 12, 2020\n\n\nJeffrey Post\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-03-12-COVID-19-Tracker/2020-03-12-COVID-19-Tracker.html",
    "href": "posts/2020-03-12-COVID-19-Tracker/2020-03-12-COVID-19-Tracker.html",
    "title": "COVID-19 Map",
    "section": "",
    "text": "[]/sars-cov2-french-tracker.png “French map preview”)"
  }
]